---
editor_options: 
  chunk_output_type: console
---
<!-- New commands for matrices in regression -->
\newcommand{\Xmatrix}{\begin{bmatrix}1 & X_{11} & X_{12} & \cdots & X_{1k}\\1 & X_{21} & X_{22} & \cdots & X_{2k}\\\vdots & \vdots & \vdots & \ddots & \vdots\\1 & X_{n1} & X_{n2} & \cdots & X_{nk}\end{bmatrix}
}

\newcommand{\Xonematrix}{\begin{bmatrix}1 & X_{11}\\1 &X_{21}\\\vdots\\1 & X_{n1}\end{bmatrix}}

\newcommand{\Ymatrix}{\begin{bmatrix}Y_1\\Y_2\\\vdots\\Y_n\end{bmatrix}}

\newcommand{\betamatrix}{\begin{bmatrix}\beta_0\\\beta_1\\\vdots\\\beta_k\end{bmatrix}}

\newcommand{\betaonematrix}{\begin{bmatrix}\beta_0\\\beta_1\end{bmatrix}}

\newcommand{\Ematrix}{\begin{bmatrix}E_1\\E_2\\\vdots\\E_n\end{bmatrix}}

 <!-- Creates a shortcommand for sums -->
\newcommand{\Sum}[1]{\sum_{i=1}^#1}

<!-- CONTENT -->

# Statistisk inferens {#sec-inference}
::: {.video-container}
{{< video https://youtu.be/tFJhALxNH-A >}}
:::

När vi anser oss ha hittat en lämplig modell kan vi fokusera på att tolka modellens resultat avseende populationen. Inom regressionsmodellering kan vi genomföra flera olika typer av statistisk inferens; på hela modellen, på grupper av parametrar, eller på enskilda parametrar. 

Vi kan börja med ett *F-test* för hela modellen för att se ifall minst en parameter är signifikant, att modellen är värd att undersöka vidare, för att sedan genomföra enskilda *t-test* för respektive parameter och bedöma vilka förklarande variabler har en signifikant påverkan på responsvariabeln. Då kvalitativa variabler ofta består utav flera parametrar behöver dessa slås samman för att undersöka variabelns samband vilket vi kan göra med ett *partiellt F-test*.

Innan vi går in på de olika testerna behöver vi presentera *ANOVA*-tabellen som används för att dela upp responsvariabelns variation i modellens olika komponenter; modellens förklarande variabler och feltermen.

## ANOVA {#sec-regression-anova}
**An**alysis **o**f **Va**riance är en samling metoder som beräknar variationen av olika modellkomponenter. Målet med en modell är att förklara den *totala variationen* i responsvariabeln på bästa sätt. Allting som de förklarande variablerna hjälper till att beskriva kallas för den *förklarade variationen* och det som modellen inte lyckas förklara (felet) är den *oförklarade variationen*.

$$
\underbrace{\mathbf{Y}}_\text{total variation} = \underbrace{\mathbf{X} \boldsymbol{\beta}}_\text{förklarad variation} + \underbrace{\mathbf{E}}_\text{oförklarad variation}
$$ {#eq-variation-components}

@eq-variation-components visar att den totala variationen är en summa av den förklarade och oförklarade variationen vilket också ses i formlerna för dessa. Respektive komponent beräknas enligt:

$$
  \text{total variation} = SST = \mathbf{Y}'\mathbf{Y} - \left(\frac{1}{n}\right)\mathbf{Y}'\mathbf{J}\mathbf{Y}
$$
där $\mathbf{J}$ är enhetsmatrisen, en $n \times n$ matris endast innehållande 1:or. 

Det kanske inte är så lätt att se vad dessa matrisberäkningar faktiskt beskriver men beräkningen motsvarar $\Sum{n}(Y_i - \bar{Y})^2$, alltså täljaren i en variansberäkning för $Y$. Den vänstra termen ($\mathbf{Y}'\mathbf{Y}$) motsvarar $Y_i$ och den högra termen ($\left(\frac{1}{n}\right)\mathbf{Y}'\mathbf{J}\mathbf{Y}$) motsvarar $\bar{Y}$, responsvariabelns medelvärde. Den totala variationen beskriver hur mycket variation som uppkommer ifall vi skulle använda medelvärdet av $Y$ som modell.

$$
  \text{oförklarad variation} = SSE = \mathbf{Y}'\mathbf{Y} - \boldsymbol{\hat{\beta}}'\mathbf{X}'\mathbf{Y}
$$
SSE har vi tidigare använt som ett mått på felet i modellen, se @eq-sse, vilket betyder att $\boldsymbol{\hat{\beta}}'\mathbf{X}'\mathbf{Y}$ motsvarar $\hat{Y}_i$. 

$$
  \text{förklarad variation} = SSR = \boldsymbol{\hat{\beta}}'\mathbf{X}'\mathbf{Y} - \left(\frac{1}{n}\right)\mathbf{Y}'\mathbf{J}\mathbf{Y}
$$

SSR beskriver variationen mellan modellens anpassade värde och medelvärdet av $Y$. Det kan i sin tur kan tolkas som hur mycket mer variation som modellen bidrar med jämfört med medelvärdet, eller kort sagt hur mycket bättre modellen är på att förklara variationen i $Y$.

Vi har tidigare använt en annan matrisformel för SSE men med hjälp av omformuleringen kan vi tydligt se hur SST = SSR + SSE:
$$
\mathbf{Y}'\mathbf{Y} - \left(\frac{1}{n}\right)\mathbf{Y}'\mathbf{J}\mathbf{Y} = \mathbf{Y}'\mathbf{Y} \underbrace{-  \boldsymbol{\hat{\beta}}'\mathbf{X}'\mathbf{Y} + \boldsymbol{\hat{\beta}}'\mathbf{X}'\mathbf{Y}}_\text{summerar till 0} - \left(\frac{1}{n}\right)\mathbf{Y}'\mathbf{J}\mathbf{Y}
$$
Vi kan också visualisera denna relation i ett stackat stapeldiagram. Den totala höjden av stapeln är SST medan de olika delarna beskriver hur stor del av den totala variationen som är förklarad eller oförklarad i en viss modell.

```{r}
#| fig-cap: Visualisering av de olika källor av variation
#| fig-height: 3
#| fig-width: 5 
#| echo: false

anovaTable <- anova(simpleModel) |> 
  rownames_to_column(var = "Source") |> 
  mutate(Source = factor(Source, levels = Source)) |> 
  mutate(
    Source = if_else(Source != "Residuals", "Förklarad", "Oförklarad")
  ) |> 
  group_by(Source) |> 
  summarize(
    `Sum Sq` = sum(`Sum Sq`)
  ) |> 
  ungroup()

ggplot(anovaTable, aes(x = "Total Variation", y = `Sum Sq`, fill = Source)) +
  geom_bar(width = 0.75, stat = "identity", position = "stack", color = "black") +
  theme_minimal() +
  theme(
    panel.grid.major.x = element_blank()
  ) +
  labs(x = "",
       y = "Sum of Squares") +
  scale_fill_manual("Variation", values = c("steelblue", "#d9230f"))


```

### ANOVA-tabellen {#sec-regression-anova-table}
En ANOVA-tabell är ett sätt att effektivt få en översikt av dessa olika komponenter samt visa ytterligare information, såsom **frihetsgraderna** ($df$) för respektive komponent och **medelkvadratsummor**. 

Frihetsgrader beskriver hur många lutningsparametrar som skattas för respektive del^[Frihetsgrader beskriver egentligen hur många bitar oberoende information som finns för en beräkning. Tänk tillbaka på beräkningen av en stickprovsstandardavvikelse vars frihetsgrader är $n - 1$, antalet observationer - 1, för att vi skattar medelvärdet när vi beräknar standardavvikelsen.] och medelkvadratsummor visar den genomsnittliga variationen per frihetsgrad, $\frac{SS}{df}$.

```{r}
#| echo: false
#| tbl-cap: Enkel ANOVA-tabell
#| tbl-cap-location: top
#| label: tbl-anova-example

anova_table <- data.frame(
  Source = c("Model (Regression)", "Error", "Total"),
  DF = c("$df_R = k$", "$df_E = n - (k + 1)$", "$df_T = n - 1$"),
  `Sum of Squares` = c(
    "$SSR = \\boldsymbol{\\hat{\\beta}}' \\mathbf{X}' \\mathbf{Y} - \\frac{1}{n} \\mathbf{Y}' \\mathbf{J} \\mathbf{Y}$",
    "$SSE = \\mathbf{Y}' \\mathbf{Y} - \\boldsymbol{\\hat{\\beta}}' \\mathbf{X}' \\mathbf{Y}$",
    "$SSY = \\mathbf{Y}' \\mathbf{Y} - \\frac{1}{n} \\mathbf{Y}' \\mathbf{J} \\mathbf{Y}$"
  ),
  `Mean Square` = c("$MSR = \\frac{SSR}{df_R}$", "$MSE = \\frac{SSE}{df_E}$", "")
)

kable(anova_table, format = "markdown", booktabs = TRUE, escape = FALSE, col.names = c("Source", "DF", "Sum of Squares", "Mean Square"), parse = TRUE) 

```

En enkel ANOVA-tabell som @tbl-anova-example visar endast de tre huvudsakliga komponenterna, men olika programvaror kan ibland visa andra uppdelningar som standard. I en multipel linjär regressionsmodell är det vanligt att dela upp den förklarade variationen ytterligare, exempelvis i **sekventiella kvadratsummor**.

### Sekventiella kvadratsummor
Beräkningarna för en ANOVA-tabell sker automatiskt i R när vi använder `lm()` och vi kan plocka ut tabellen från modellobjektet med hjälp av `anova()`, (se @tip-lm-objects).

```{r}
#| tbl-cap: ANOVA-tabell från R
#| tbl-cap-location: top
#| label: tbl-anova-example-R

anova(simpleModel) |> 
  round(4) |> 
  kable() |> 
  kable_styling("striped")

```

Som standard, delar R upp modellens kvadratsumma (SSR) i de enskilda förklarande variablerna med hjälp av sekventiella (även kallad betingade) kvadratsummor. En sekventiell kvadratsumma beskriver hur mycket variation en förklarande variabel bidrar med givet att modellen redan innehåller andra förklarande variabler. 

Ordningen som presenteras i @tbl-anova-example-R är ordningen som variablerna läggs till i modellen, till exempel visar andra raden $SS(\text{bill\_depth\_mm} | \text{species})$, att näbbredden bidrar med `r anova(simpleModel)[2,2] |> round(4)` ytterligare unik förklarad variation av responsvariabeln som art inte redan har förklarat. Den tredje raden visar $SS(\text{flipper\_length\_mm} | \text{species}, \text{bill\_depth\_mm})$, det vill säga hur mycket ytterligare unik variation som fenlängden förklarar i en modell som inkluderar näbbredd och art.

Rent matematiskt beräknas den sekventiella kvadratsumman som en summa av antingen SSE eller SSR mellan två olika modeller, en utan den tillagda variabeln och en med variabeln inkluderad. Anta att vi vill lägga till variabel $X^*$ till en modell som har $k$ andra variabler, då ser beräkningen ut som följer:

$$
\begin{aligned}
SS(X^*|X_1, \ldots, X_k) &= SSE_{X_1, \ldots, X_k} - SSE_{X_1, \ldots, X_k, X^*} = \\
&= SSR_{X_1, \ldots, X_k, X^*} - SSR_{X_1, \ldots, X_k}
\end{aligned}
$$ {#eq-seq-ss}

Notera att SSR ökar för varje ytterligare variabel som läggs till i modellen, medan SSE alltid minskar. En variation måste alltid vara positiv, därav beräknas $SSE_{reducerad} - SSE_{komplett}$ eller $SSR_{komplett} - SSR_{reducerad}$.

Sekventiella kvadratsummor påverkas av ordningen variablerna läggs till i modellen. Låt oss byta ordning på de förklarande variablerna när vi anpassar modellen:

```{r}
#| tbl-cap: Annan ordning på modellernas variabler
#| tbl-cap-location: top
#| label: tbl-anova-example-sex

model <- lm(formula = bill_length_mm ~ sex + ., data = modelData)

anova(model) |> 
  round(4) |> 
  kable() |> 
  kable_styling("striped")

```

I @tbl-anova-example-sex ser vi att $SS(\text{sex}) = `r anova(model)[1,2] |> round(4)`$ vilket är betydligt högre än $SS(\text{sex}|\text{species}, \text{bill\_depth\_mm}, \text{flipper\_length\_mm}, \text{body\_mass\_g}) = `r anova(simpleModel)[5,2] |> round(4)`$ från @tbl-anova-example-R. Variabeln kön bidrar med mycket variation när den är ensam i en modell, men när den läggs till i en modell som redan har andra variabler bidrar den inte med lika mycket unik information. Detta betyder att den förklarade variationen som variabeln bidrar med verkar finnas i övriga variabler också. Denna iakttagelse kommer vi komma tillbaka till i ett senare kapitel. 

Någonting som är lika i de två tabellerna är SSE. Vi har i båda modellerna inkluderad samma variabler vilket innebör att SST, SSR, och SSE överlag är densamma. Summan av alla sekventiella kvadratsummor ska fortfarande bli SSR oavsett ordningen på variablerna och på grund av den additiva egenskapen hos variationen har SST och SSE inte heller förändrats.

## Statistisk inferens {#sec-stat-inference}
Med hjälp av de olika källorna av variation kan vi beräkna tester för hela eller delar av modellen i olika F-test, medan de enskilda parameterskattningarna och dess tillhörande medelfel kan användas i tester för enskilda lutningsparametrar.

### F-test för modellen
I en multipel linjär regression är ett F-test för hela modellen bra att börja med för att se ifall minst en lutningsparameter är signifikant. Vi undersöker hypoteserna:

\begin{align*}
H_0&: \beta_1 = \beta_2 = \beta_3 = \cdots = \beta_k = 0\\
H_a&: \text{Minst en av } \beta_j \text{ i } H_0 \text{ är skild från } 0
\end{align*}

Om minst en lutningsparameter är signifikant betyder det att det finns åtminstone en variabel som bidrar med förklarad variation, att modellen är bättre än att använda enbart $\bar{Y}$. Testvariabeln undersöker relationen mellan den förklarande och oförklarande variationen genom dess medelkvadratsummor. 

$$
F_{test} = \frac{SSR / k}{SSE / (n - (k+1))} = \frac{MSR}{MSE} 
$$

Testvariabeln följer en *F-fördelning* som styrs av två frihetsgrader; $df1$ från täljaren och $df2$ från nämnaren i beräkningen, det vill säga modellens och felets frihetsgrader. Om $H_0$ är sann kommer testvariabeln bli 0, medan om $H_a$ är sann kommer testvariabeln bli ett stort positivt tal. Eftersom båda medelkvadratsummorna är positiva tal innebär det att kvoten alltid kommer vara positiv och vi kan förkasta $H_0$ om testvariabeln befinner sig nog långt från 0.

```{r}
#| fig-cap: Olika F-fördelningar och deras frihetsgrader
#| fig-height: 4
#| fig-width: 7 

# Skapar en funktion för att generera olika F-fördelningar
generateFdistribution <- function(df1, df2, n = 1000) {
  x <- seq(0, 5, length.out = n)  
  y <- df(x, df1, df2)  
  tibble(x = x, y = y, df1 = df1, df2 = df2)  
}

# Skapar en lista med olika frihetsgrader
dfs <- list(c(5, 30), c(10, 100), c(20, 50), c(30, 300))

# Genererar data
Fdistributions <- dfs |>
  purrr::map_df(~generateFdistribution(.x[1], .x[2]), .id = "Distribution") |>
  mutate(Distribution = paste0("df1 = ", df1, ", df2 = ", df2))

# Plot the F-distributions using ggplot2
ggplot(Fdistributions) + 
  aes(x = x, y = y, color = Distribution) +
  geom_line(linewidth = 1) +
  labs(
    x = "F-värde",
    y = "Densitet",
    color = "Frihetsgrader"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
    legend.position = "right",
    legend.title = element_text(face = "bold")
  ) +
  scale_color_manual(values = c("steelblue", "#d9230f", "black", "grey50"))


```

För att få fram SSR från en ANOVA-tabell i R behöver vi summera de sekventiella kvadratsummorna. Vi kan sedan bearbeta tabellen för att få fram testvariabeln och använda frihetsgraderna för respektive källa i `pf(lower.tail = FALSE)` för att få fram p-värdet för testet.

```{r}
#| tbl-cap: Bearbetad och förenklad ANOVA tabell
#| tbl-cap-location: top
#| label: tbl-anova-example-simple


anovaTable <- anova(simpleModel)

# Beräknar raden för SSR utifrån alla rader förutom SSE
SSR <- anovaTable[-nrow(anovaTable),] |> 
  summarize(across(Df:`Sum Sq`, ~sum(.x))) |> 
  mutate(`Mean Sq` = `Sum Sq` / Df,
         `F value` = NA,
         `Pr(>F)` = NA)

# Kombinerar SSR med SSE från ursprungliga tabellen
simpleAnova <- SSR |> 
  add_row(anovaTable[nrow(anovaTable),]) |> 
  mutate(
    `F value` = 
      ifelse(row_number() == 1,
            `Mean Sq`[1] / `Mean Sq`[2], 
            NA),
    `Pr(>F)` = 
        ifelse(row_number() == 1, 
              pf(q = `F value`[1], df1 = Df[1], df2 = Df[2], lower.tail = FALSE), 
              NA)
    )

rownames(simpleAnova) <- c("Model", "Residuals")

kable(simpleAnova, digits = 4) |> 
  kable_styling("striped")

```

Eftersom p-värdet är mindre än 5 procent, kan $H_0$ förkastas och minst en av variablerna har ett samband med responsvariabeln.^[Om vi hade tagit ett annat beslut (att inte förkasta nollhypotesen) hade det inte varit relevant att fortsätta med analysen, eller åtminstone att fokusera resterande analys på att undersöka varför en multipel linjär regressionsmodell som vi förväntar har ett samband utifrån parvisa spridningsdiagram inte visar på det tillsammans.]

### Partiella F-test för grupper av parametrar
Ibland är vi intresserade att undersöka delar av modellen, en grupp med lutningsparametrar. Ett sådant fall är om vi vill undersöka en kvalitativ variabels påverkan eftersom den kan ha transformerats till flera indikatorvariabler alla med en tillhörande lutningsparameter. Ett annat tillfälle är om vi vill undersöka om flera variabler tillsammans bidrar med förklarad variation till modellen.

Istället för att undersöka alla lutningsparametrar undersöks nu ett urval:
\begin{align*}
H_0&: \beta_1 = \beta_2 = \beta_3 = \cdots = \beta_s = 0\\
H_a&: \text{Minst en av } \beta_j \text{ i } H_0 \text{ är skild från } 0
\end{align*}
där $s$ är antalet parametrar som undersöks.

Testvariabeln för ett partiellt F-test kräver en komplett (betecknad $_F$) och en reducerad modell (betecknad $_R$). Den kompletta modellen består av alla variabler medan den reducerade modellen utgår från att $H_0$ är sann och variablerna som undersöks har plockats bort från anpassningen. Vi kan välja att antingen använda SSR eller SSE för att beräkna hur mycket förklarad variation som försvinner mellan de två modellerna enligt samma princip som @eq-seq-ss.

$$
F_{test} = \frac{(SSR_F - SSR_R) / s}{SSE_F / (n - (k+1))} = \frac{(SSE_R - SSE_F) / s}{SSE_F / (n - (k+1))}
$$ {#eq-partial-f}

Testvariabeln är fortfarande F-fördelat med $s$ respektive $n - (k+1)$ frihetsgrader.

#### Räkneknep för partiella F-test
Med hjälp av @eq-seq-ss kan @eq-partial-f formuleras på ett tredje sätt som underlättar vår analysprocess. Vi kan skriva om skillnaden i förklarad variation mellan den kompletta och reducerade modellen som en sekventiell kvadratsumma. Exempelvis kan vi vilja undersöka om variabeln art har ett samband med responsvariabeln. Eftersom den variabeln transformeras till två indikatorvariabler omfattar hypoteserna två lutningsparametrar.

\begin{align*}
H_0&: \beta_{Chinstrap} = \beta_{Gentoo} = 0\\
H_a&: \text{Minst en av } \beta_j \text{ i } H_0 \text{ är skild från } 0
\end{align*}

Den reducerade modellen skapas utifrån att $H_0$ är sann, det vill säga $\beta_{Chinstrap} = \beta_{Gentoo} = 0$ och de två modellernas förklarade variation skulle betecknas som:
$$
\begin{aligned}
  SSR_{R} &= SSR_{bill\_depth\_mm, flipper\_length\_mm, body\_mass\_g, sex} \\
  SSR_{F} &= SSR_{bill\_depth\_mm, flipper\_length\_mm, body\_mass\_g, sex, species}
\end{aligned}
$$

Vi kan omformulera täljaren i @eq-partial-f till: 
$$
SS(species|bill\_depth\_mm, flipper\_length\_mm, body\_mass\_g, sex)
$$
I de ANOVA-tabeller som presenterats tidigare kan vi få fram denna kvadratsumma direkt om art läggs till som den sista variabeln i modellen.

```{r}
#| tbl-cap: ANOVA-tabell från en modell där art läggs till sist
#| tbl-cap-location: top
#| label: tbl-anova-example-species

model <- lm(bill_length_mm ~ bill_depth_mm + flipper_length_mm + body_mass_g + sex + species, data = modelData)

anova(model) |> 
  round(4) |> 
  kable() |> 
  kable_styling("striped")

```

En ANOVA-tabell med sekventiella kvadratsummor beräknar ett partiellt F-test för respektive variabel (och dess parameter/parametrar) som undersöker huruvida variabeln bidrar med en signifikant ökning av den förklarade variationen till en modell som redan inkluderar variablerna ovanför. @tbl-anova-example-species beräknar nu det partiella F-test för art ($F_{test} = `r anova(model)[5, 4] |> round(4)`$) som vi var intresserade av och vi kan direkt tolka p-värdet för testet ($p-värde < 0.001$) som att minst en av lutningsparametrarna är signifikant skild från 0. 

Om vi genomför ett partiellt F-test för **flera variabler** kan vi inte använda p-värden som anges i tabellen då hypoteserna omfattar fler lutningsparametrar/variabler än vad de sekventiella kvadratsummorna visar. Anta att vi vill undersöka om art och kön tillsammans bidrar något till modellen. Hypotesprövningen skulle då omfatta:

$$
\begin{aligned}
H_0&: \beta_{sexMale} = \beta_{Chinstrap} = \beta_{Gentoo} = 0\\
H_a&: \text{Minst en av } \beta_j \text{ i } H_0 \text{ är skild från } 0
\end{aligned}
$$

Den sekventiella kvadratsumman som vi vill använda anges som $SS(species, sex|bill\_depth\_mm, flipper\_length\_mm, body\_mass\_g)$ och vi kan beräkna fram detta värde genom att summera de två variablernas SS från @tbl-anova-example-species.


$$
\begin{aligned}
SS(species, sex|bill\_depth\_mm, flipper\_length\_mm, body\_mass\_g) = \\
SS(species|bill\_depth\_mm, flipper\_length\_mm, body\_mass\_g, sex) + \\
SS(sex|bill\_depth\_mm, flipper\_length\_mm, body\_mass\_g)
\end{aligned}
$$
Alternativet är att anpassa två modeller i R, den kompletta och reducerade och läsa av SSE eller summera SSR från respektive ANOVA-tabell.

#### Partiellt F-test för specifika värden
Vi kan ställa upp en generell modell som:
\begin{align*}
  Y = \beta_0 + \beta_1 \cdot X_1 + \beta_2 \cdot X_2 + \beta_3 \cdot X_3 +\beta_4 \cdot X_4 +\beta_5 \cdot X_5+ E
\end{align*}

Om vi ska undersöka specifika parametrars värden (som inte är 0) kan vi genomföra följande härledning. Anta $H_0:$ $\beta_2=4$ och $\beta_5 = -2$ som ska undersökas med ett test. 

\begin{align*}
Y &= \beta_0 + \beta_1 \cdot X_1 + 4 \cdot X_2 + \beta_3 \cdot X_3 +\beta_4 \cdot X_4 - 2 \cdot X_5+ E\\
Y - 4 \cdot X_2 + 2 \cdot X_5 &= \beta_0 + \beta_1 \cdot X_1  + \beta_3 \cdot X_3 +\beta_4 \cdot X_4 + E \\
Y^* &= \beta_0 + \beta_1 \cdot X_1  + \beta_3 \cdot X_3 +\beta_4 \cdot X_4 + E
\end{align*}

$Y^*$ kan anses vara en reducerad modell för ett F-test. I R kan detta inte lösas genom `anova()` utan måste beräknas ''för hand'' genom att anpassa två modeller, den kompletta och den reducerade.

### t-test för enskilda parametrar
Att använda ANOVA-tabellen för att undersöka enskilda parametrar är inte lämpligt då det kräver att variabeln anges sist i modelleringen för att det partiella F-testet undersöker just den enskilda variabeln i relation till övriga modellen. Istället bör vi använda t-test för respektive parameter.

Formellt undersöks hypoteserna:
$$
\begin{aligned}
  H_0&: \beta_j = 0\\
  H_a&: \beta_j \ne 0
\end{aligned}
$$
där $j$ är någon av lutningsparametrarna i en anpassad modell. 

Testvariabeln beräknas utifrån den skattade lutningsparametern och dess medelfel:
$$
\begin{aligned}
t_{test} = \frac{b_j - 0}{s_{b_j}}
\end{aligned}
$$

Testvariabeln är t-fördelad givet $H_0$ med $n-(k+1)$ frihetsgrader.

I R används t-test i koefficienttabellen som vi kan plocka ut ur `summary()`-objektet genom `coef()`.

```{r}
#| tbl-cap: Koefficienttabell för en modell med tillhörande t-test för enskilda parametrar
#| tbl-cap-location: top
#| label: tbl-coef-example

summary(simpleModel) |> 
  coef() |> 
  round(4) |> 
  kable(format = "markdown",
        col.names = c("Variabel", "Skattning", "Medelfel", "t-värde", "p-värde"), 
        parse = TRUE) |> 
  kable_styling("striped")
```

I @tbl-coef-example ser vi att p-värdet för alla t-testen är väldigt låga (nära 0). För varje enskilda hypotesprövning kan vi på fem procents signifikans förkasta $H_0$ vilket betyder att variabeln har en signifikant påverkan på responsvariabeln. 

::: {.callout-important}
Om en parameter inte anses signifikant är det en motivering till att variabeln kan plockas bort, vi anpassar en reducerad modell och en ny analys påbörjas. Om en variabel plockas bort kommer de övriga parameterskattningarna förändras och tolkningar samt inferens behöver uppdateras. 
:::

### Konfidensintervall för $\beta$
Slutsatsen vi kan dra från dessa hypotesprövningar är att modellen innehåller variabler som alla har ett signifikant samband med responsvariabeln. Om vi vill tolka magnituden av effekten gentemot populationen, inte bara om sambandet är signifikant, behöver vi beräkna intervallskattningar.

$$
\begin{aligned}
b_j \pm t_{n - (k+1); 1- \alpha/2} \cdot s_{b_j}
\end{aligned}
$$

## Enkla utvärderingsmått {#sec-r-square}
Bara för att en modell är lämplig, uppfyller modellantaganden och innehåller signifikanta parametrar, betyder det inte att modellen är den bästa som kan skapas eller överhuvudtaget bra. Med hjälp av olika utvärderingsmått kan vi få en överblick på hur bra modellen är.

*Förklaringsgraden* ($R^2$) beskriver hur stor andel av den totala variationen som förklaras av modellens förklarande variabler. Med denna beskrivning kan vi beräkna $R^2$ som:
$$
\begin{aligned}
  R^2 = \frac{SSR}{SST} = 1 - \frac{SSE}{SST}
\end{aligned}
$$
På grund av att SSR alltid blir större ju fler variabler som en modell innehåller, behöver vi justera måttet för att kunna jämföra modeller av olika storlekar. Istället bör vi titta på den justerade förklaringsgraden ($R^2_{a}$) för att se vilken modell som är bäst. En förbättrad $R^2_{a}$ betyder att modellen har tagit bort onödig komplexitet.

$$
\begin{aligned}
  R^2_a = 1 - \frac{SSE / (n - (k+1))}{SST / (n - 1)}
\end{aligned}
$$

## Prediktioner {#sec-predictions}
Prediktioner innebär att vi skattar värdet på Y givet observerade värden på X med hjälp av den anpassade regressionslinjen. Dessa prediktioner kommer falla längsmed linjen vilket ytterligare motiverar att modellen behöver vara lämplig och bra. Vi vill inte att en prediktion i ett område är mer träffsäker än en prediktion i en annan eller att regressionslinjen generellt är en dålig representation av responsvariabeln. 

Modellen utgår från en specifik *definitionsmängd*, de observerade värdena på $\mathbf{X}$, och det är även inom denna mängd som prediktioner bör göras. Det finns vissa tillfällen, till exempel inom tidsserieanalys, där prediktioner görs utanför definitionsmängden men där finns ett beroende i tiden som möjliggör dessa *extrapoleringar*. Inom "vanlig" regression bör vi undvika att extrapolera regressionslinjen utanför definitionsmängden.

### Medelvärdet av Y för givna $\mathbf{X}$
Om vi är intresserad av det genomsnittliga värdet på responsvariabeln för alla nya observationer med givna värden på $\mathbf{X}$ kan vi skatta $\mu_{Y|{\mathbf{X}_0}}$ där $\mathbf{X}_0$ innehåller värden för den nya observationen.

$$
\mathbf{X}_0 = \begin{bmatrix}
    1 \\
    X_{1,0}\\
    \vdots\\
    X_{k,0}
    \end{bmatrix}
$$
Vi utgår från den anpassade regressionsmodellen och beräknar en punktprediktion av responsvariabeln enligt:
$$
 \hat{Y}_{\mathbf{X}_0} = \mathbf{X}_0'\boldsymbol{\hat{\beta}}
$$ 

Medelfelet för skattningen tar hänsyn till:

$$
 s^2_{\hat{Y}_{\mathbf{X}_0}} = \mathbf{X}_0'\mathbf{s}^2_{\boldsymbol{\hat{\beta}}}\mathbf{X}_0
$$

Intervallskattningen för ett genomsnitt blir ett konfidensintervall.

### Enskild prediktion av Y för givna $\mathbf{X}$
Om vi istället är intresserad av ett enskilt värde på Y med givna värden på $\mathbf{X}$, kan vi skatta $Y_{\mathbf{X}_0}$.

$$
s^2_{pred} = MSE + s^2_{\hat{Y}_{\mathbf{X}_0}}
$$
Intervallskattningen för ett värde av Y blir ett prediktionsintervall.

<!-- ## Övningsuppgifter {#sec-exercise-evaluate} -->
<!-- Använd återigen `marketing` från @sec-exercise-explore. -->

<!-- 1. Undersök med ett -->
