[
  {
    "objectID": "regression.html",
    "href": "regression.html",
    "title": "Introduktion till regression",
    "section": "",
    "text": "Att undersöka samband mellan variabler är ett vanligt steg i att förstå relationer eller fenomen. Till exempel hur åldern på ett träd påverkar dess volym, hur olika doser av en medicin påverkar en individs blodtryck, eller hur en persons ålder och utbildningsnivå påverkar dens ingångslön.\nRegressionsanalys omfattar metoder som anpassar matematiska modeller vilka på bästa sätt kan ge en förenklad bild av verkligheten. I det allra enklaste fallet anpassas en linjär modell där variablernas samband antas enkelriktat och konstant. \\[\nY = \\beta_0 + \\beta_1 \\cdot X + E\n\\tag{1}\\]\ndär:\nOm flera förklarande variabler antas påverka responsvariabeln utökas den linjära modellen med flera \\(\\beta_j\\), en för varje förklarande variabel \\(X_j\\).",
    "crumbs": [
      "Regression",
      "Introduktion till regression"
    ]
  },
  {
    "objectID": "regression.html#studier-och-andra-variabler",
    "href": "regression.html#studier-och-andra-variabler",
    "title": "Introduktion till regression",
    "section": "Studier och andra variabler",
    "text": "Studier och andra variabler\nEn matematisk modell behöver nödvändigtvis inte beskriva ett “orsak-och-verkan” samband, eller som vi brukar benämna det, ett kausalt samband. Samband kan ibland uppstå utav ren slump där det inte finns någon logisk koppling mellan olika variabler. Denna typ av samband benämns som korrelationssamband.\n\n\n\n\n\n\n\n\nFigur 1: Sambandet mellan antalet filmer som Nicolas Cage medverkade i och antalet dödsfall av drunkning mellan 1999 och 2009 i USA (“CDC - NCHS - National Center for Health Statistics — Cdc.gov”; “Nicolas Cage | Actor, Producer, Director — Imdb.com”)\n\n\n\n\n\nFigur 1 uppvisar ett exempel på korrelationssamband där de två variablerna inte har någon logisk koppling till varandra utan endast har observerats ha en positiv korrelation.\nEn regressionsmodell som beskriver sambandet kan inte urskilja mellan de två typer av samband vilket innebär att vi som analytiker måste ta hänsyn till vilken sorts och hur data har samlats in.\nExemplet i figuren är insamlad som en observationsstudie där mätvärden (antal dödsfall och filmer) på enheterna (år) har observerats från olika registerdata. Vi har inte kunnat styra vilken relation dessa variabler har till varandra och studien i sig har inte tagit hänsyn till någon specifik orsak och verkan mellan de två. Vi kan därför endast dra slutsatser om korrelationssamband från en observationsstudie, vi kan säga att desto fler filmer Nicolas Cage medverkar i medför ett större antal dödsfall, men vi kan inte säga något om den kausala effekten.\nFör att kunna dra slutsatser om kausala samband behöver vi genomföra en experimentell studie där vi styr vilka mätvärden som enheter får eller har och responsvariablen antas vara en direkt verkan av de förklarande variablerna. Medicinska studier, till exempel studier om Covid-vaccinets effektivitet på att motverka en infektion, är typiska exempel på experimentella studier där en förklarande variabel (dos) ges till vissa grupper av enheter där andra påverkande effekter kontrolleras för att justera den förklarande variabelns verkliga påverkan.\nI en observationsstudie kan vi ibland observera kontrollvariabler som kan justera den förklarande variabelns faktiska påverkan men det är främst i experimentella studier som dessa typer av variabler kan användas. Okända variabler kallas för confounding-effekter och antas påverka både den förklarande och responsvariabeln.\nI följande två figurer visas den förklarande och responsvariabeln med ovaler. De kända (heldragen) och okända (streckade) kontrollvariablerna visas som rektanglar.\n\n\n\n\n\n\n\n\n\n\nG\n\n\n\nOkänd\n\nOkänd\n\n\n\nFilmer\n\nFilmer\n\n\n\nOkänd-&gt;Filmer\n\n\n\n\n\nDödsfall\n\nDödsfall\n\n\n\nOkänd-&gt;Dödsfall\n\n\n\n\n\nFilmer-&gt;Dödsfall\n\n\n\n\n\n\n\n\n\n\n\n\nFigur 2: Exempel på relationen mellan variabler i en observationsstudie.\n\n\n\nEftersom sambandet mellan filmer och dödsfall förmodligen endast uppkommit av slumpen kan det finnas andra okända variabler som påverkar de båda.\n\n\n\n\n\n\n\n\n\n\nG\n\n\n\nÅlder\n\nÅlder\n\n\n\nBlodtryck\n\nBlodtryck\n\n\n\nÅlder-&gt;Blodtryck\n\n\n\n\n\nKön\n\nKön\n\n\n\nKön-&gt;Blodtryck\n\n\n\n\n\nMedicin\n\nMedicin\n\n\n\nMedicin-&gt;Blodtryck\n\n\n\n\n\n\n\n\n\n\n\n\nFigur 3: Exempel på relationen mellan variabler i en experimentell studie.\n\n\n\nEffekten av en medicin på blodtryck kan också påverkas av personens ålder och kön (Gu et al. 2008) vilka inkluderas i modellen för att isolera den förklarande variabelns effekt.",
    "crumbs": [
      "Regression",
      "Introduktion till regression"
    ]
  },
  {
    "objectID": "regression.html#modellens-antaganden",
    "href": "regression.html#modellens-antaganden",
    "title": "Introduktion till regression",
    "section": "Modellens antaganden",
    "text": "Modellens antaganden\nSyftet med en modell är att ge en lämplig förenkling av verkligheten. En linjär regressionsmodell kan vara en lämplig förenkling av ett samband om följande antaganden uppfylls:\n\natt det för varje \\(X\\) finns en slumpvariabel \\(Y\\) med ett ändligt medelvärde och varians,\nobservationerna är oberoende av varandra,\nmedelvärdet, \\(\\mu_{Y|X}\\), kan modelleras linjärt,\nvariansen för \\(Y\\) är lika för alla värden av \\(X\\), \\(\\sigma^2_{Y|X} \\equiv \\sigma^2\\),\nslumpvariabeln \\(Y\\) är normalfördelad för alla värden av \\(X\\).\n\nVi kan sammanfatta majoriteten av dessa antaganden med: \\[\n   Y|X \\overset{\\mathrm{iid}}{\\sim} N(\\mu_{Y|X}, \\sigma^2_{Y|X})\n\\] där \\(\\mathrm{iid}\\) betyder “independent and identically distributed” motsvarande antagande 2.\n\n\n\n\n\n\nViktigt\n\n\n\nDet finns inget antagande om att \\(Y \\sim N(\\mu_Y, \\sigma^2_Y)\\)! Alla dessa antaganden fokuserar på att vi med hjälp av \\(X\\) har en normalfördelad slumpvariabel \\(Y\\).\n\n\nOm det tredje antagandet uppfylls kan vi modellera väntevärdet av \\(Y|X\\) med den linjära modellen: \\[\n  E[Y|X] = \\beta_0 + \\beta_1 \\cdot X\n\\tag{2}\\] så att: \\[\nY|X \\overset{\\mathrm{iid}}{\\sim} N(\\beta_0 + \\beta_1 \\cdot X, \\sigma^2_{Y|X})\n\\]\nTill skillnad från Formel 1 saknar Formel 2 modellens felterm på grund av att vi nu modellerar endast medelvärdet av slumpvariabelns fördelning, \\(\\mu_{Y|X}\\). Osäkerheten runtomkring medelvärdet är variansen av fördelningen.\nNär vi modellerar varje enskilda observation inkluderas \\(E\\) vilket innebär att vi kan flytta modellens antaganden från \\(Y|X\\) till \\(E\\). \\[\n  E \\overset{\\mathrm{iid}}{\\sim} N(0, \\sigma^2)\n\\tag{3}\\]\nDenna omskrivning ger oss en bra utgångspunkt att utvärdera lämpligheten av en anpassad modell.",
    "crumbs": [
      "Regression",
      "Introduktion till regression"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "01-regression/01-intro-to-regression.html",
    "href": "01-regression/01-intro-to-regression.html",
    "title": "intro-to-regression",
    "section": "",
    "text": "Vi kan anse variansanalys vara ett specialfall av en mycket mer flexibel metodik, regressionsanalys. Regression avser att undersöka sambandet mellan en responsvariabel (\\(y\\)) och en eller flera förklarande variabler (\\(x\\)). Den enklaste formen av regressionsmodell är den linjära modellen där vi antar att sambandet mellan en förklarande variabel och responsvariabeln är konstant ökande eller minskande, men regressionsmodeller kan också anpassas för icke-linjära samband.\n\n\nLåt oss börja med den enkla linjära modellen när vi har en förklarande och en responsvariabel. Från matematiken kan vi uttrycka den räta linjens ekvation med \\(y = kx + m\\) och det är samma struktur på regressionsmodellen inom statistiken, dock med några andra beteckningar.\nSkärningspunkten med y-axeln (\\(m\\)) kallar vi numera för interceptet och lutningen (\\(k\\)) heter fortfarande samma sak men mäter numera sambandet som den förklarande variabeln har med responsvariabeln. Inom regression har vi en sann modell för populationen där vi också tar hänsyn till de avvikelser som finns mellan varje enskilda observation och den räta linjen. I kapitel @ref(fig:penguins) visualiserades ett samband mellan två stycken kontinuerliga variabler likt:\n\n\nLoading required package: tidyverse\n\n\nWarning: package 'ggplot2' was built under R version 4.3.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\nLoading required package: palmerpenguins\n\n\nWarning in geom_segment(aes(x = 58, y = 20.88547 - 0.08502 * 58 + 0.1, xend = 58, : All aesthetics have length 1, but the data has 344 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\nSpridningsdiagram som visar sambandet mellan näbbens längd och bredd på pingviner.\n\n\n\n\nSom vi kan se i diagrammet kommer den anpassade linjen inte träffa exakt varje enskilda punkt och avståndet från regressionslinjen till respektive punkt, den rödstreckade linjen, kallas för modellens fel. Den sanna regressionsmodellen i populationen ser ut som följer:\n\\[\\begin{align*}\n  Y_i = \\alpha + \\beta \\cdot X_i + \\varepsilon_i\n\\end{align*}\\] där \\(\\alpha\\) är modellens intercept, \\(\\beta\\) är modellens lutning som beskriver sambandet mellan \\(X\\) och \\(Y\\), och \\(\\varepsilon_i\\) är modellens felterm, det vill säga avståndet mellan observation \\(i\\) och punkten på regressionslinjen vid samma värde på \\(X\\). \\(i\\) är ett index som indikerar på observationen.\nI praktiken kommer vi aldrig få kunskap om den sanna modellen då information som samlas in ofta är ett urval från populationen. Vi måste istället skatta modellen: \\[\\begin{align*}\n  \\hat{Y} = a + b \\cdot X_i\n\\end{align*}\\]\nFeltermen inkluderas nu inte i modellens uttryck, men vi kan skatta felet i modellen med hjälp utav dess residualer. \\[\\begin{align*}\n  e_i = Y_i - \\hat{Y}_i\n\\end{align*}\\]\nMed hjälp utav dessa residualer kan vi sedan undersöka hur bra modellen är på att anpassa datamaterialet och kontrollera modellens antaganden.\n\n\nOm vi vill skatta en regressionsmodell i R används lm(). Denna funktion skattar en linjär modell och kan användas för alla “enkla” linjära modeller som vi vill skatta. Funktionen kräver två stycken argument; formula anger vilken modell som ska skattas med angivna variabler och data anger vilket datamaterial som variablerna finns i.\n\n## Anpassar en enkel linjär regression\nmodell &lt;- \n  lm(\n    formula = bill_depth_mm ~ bill_length_mm,\n    data = penguins\n  )\n\nResultatet från funktionen är ett objekt som innehåller många olika delar som vi kan nå med andra relevanta funktioner. De två viktigaste funktionerna som innehåller majoriteten av resultaten som vi är intresserade utav är summary() och anova().\n\nsummary(modell)\n\n\nCall:\nlm(formula = bill_depth_mm ~ bill_length_mm, data = penguins)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.1381 -1.4263  0.0164  1.3841  4.5255 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    20.88547    0.84388  24.749  &lt; 2e-16 ***\nbill_length_mm -0.08502    0.01907  -4.459 1.12e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.922 on 340 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.05525,   Adjusted R-squared:  0.05247 \nF-statistic: 19.88 on 1 and 340 DF,  p-value: 1.12e-05\n\n\nUtksriften från denna funktion ger en översikt av skattningarna för parametrarna i modellen och en sammanfattande modellutvärdering. Utskriften kan delas in i fyra huvudsakliga delar:\n\nCall visar funktionen som använts för att producera utskriften,\nResiduals visar beskrivande statistik för de skattade residualerna från modellen,\nCoefficients innehåller en tabell med regressionskoefficienter (skattade parametrar), deras medelfel, och tillhörande t-test för parametrarnas signifikans,\nDe sista tre raderna visar sammanfattande mått för modellen.\n\nVi kan också beräkna en ANOVA-tabell för regression men i detta läge kan vi använda funktionen anova() för att plocka ut denna information från den redan skattade regressionsmodellen.\n\nanova(modell)\n\nAnalysis of Variance Table\n\nResponse: bill_depth_mm\n                Df  Sum Sq Mean Sq F value   Pr(&gt;F)    \nbill_length_mm   1   73.47  73.473  19.884 1.12e-05 ***\nResiduals      340 1256.36   3.695                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nDenna tabell innehåller de olika källorna av variation som modellen innehåller, den förklarande variabeln och felet. Vi får också ett tillhörande F-test för modellens totala anpassning i denna utskrift till skillnad från den tidigare koefficient-tabellen.\n\n\n\n\nOm vi har ett datamaterial med flera potentiella förklarande variabler kan vi skatta en multipel linjär regressionsmodell där alla variablerna inkluderas samtidigt. Detta medför att vi får en modell som, på gott och ont, tar hänsyn till eventuella påverkan mellan de olika förklarande variablerna och ger möjligheten att lägga till mer komplexa samband, till exempel interaktioner.\nFormellt ser den generella regressionsmodellen ut som: \\[\\begin{align*}\n  Y_j = \\alpha + \\beta_1 \\cdot X_{1, j} + \\beta_2 \\cdot X_{2, j} + \\hdots + \\beta_m \\cdot X_{m, j} + \\varepsilon_j\n\\end{align*}\\] där index \\(j\\) nu är observationsindex och \\(m\\) anger hur många förklarande variabler som inkluderas i modellen.\nTill exempel skulle vi vara intresserade av att lägga till ytterligare en variabel i vår modell över pingvinerna, flipper_length_mm.\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\nSpridningsdiagram som visar på sambandet mellan fenlängd och näbbredd\n\n\n\n\nDen utökade modellen anges i formula med ett + mellan de olika förklarande variablerna som ska inkluderas.\n\nmodell &lt;- \n  lm(\n    formula = bill_depth_mm ~ bill_length_mm + flipper_length_mm,\n    data = penguins\n  )\n\nsummary(modell)\n\n\nCall:\nlm(formula = bill_depth_mm ~ bill_length_mm + flipper_length_mm, \n    data = penguins)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.0875 -1.1351 -0.0724  1.0879  4.5167 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       34.308407   1.219366  28.136  &lt; 2e-16 ***\nbill_length_mm     0.094050   0.020510   4.586 6.38e-06 ***\nflipper_length_mm -0.105956   0.007963 -13.306  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.56 on 339 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.3794,    Adjusted R-squared:  0.3757 \nF-statistic: 103.6 on 2 and 339 DF,  p-value: &lt; 2.2e-16\n\nanova(modell)\n\nAnalysis of Variance Table\n\nResponse: bill_depth_mm\n                   Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nbill_length_mm      1  73.47   73.47  30.179 7.748e-08 ***\nflipper_length_mm   1 431.04  431.04 177.049 &lt; 2.2e-16 ***\nResiduals         339 825.32    2.43                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nOm vi jämför de två olika utskrifterna från de två modellerna ser vi att förklaringsgraden (Multiple R-squared) gått från ca 5 procent i den enkla linjära modellen till ca 37 procent i den multipla linjära modellen. Detta innebär att andelen av variation i responsvariabeln som modellen förklarar har ökat markant med tillägget av en ytterligare variabel.\nVi ser också att ANOVA-tabellen fått en annan struktur med den nya variabeln. Vi får en rad per förklarande variabel men tolkningen av dessa F-test är inte samma som t-testen i koefficienttabellen. Vi kan alltså inte använda enskilda F-test från ANOVA-tabellen för att undersöka huruvida en enskild parameter är signifikant, utan vi måste fortfarande använda t-testen för denna frågeställning. För att undersöka hela modellens signifikans kommer inte heller ANOVA-tabellen innehålla den information vi behöver då F-testen är uppdelade. Istället kan vi titta på den sista raden från summary() som innehåller testvariabeln (F-statistic) och tillhörande p-värdet (p-value) för F-testet.\n\n\n\nEn regressionsmodell är inte begränsad till enbart kontinuerliga förklarande variabler. Som tidigare nämnt är variansanalys ett specialfall av regression där vi har kategoriska faktorer/variabler men om vi också inkluderar kontinuerliga faktorer/variabler inom den metodiken kommer vi i praktiken genomföra en regressionsanalys. Vi kan för enkelhetens skull utöka den multipla linjära regressionsmodellen med kategoriska variabler som dock måste genomgå en förändring för att kunna skattas korrekt i modellen.\nEn kategorisk variabel kan transformeras till indikatorvariabler som indikerar på de olika kategorierna i den ursprungliga variabeln. Antalet indikatorvariabler som skapas styrs utav antalet kategorier och antar värdet 1 eller 0. Mer exakt skapas en färre indikatorvariabel än kategorier då om alla indikatorvariabler är 0 pekar kombinationen av värden på den kategori som “saknas”.\nTill exempel skulle variabeln species som innehåller de tre pingvinarterna transformeras till indikatorvariabler enligt följande struktur: \\[\\begin{align*}\n    X_3 &=\n\\begin{cases}\n    1   ,& \\text{om pingvinen är art Chinstrap}\\\\\n    0   ,& \\text{annars}\n\\end{cases} \\\\\n    X_4 &=\n\\begin{cases}\n    1   ,& \\text{om pingvinen är art Gentoo}\\\\\n    0   ,& \\text{annars}\n\\end{cases}\n\\end{align*}\\]\nDetta kan R göra automatiskt i modellanpassningen givet att variabeln i datamaterialet är en factor.\n\nmodell &lt;- \n  lm(\n    formula = bill_depth_mm ~ bill_length_mm + flipper_length_mm + species,\n    data = penguins\n  )\n\nsummary(modell)\n\n\nCall:\nlm(formula = bill_depth_mm ~ bill_length_mm + flipper_length_mm + \n    species, data = penguins)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.15864 -0.59589 -0.07369  0.57685  2.94687 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        3.013932   1.411061   2.136   0.0334 *  \nbill_length_mm     0.144433   0.018997   7.603 2.91e-13 ***\nflipper_length_mm  0.051221   0.008466   6.050 3.85e-09 ***\nspeciesChinstrap  -1.676891   0.217380  -7.714 1.39e-13 ***\nspeciesGentoo     -6.017686   0.236344 -25.462  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9068 on 337 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.7916,    Adjusted R-squared:  0.7892 \nF-statistic: 320.1 on 4 and 337 DF,  p-value: &lt; 2.2e-16\n\nanova(modell)\n\nAnalysis of Variance Table\n\nResponse: bill_depth_mm\n                   Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nbill_length_mm      1  73.47   73.47  89.355 &lt; 2.2e-16 ***\nflipper_length_mm   1 431.04  431.04 524.214 &lt; 2.2e-16 ***\nspecies             2 548.22  274.11 333.363 &lt; 2.2e-16 ***\nResiduals         337 277.10    0.82                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nI utskriften får vi nu ytterligare två regressionskoefficienter som kopplas till respektive indikatorvariabel. Tolkningen av dessa koefficienter görs i jämförelse med referenskategorin, alltså den kategori som saknas. I detta fall blir tolkningarna i jämförelse med arten Adelie.\n\n\n\nDen sista enkla utökningen vi skulle kunna lägga till i vår linjära regressionsmodell för att anpassa responsvariabeln bättre är interaktioner, alltså ett sätt att modellera synergier mellan olika variabler. Rent matematiskt blir en interaktion en “ny” variabel där de grundvariabler som inkluderas i modellen multipliceras med varandra.\nEtt tydligt exempel på en potentiell interaktion kan ses i figur @ref(fig:flipper). Tolkningen av punktsvärmen i sin helhet antyder att sambandet mellan de två variablerna är negativt, och detta visas också i koefficienten från den första modell som skattades med fenlängd som en förklarande variabel. Detta samband verkar inte vara logiskt så det verkar vara något skumt som finns gömt i datat.\nPunkterna ligger i två stycken grupperingar och sambandet inom varje grupp ser istället positivt ut. Vi kan visualisera vad som händer om art också inkluderas i diagrammet som en grupperingsvariabel.\n\nggplot(penguins) + \n  aes(x = flipper_length_mm, y = bill_depth_mm, color = species, group = species) + \n  geom_point() +\n  theme_bw() + \n  labs(x = \"Fenlängd\", y = \"Näbbredd\") +\n  scale_color_manual(\n    \"Art\",\n    values =  c(\"darkorange\",\"purple\",\"cyan4\")\n  )\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\nSpridningsdiagram över fenlängd och näbbredd grupperat på art.\n\n\n\n\nInom vardera art verkar de två variablerna istället ha ett positivt samband. Detta fenomen kallas för Simpson’s paradox. Vi bör alltså inkludera interaktionen mellan art och fenlängd (och även interaktionen mellan art och näbblängd) i modellen.\nEtt enkelt sätt att skapa interaktioner mellan variabler i en formel är att använda * istället för +. I detta fall vill vi endast ha interaktionen mellan de två kontinuerliga variablerna och den kvalitativa variabeln species så vi kan använda () för att, likt inom vanlig algebra, skapa ett förenklat uttryck.\n\nmodell &lt;- \n  lm(\n    formula = bill_depth_mm ~ (bill_length_mm + flipper_length_mm) * species,\n    data = penguins\n  )\n\nsummary(modell)\n\n\nCall:\nlm(formula = bill_depth_mm ~ (bill_length_mm + flipper_length_mm) * \n    species, data = penguins)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.07552 -0.59413 -0.05304  0.54671  3.07596 \n\nCoefficients:\n                                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                          5.45245    2.18764   2.492  0.01317 *  \nbill_length_mm                       0.14885    0.02939   5.065 6.78e-07 ***\nflipper_length_mm                    0.03748    0.01197   3.131  0.00189 ** \nspeciesChinstrap                    -6.04709    3.75311  -1.611  0.10808    \nspeciesGentoo                      -11.62220    3.58979  -3.238  0.00133 ** \nbill_length_mm:speciesChinstrap      0.01728    0.04773   0.362  0.71751    \nbill_length_mm:speciesGentoo        -0.04917    0.04608  -1.067  0.28673    \nflipper_length_mm:speciesChinstrap   0.01819    0.02129   0.855  0.39344    \nflipper_length_mm:speciesGentoo      0.03810    0.02068   1.842  0.06630 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9064 on 333 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.7943,    Adjusted R-squared:  0.7894 \nF-statistic: 160.7 on 8 and 333 DF,  p-value: &lt; 2.2e-16\n\n\nInteraktionerna inkluderas nu på de sista raderna i utskriften enligt formatet variabel1:variabel2.",
    "crumbs": [
      "Regression",
      "intro-to-regression"
    ]
  },
  {
    "objectID": "01-regression/01-intro-to-regression.html#enkel-linjär-regression",
    "href": "01-regression/01-intro-to-regression.html#enkel-linjär-regression",
    "title": "intro-to-regression",
    "section": "",
    "text": "Låt oss börja med den enkla linjära modellen när vi har en förklarande och en responsvariabel. Från matematiken kan vi uttrycka den räta linjens ekvation med \\(y = kx + m\\) och det är samma struktur på regressionsmodellen inom statistiken, dock med några andra beteckningar.\nSkärningspunkten med y-axeln (\\(m\\)) kallar vi numera för interceptet och lutningen (\\(k\\)) heter fortfarande samma sak men mäter numera sambandet som den förklarande variabeln har med responsvariabeln. Inom regression har vi en sann modell för populationen där vi också tar hänsyn till de avvikelser som finns mellan varje enskilda observation och den räta linjen. I kapitel @ref(fig:penguins) visualiserades ett samband mellan två stycken kontinuerliga variabler likt:\n\n\nLoading required package: tidyverse\n\n\nWarning: package 'ggplot2' was built under R version 4.3.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\nLoading required package: palmerpenguins\n\n\nWarning in geom_segment(aes(x = 58, y = 20.88547 - 0.08502 * 58 + 0.1, xend = 58, : All aesthetics have length 1, but the data has 344 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\nSpridningsdiagram som visar sambandet mellan näbbens längd och bredd på pingviner.\n\n\n\n\nSom vi kan se i diagrammet kommer den anpassade linjen inte träffa exakt varje enskilda punkt och avståndet från regressionslinjen till respektive punkt, den rödstreckade linjen, kallas för modellens fel. Den sanna regressionsmodellen i populationen ser ut som följer:\n\\[\\begin{align*}\n  Y_i = \\alpha + \\beta \\cdot X_i + \\varepsilon_i\n\\end{align*}\\] där \\(\\alpha\\) är modellens intercept, \\(\\beta\\) är modellens lutning som beskriver sambandet mellan \\(X\\) och \\(Y\\), och \\(\\varepsilon_i\\) är modellens felterm, det vill säga avståndet mellan observation \\(i\\) och punkten på regressionslinjen vid samma värde på \\(X\\). \\(i\\) är ett index som indikerar på observationen.\nI praktiken kommer vi aldrig få kunskap om den sanna modellen då information som samlas in ofta är ett urval från populationen. Vi måste istället skatta modellen: \\[\\begin{align*}\n  \\hat{Y} = a + b \\cdot X_i\n\\end{align*}\\]\nFeltermen inkluderas nu inte i modellens uttryck, men vi kan skatta felet i modellen med hjälp utav dess residualer. \\[\\begin{align*}\n  e_i = Y_i - \\hat{Y}_i\n\\end{align*}\\]\nMed hjälp utav dessa residualer kan vi sedan undersöka hur bra modellen är på att anpassa datamaterialet och kontrollera modellens antaganden.\n\n\nOm vi vill skatta en regressionsmodell i R används lm(). Denna funktion skattar en linjär modell och kan användas för alla “enkla” linjära modeller som vi vill skatta. Funktionen kräver två stycken argument; formula anger vilken modell som ska skattas med angivna variabler och data anger vilket datamaterial som variablerna finns i.\n\n## Anpassar en enkel linjär regression\nmodell &lt;- \n  lm(\n    formula = bill_depth_mm ~ bill_length_mm,\n    data = penguins\n  )\n\nResultatet från funktionen är ett objekt som innehåller många olika delar som vi kan nå med andra relevanta funktioner. De två viktigaste funktionerna som innehåller majoriteten av resultaten som vi är intresserade utav är summary() och anova().\n\nsummary(modell)\n\n\nCall:\nlm(formula = bill_depth_mm ~ bill_length_mm, data = penguins)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.1381 -1.4263  0.0164  1.3841  4.5255 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    20.88547    0.84388  24.749  &lt; 2e-16 ***\nbill_length_mm -0.08502    0.01907  -4.459 1.12e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.922 on 340 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.05525,   Adjusted R-squared:  0.05247 \nF-statistic: 19.88 on 1 and 340 DF,  p-value: 1.12e-05\n\n\nUtksriften från denna funktion ger en översikt av skattningarna för parametrarna i modellen och en sammanfattande modellutvärdering. Utskriften kan delas in i fyra huvudsakliga delar:\n\nCall visar funktionen som använts för att producera utskriften,\nResiduals visar beskrivande statistik för de skattade residualerna från modellen,\nCoefficients innehåller en tabell med regressionskoefficienter (skattade parametrar), deras medelfel, och tillhörande t-test för parametrarnas signifikans,\nDe sista tre raderna visar sammanfattande mått för modellen.\n\nVi kan också beräkna en ANOVA-tabell för regression men i detta läge kan vi använda funktionen anova() för att plocka ut denna information från den redan skattade regressionsmodellen.\n\nanova(modell)\n\nAnalysis of Variance Table\n\nResponse: bill_depth_mm\n                Df  Sum Sq Mean Sq F value   Pr(&gt;F)    \nbill_length_mm   1   73.47  73.473  19.884 1.12e-05 ***\nResiduals      340 1256.36   3.695                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nDenna tabell innehåller de olika källorna av variation som modellen innehåller, den förklarande variabeln och felet. Vi får också ett tillhörande F-test för modellens totala anpassning i denna utskrift till skillnad från den tidigare koefficient-tabellen.",
    "crumbs": [
      "Regression",
      "intro-to-regression"
    ]
  },
  {
    "objectID": "01-regression/01-intro-to-regression.html#multipel-linjär-regression",
    "href": "01-regression/01-intro-to-regression.html#multipel-linjär-regression",
    "title": "intro-to-regression",
    "section": "",
    "text": "Om vi har ett datamaterial med flera potentiella förklarande variabler kan vi skatta en multipel linjär regressionsmodell där alla variablerna inkluderas samtidigt. Detta medför att vi får en modell som, på gott och ont, tar hänsyn till eventuella påverkan mellan de olika förklarande variablerna och ger möjligheten att lägga till mer komplexa samband, till exempel interaktioner.\nFormellt ser den generella regressionsmodellen ut som: \\[\\begin{align*}\n  Y_j = \\alpha + \\beta_1 \\cdot X_{1, j} + \\beta_2 \\cdot X_{2, j} + \\hdots + \\beta_m \\cdot X_{m, j} + \\varepsilon_j\n\\end{align*}\\] där index \\(j\\) nu är observationsindex och \\(m\\) anger hur många förklarande variabler som inkluderas i modellen.\nTill exempel skulle vi vara intresserade av att lägga till ytterligare en variabel i vår modell över pingvinerna, flipper_length_mm.\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\nSpridningsdiagram som visar på sambandet mellan fenlängd och näbbredd\n\n\n\n\nDen utökade modellen anges i formula med ett + mellan de olika förklarande variablerna som ska inkluderas.\n\nmodell &lt;- \n  lm(\n    formula = bill_depth_mm ~ bill_length_mm + flipper_length_mm,\n    data = penguins\n  )\n\nsummary(modell)\n\n\nCall:\nlm(formula = bill_depth_mm ~ bill_length_mm + flipper_length_mm, \n    data = penguins)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.0875 -1.1351 -0.0724  1.0879  4.5167 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       34.308407   1.219366  28.136  &lt; 2e-16 ***\nbill_length_mm     0.094050   0.020510   4.586 6.38e-06 ***\nflipper_length_mm -0.105956   0.007963 -13.306  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.56 on 339 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.3794,    Adjusted R-squared:  0.3757 \nF-statistic: 103.6 on 2 and 339 DF,  p-value: &lt; 2.2e-16\n\nanova(modell)\n\nAnalysis of Variance Table\n\nResponse: bill_depth_mm\n                   Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nbill_length_mm      1  73.47   73.47  30.179 7.748e-08 ***\nflipper_length_mm   1 431.04  431.04 177.049 &lt; 2.2e-16 ***\nResiduals         339 825.32    2.43                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nOm vi jämför de två olika utskrifterna från de två modellerna ser vi att förklaringsgraden (Multiple R-squared) gått från ca 5 procent i den enkla linjära modellen till ca 37 procent i den multipla linjära modellen. Detta innebär att andelen av variation i responsvariabeln som modellen förklarar har ökat markant med tillägget av en ytterligare variabel.\nVi ser också att ANOVA-tabellen fått en annan struktur med den nya variabeln. Vi får en rad per förklarande variabel men tolkningen av dessa F-test är inte samma som t-testen i koefficienttabellen. Vi kan alltså inte använda enskilda F-test från ANOVA-tabellen för att undersöka huruvida en enskild parameter är signifikant, utan vi måste fortfarande använda t-testen för denna frågeställning. För att undersöka hela modellens signifikans kommer inte heller ANOVA-tabellen innehålla den information vi behöver då F-testen är uppdelade. Istället kan vi titta på den sista raden från summary() som innehåller testvariabeln (F-statistic) och tillhörande p-värdet (p-value) för F-testet.",
    "crumbs": [
      "Regression",
      "intro-to-regression"
    ]
  },
  {
    "objectID": "01-regression/01-intro-to-regression.html#kategoriska-variabler",
    "href": "01-regression/01-intro-to-regression.html#kategoriska-variabler",
    "title": "intro-to-regression",
    "section": "",
    "text": "En regressionsmodell är inte begränsad till enbart kontinuerliga förklarande variabler. Som tidigare nämnt är variansanalys ett specialfall av regression där vi har kategoriska faktorer/variabler men om vi också inkluderar kontinuerliga faktorer/variabler inom den metodiken kommer vi i praktiken genomföra en regressionsanalys. Vi kan för enkelhetens skull utöka den multipla linjära regressionsmodellen med kategoriska variabler som dock måste genomgå en förändring för att kunna skattas korrekt i modellen.\nEn kategorisk variabel kan transformeras till indikatorvariabler som indikerar på de olika kategorierna i den ursprungliga variabeln. Antalet indikatorvariabler som skapas styrs utav antalet kategorier och antar värdet 1 eller 0. Mer exakt skapas en färre indikatorvariabel än kategorier då om alla indikatorvariabler är 0 pekar kombinationen av värden på den kategori som “saknas”.\nTill exempel skulle variabeln species som innehåller de tre pingvinarterna transformeras till indikatorvariabler enligt följande struktur: \\[\\begin{align*}\n    X_3 &=\n\\begin{cases}\n    1   ,& \\text{om pingvinen är art Chinstrap}\\\\\n    0   ,& \\text{annars}\n\\end{cases} \\\\\n    X_4 &=\n\\begin{cases}\n    1   ,& \\text{om pingvinen är art Gentoo}\\\\\n    0   ,& \\text{annars}\n\\end{cases}\n\\end{align*}\\]\nDetta kan R göra automatiskt i modellanpassningen givet att variabeln i datamaterialet är en factor.\n\nmodell &lt;- \n  lm(\n    formula = bill_depth_mm ~ bill_length_mm + flipper_length_mm + species,\n    data = penguins\n  )\n\nsummary(modell)\n\n\nCall:\nlm(formula = bill_depth_mm ~ bill_length_mm + flipper_length_mm + \n    species, data = penguins)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.15864 -0.59589 -0.07369  0.57685  2.94687 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        3.013932   1.411061   2.136   0.0334 *  \nbill_length_mm     0.144433   0.018997   7.603 2.91e-13 ***\nflipper_length_mm  0.051221   0.008466   6.050 3.85e-09 ***\nspeciesChinstrap  -1.676891   0.217380  -7.714 1.39e-13 ***\nspeciesGentoo     -6.017686   0.236344 -25.462  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9068 on 337 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.7916,    Adjusted R-squared:  0.7892 \nF-statistic: 320.1 on 4 and 337 DF,  p-value: &lt; 2.2e-16\n\nanova(modell)\n\nAnalysis of Variance Table\n\nResponse: bill_depth_mm\n                   Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nbill_length_mm      1  73.47   73.47  89.355 &lt; 2.2e-16 ***\nflipper_length_mm   1 431.04  431.04 524.214 &lt; 2.2e-16 ***\nspecies             2 548.22  274.11 333.363 &lt; 2.2e-16 ***\nResiduals         337 277.10    0.82                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nI utskriften får vi nu ytterligare två regressionskoefficienter som kopplas till respektive indikatorvariabel. Tolkningen av dessa koefficienter görs i jämförelse med referenskategorin, alltså den kategori som saknas. I detta fall blir tolkningarna i jämförelse med arten Adelie.",
    "crumbs": [
      "Regression",
      "intro-to-regression"
    ]
  },
  {
    "objectID": "01-regression/01-intro-to-regression.html#interaktioner",
    "href": "01-regression/01-intro-to-regression.html#interaktioner",
    "title": "intro-to-regression",
    "section": "",
    "text": "Den sista enkla utökningen vi skulle kunna lägga till i vår linjära regressionsmodell för att anpassa responsvariabeln bättre är interaktioner, alltså ett sätt att modellera synergier mellan olika variabler. Rent matematiskt blir en interaktion en “ny” variabel där de grundvariabler som inkluderas i modellen multipliceras med varandra.\nEtt tydligt exempel på en potentiell interaktion kan ses i figur @ref(fig:flipper). Tolkningen av punktsvärmen i sin helhet antyder att sambandet mellan de två variablerna är negativt, och detta visas också i koefficienten från den första modell som skattades med fenlängd som en förklarande variabel. Detta samband verkar inte vara logiskt så det verkar vara något skumt som finns gömt i datat.\nPunkterna ligger i två stycken grupperingar och sambandet inom varje grupp ser istället positivt ut. Vi kan visualisera vad som händer om art också inkluderas i diagrammet som en grupperingsvariabel.\n\nggplot(penguins) + \n  aes(x = flipper_length_mm, y = bill_depth_mm, color = species, group = species) + \n  geom_point() +\n  theme_bw() + \n  labs(x = \"Fenlängd\", y = \"Näbbredd\") +\n  scale_color_manual(\n    \"Art\",\n    values =  c(\"darkorange\",\"purple\",\"cyan4\")\n  )\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\nSpridningsdiagram över fenlängd och näbbredd grupperat på art.\n\n\n\n\nInom vardera art verkar de två variablerna istället ha ett positivt samband. Detta fenomen kallas för Simpson’s paradox. Vi bör alltså inkludera interaktionen mellan art och fenlängd (och även interaktionen mellan art och näbblängd) i modellen.\nEtt enkelt sätt att skapa interaktioner mellan variabler i en formel är att använda * istället för +. I detta fall vill vi endast ha interaktionen mellan de två kontinuerliga variablerna och den kvalitativa variabeln species så vi kan använda () för att, likt inom vanlig algebra, skapa ett förenklat uttryck.\n\nmodell &lt;- \n  lm(\n    formula = bill_depth_mm ~ (bill_length_mm + flipper_length_mm) * species,\n    data = penguins\n  )\n\nsummary(modell)\n\n\nCall:\nlm(formula = bill_depth_mm ~ (bill_length_mm + flipper_length_mm) * \n    species, data = penguins)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.07552 -0.59413 -0.05304  0.54671  3.07596 \n\nCoefficients:\n                                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                          5.45245    2.18764   2.492  0.01317 *  \nbill_length_mm                       0.14885    0.02939   5.065 6.78e-07 ***\nflipper_length_mm                    0.03748    0.01197   3.131  0.00189 ** \nspeciesChinstrap                    -6.04709    3.75311  -1.611  0.10808    \nspeciesGentoo                      -11.62220    3.58979  -3.238  0.00133 ** \nbill_length_mm:speciesChinstrap      0.01728    0.04773   0.362  0.71751    \nbill_length_mm:speciesGentoo        -0.04917    0.04608  -1.067  0.28673    \nflipper_length_mm:speciesChinstrap   0.01819    0.02129   0.855  0.39344    \nflipper_length_mm:speciesGentoo      0.03810    0.02068   1.842  0.06630 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9064 on 333 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.7943,    Adjusted R-squared:  0.7894 \nF-statistic: 160.7 on 8 and 333 DF,  p-value: &lt; 2.2e-16\n\n\nInteraktionerna inkluderas nu på de sista raderna i utskriften enligt formatet variabel1:variabel2.",
    "crumbs": [
      "Regression",
      "intro-to-regression"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistical Applications in R",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\n\n1 + 1\n\n[1] 2"
  }
]