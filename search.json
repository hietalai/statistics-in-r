[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistisk teori och tillämpningar i R",
    "section": "",
    "text": "Inledning\nOm du har några kommentarer, synpunkter eller vill meddela något fel? Maila mig på isak.hietala@liu.se."
  },
  {
    "objectID": "01-regression/00-intro-regression.html",
    "href": "01-regression/00-intro-regression.html",
    "title": "1  Introduktion till regression",
    "section": "",
    "text": "1.1 Den linjära regressionsmodellen\nDet finns många olika sorters modeller inom regressionsanalys, men i det allra enklaste fallet anpassas en linjär modell där variablernas samband antas enkelriktat och konstant. \\[\nY = \\beta_0 + \\beta_1 \\cdot X + E\n\\tag{1.1}\\]\ndär:\nOm flera förklarande variabler antas påverka responsvariabeln utökas den linjära modellen med flera \\(\\beta_j\\), en för varje förklarande variabel \\(X_j\\). Senare kapitel kommer titta närmare på utökningar av denna linjära modell.",
    "crumbs": [
      "Regressionsanalys",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduktion till regression</span>"
    ]
  },
  {
    "objectID": "01-regression/00-intro-regression.html#den-linjära-regressionsmodellen",
    "href": "01-regression/00-intro-regression.html#den-linjära-regressionsmodellen",
    "title": "1  Introduktion till regression",
    "section": "",
    "text": "\\(Y\\) är den beroende/respons- variabeln som antas påverkas av \\(X\\).\n\\(X\\) är den oberoende/förklarande variabeln som antas påverka \\(Y\\).\n\\(\\beta_0\\) är modellens intercept där linjen skär y-axeln när \\(X = 0\\).\n\\(\\beta_1\\) är lutningen som beskriver det enkelriktade samband mellan \\(X\\) och \\(Y\\). Mer specifikt beskriver parametern förändringen i \\(Y\\) när \\(X\\) ökar med en enhet.\n\\(E\\) är modellens felterm, avståndet mellan det observerade värdet på \\(Y\\) och modellens skattade värde \\(\\hat{Y}\\).",
    "crumbs": [
      "Regressionsanalys",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduktion till regression</span>"
    ]
  },
  {
    "objectID": "01-regression/00-intro-regression.html#sec-model-assumptions",
    "href": "01-regression/00-intro-regression.html#sec-model-assumptions",
    "title": "1  Introduktion till regression",
    "section": "1.2 Modellens antaganden",
    "text": "1.2 Modellens antaganden\nSyftet med en modell är att ge en lämplig förenkling av verkligheten. En linjär regressionsmodell kan vara en lämplig förenkling av ett samband om följande antaganden uppfylls:\n\natt det för varje \\(X\\) finns en slumpvariabel \\(Y\\) med ett ändligt medelvärde och varians,\nobservationerna är oberoende av varandra,\nmedelvärdet, \\(\\mu_{Y|X}\\), kan modelleras linjärt,\nvariansen för \\(Y\\) är lika för alla värden av \\(X\\), \\(\\sigma^2_{Y|X} \\equiv \\sigma^2\\),\nslumpvariabeln \\(Y\\) är normalfördelad för alla värden av \\(X\\).\n\nVi kan sammanfatta majoriteten av dessa antaganden med: \\[\n   Y|X \\overset{\\mathrm{iid}}{\\sim} N(\\mu_{Y|X}, \\sigma^2_{Y|X})\n\\] där \\(\\mathrm{iid}\\) betyder “independent and identically distributed” motsvarande antagande 2.\n\n\n\n\n\n\nViktigt\n\n\n\nDet finns inget antagande om att \\(Y \\sim N(\\mu_Y, \\sigma^2_Y)\\)! Alla antaganden för en linjär regressionsmodell fokuserar på att vi med hjälp av \\(X\\) har en normalfördelad slumpvariabel \\(Y\\).\n\n\nOm det tredje antagandet uppfylls kan vi modellera väntevärdet av \\(Y|X\\) med den linjära modellen: \\[\n  E[Y|X] = \\beta_0 + \\beta_1 \\cdot X\n\\tag{1.2}\\] så att: \\[\nY|X \\overset{\\mathrm{iid}}{\\sim} N(\\beta_0 + \\beta_1 \\cdot X, \\sigma^2_{Y|X})\n\\]\nTill skillnad från Ekvation 1.1 saknar Ekvation 1.2 modellens felterm på grund av att vi nu modellerar endast medelvärdet av slumpvariabelns fördelning, \\(\\mu_{Y|X}\\). Osäkerheten runtomkring medelvärdet är variansen av fördelningen.\nNär vi modellerar varje enskilda observation inkluderas \\(E\\) vilket innebär att vi kan flytta modellens antaganden från \\(Y|X\\) till \\(E\\). \\[\n  E \\overset{\\mathrm{iid}}{\\sim} N(0, \\sigma^2)\n\\tag{1.3}\\]\nDenna omskrivning ger oss en bra utgångspunkt att utvärdera lämpligheten av en anpassad modell.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigur 1.2: Ej lämplig flygplansmodell, (”Create a model of a commercial airplane where some parts are taken from a car or a boat” 2024)\n\n\n\nOm en flygplansmodell ser endast till viss del ut som ett flygplan kommer modellen inte vara lämplig att använda för att förstå eller förenkla verkligheten. Detsamma gäller för regressionsmodeller; om modellen inte uppfyller dess antaganden riskerar slutsatser som dras inte stämma överens med verkligheten.",
    "crumbs": [
      "Regressionsanalys",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduktion till regression</span>"
    ]
  },
  {
    "objectID": "01-regression/00-intro-regression.html#studier-och-andra-variabler",
    "href": "01-regression/00-intro-regression.html#studier-och-andra-variabler",
    "title": "1  Introduktion till regression",
    "section": "1.3 Studier och andra variabler",
    "text": "1.3 Studier och andra variabler\nEn regressionsmodell behöver nödvändigtvis inte beskriva ett “orsak-och-verkan” samband, eller som vi brukar benämna det, ett kausalt samband. Samband kan ibland uppstå utav ren slump där det inte finns någon logisk koppling mellan variablerna. Denna typ av samband benämns som korrelationssamband. Trots att korrelationssamband rent matematiskt beskriver en relation mellan den ena variabeln och den andra, är det i vissa fall inte lämpligt eller relevant att använda eller tolka modellen i verkligheten.\n\n\n\n\n\n\n\n\nFigur 1.3: Sambandet mellan antalet filmer som Nicolas Cage medverkade i och antalet dödsfall av drunkning mellan 1999 och 2009 i USA (”CDC - NCHS - National Center for Health Statistics — cdc.gov”; ”Nicolas Cage | Actor, Producer, Director — imdb.com”)\n\n\n\n\n\nFigur 1.3 uppvisar ett exempel på korrelationssamband där de två variablerna inte har någon logisk koppling till varandra utan endast har observerats ha en positiv korrelation. Att beskriva detta samband skulle inte ge någon information om verkligheten så en viktig del av regressionsanalys är att bedöma lämpligheten och relevansen av utvalda variabler. Rent matematiskt kan inte heller en regressionsmodell urskilja mellan kausala eller korrelationssamband vilket innebär att vi som analytiker måste ta hänsyn till vilken sorts data och hur data har samlats in för att använda och tolka modellerna på rätt sätt.\nExemplet i figuren är insamlad som en observationsstudie där mätvärden (antal dödsfall och filmer) på enheterna (år) har observerats från olika registerdata. Vi har inte kunnat styra vilken relation dessa variabler har till varandra och studien i sig har inte tagit hänsyn till någon specifik orsak och verkan mellan de två. Vi kan därför endast dra slutsatser om korrelationssamband från en observationsstudie, vi kan säga att desto fler filmer Nicolas Cage medverkar i medför ett större antal dödsfall, vilket egentligen inte är relevant, men vi kan inte säga något om den kausala effekten.\nFör att kunna dra slutsatser om kausala samband behöver vi genomföra en experimentell studie där vi styr vilka mätvärden som enheter får eller har och responsvariablen antas vara en direkt effekt från de förklarande variablerna. Medicinska studier, till exempel studier om Covid-vaccinets effektivitet på att motverka en infektion, är typiska exempel på experimentella studier där en förklarande variabel (dos) ges till vissa grupper av enheter där andra påverkande effekter kontrolleras för att justera den förklarande variabelns verkliga påverkan.\n\n1.3.1 Kontrollvariabler\nI en observationsstudie kan vi ibland observera kontrollvariabler som kan justera den förklarande variabelns faktiska påverkan men det är främst i experimentella studier som dessa typer av variabler kan användas. Okända variabler kallas för confounding-effekter och antas påverka både den förklarande och responsvariabeln.\nI följande två figurer visas den huvudsakliga förklarande variabeln och den valda responsvariabeln med ovaler. De kända (heldragen) och okända (streckade) kontrollvariablerna visas som rektanglar.\n\n\n\n\n\n\n\n\n\n\nG\n\n\n\nOkänd\n\nOkänd\n\n\n\nFilmer\n\nFilmer\n\n\n\nOkänd-&gt;Filmer\n\n\n\n\n\nDödsfall\n\nDödsfall\n\n\n\nOkänd-&gt;Dödsfall\n\n\n\n\n\nFilmer-&gt;Dödsfall\n\n\n\n\n\n\n\n\n\n\n\n\nFigur 1.4: Exempel på relationen mellan variabler i en observationsstudie.\n\n\n\nEftersom sambandet mellan filmer och dödsfall förmodligen endast uppkommit av slumpen kan det finnas andra okända variabler som påverkar de båda.\n\n\n\n\n\n\n\n\n\n\nG\n\n\n\nÅlder\n\nÅlder\n\n\n\nBlodtryck\n\nBlodtryck\n\n\n\nÅlder-&gt;Blodtryck\n\n\n\n\n\nKön\n\nKön\n\n\n\nKön-&gt;Blodtryck\n\n\n\n\n\nMedicin\n\nMedicin\n\n\n\nMedicin-&gt;Blodtryck\n\n\n\n\n\n\n\n\n\n\n\n\nFigur 1.5: Exempel på relationen mellan variabler i en experimentell studie.\n\n\n\nEffekten av en medicin på blodtryck kan också påverkas av personens ålder och kön (Gu m.fl. 2008) vilka inkluderas i modellen för att isolera den förklarande variabelns effekt. I alla dessa exempel är det endast en effekt som är av intresse att undersöka, trots att modellen innehåller flera variabler och tillhörande lutningsparametrar.\n\n\n\n\n”CDC - NCHS - National Center for Health Statistics — cdc.gov”. https://www.cdc.gov/nchs/.\n\n\n”Create a model of a commercial airplane where some parts are taken from a car or a boat”. 2024. OpenAI. https://chat.openai.com/chat.\n\n\nGu, Qiuping, Vicki L. Burt, Ryne Paulose-Ram, och Charles F. Dillon. 2008. ”Gender Differences in Hypertension Treatment, Drug Utilization Patterns, and Blood Pressure Control Among US Adults With Hypertension: Data From the National Health and Nutrition Examination Survey 1999–2004”. American Journal of Hypertension 21 (7): 789–98. https://doi.org/10.1038/ajh.2008.185.\n\n\n”Nicolas Cage | Actor, Producer, Director — imdb.com”. https://www.imdb.com/name/nm0000115/.",
    "crumbs": [
      "Regressionsanalys",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduktion till regression</span>"
    ]
  },
  {
    "objectID": "01-regression/01-explorative-analysis.html",
    "href": "01-regression/01-explorative-analysis.html",
    "title": "2  Utforska samband",
    "section": "",
    "text": "2.1 Pingviner vid Antarktis\nÅterkommande i underlaget kommer ett insamlat datamaterial från ett forskarteam vid Antarktis användas. Teamet har mellan 2007 och 2009 samlat in information om 333 pingviner vid tre öar runtomkring Palmer Research Station. Datamaterialet kan hämtas via paketet palmerpenguins (Horst, Hill, och Gorman 2020) och laddas in i R via följande kod:\n# Laddar paketet med datamaterialet\nrequire(palmerpenguins)\n\n# Filtrerar bort observationer med saknade värden\npenguins &lt;- \n  penguins %&gt;% \n  filter(!is.na(sex))\nVi kan titta närmare på ett urval av datamaterialet i Tabell 2.1.\nVisa kod\n# Generera en formaterad tabell med hjälp av kable()\npenguins %&gt;% \n  slice_head(n = 5) %&gt;% \n  kable() %&gt;% \n  kable_styling(\"striped\")\n\n\n\n\nTabell 2.1: Urval av observationer från datamaterialet.\n\n\n\n\n \n  \n    species \n    island \n    bill_length_mm \n    bill_depth_mm \n    flipper_length_mm \n    body_mass_g \n    sex \n    year \n  \n \n\n  \n    Adelie \n    Torgersen \n    39.1 \n    18.7 \n    181 \n    3750 \n    male \n    2007 \n  \n  \n    Adelie \n    Torgersen \n    39.5 \n    17.4 \n    186 \n    3800 \n    female \n    2007 \n  \n  \n    Adelie \n    Torgersen \n    40.3 \n    18.0 \n    195 \n    3250 \n    female \n    2007 \n  \n  \n    Adelie \n    Torgersen \n    36.7 \n    19.3 \n    193 \n    3450 \n    female \n    2007 \n  \n  \n    Adelie \n    Torgersen \n    39.3 \n    20.6 \n    190 \n    3650 \n    male \n    2007\nFrån tabellen kan vi utläsa följande variabler:\nVi kommer fokusera på näbblängden som vår responsvariabel i efterföljande exempel. I och med att datamaterialet är en observationsstudie kommer vi inte kunna dra slutsatser om kausala samband, utan kan endast undersöka korrelationssamband mellan pingvinernas olika egenskaper.",
    "crumbs": [
      "Regressionsanalys",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Utforska samband</span>"
    ]
  },
  {
    "objectID": "01-regression/01-explorative-analysis.html#sec-example-data",
    "href": "01-regression/01-explorative-analysis.html#sec-example-data",
    "title": "2  Utforska samband",
    "section": "",
    "text": "Viktigt\n\n\n\nSom en del av utforskningen kan vi identifiera saknade värden på vissa variabler och väljer att filtrera bort dessa variabler i just detta exempel. Hantering av saknade värden är ett stort fält inom statistiken och det finns flertalet metoder som kan imputera (skatta det saknade värdet) värden så att vi inte behöver ta bort hela observationer från undersökningen.\nEn enkel imputeringsmetod är medelvärdesimputering där vi byter ut det saknade värdet med medelvärdet av de övriga mätvärdena eller typvärdet ifall en kvalitativ variabel ska imputeras. I praktiken används mer avancerade metoder som kan tillämpas i många olika fall där vi också tar hänsyn till annan information om observationerna.\n\n\n\n\n\n\nspecies: Pingvinens art mäts som en kvalitativ variabel och vi kan inte säga att en art är “bättre” eller “större” än någon annan. Vi kan alltså inte rangordna kategorierna och denna variabel följer då en nominalskala.\nisland: Vilken ö pingvinen har befunnit sig på vid mättidpunkten är också en kvalitativ variabel som inte går att rangordna. Därav följer även denna variabel en nominalskala.\nbill_length_mm: En kvantitativ variabel som mäter längden på näbben i millimeter (mm). Längd är en typisk variabel som följer en kvotskala eftersom det finns en tydlig nollpunkt.\nbill_depth_mm: Mäter näbbens djup i millimeter och följer samma resonemang som näbblängden.\nflipper_length_mm: Ytterligare en variabel som mäter en längd, nu längden av pingvinens fena. Samma resonemang som näbbens olika längder kan föras.\nbody_mass_g: Vikt av pingvinen mätt i gram. Även vikt har en tydlig nollpunkt och variabeln anses vara kvantitativ och följa en kvotskala.\nsex: Pingvinens biologiska kön vilket är en kvalitativ variabel som inte går att rangordna, nominalskala.\nyear: Denna variabel är lite svårare att bedöma då den mäter året då pingvinen är mätt som en numerisk variabel (heltal så R har sparat det som en int), men variabeln i sig behöver inte bedömas vara kvantitativ i denna kontext. Vi går inte in vidare på detta utan för enkelhetens skull kan vi säga att eftersom det går att beräkna differenser mellan åren, (det är 1 år mellan 2007 och 2008) men ingen tydlig nollpunkt finns på skalan, så kan vi anse denna variabel vara en kvantitativ variabel som följer en intervallskala.",
    "crumbs": [
      "Regressionsanalys",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Utforska samband</span>"
    ]
  },
  {
    "objectID": "01-regression/01-explorative-analysis.html#visualisera-responsvariabeln",
    "href": "01-regression/01-explorative-analysis.html#visualisera-responsvariabeln",
    "title": "2  Utforska samband",
    "section": "2.2 Visualisera responsvariabeln",
    "text": "2.2 Visualisera responsvariabeln\nSom ett första steg i den explorativa analysen kan vi visualisera fördelningen av responsvariabeln med ett histogram.\n\n\nVisa kod\nggplot(penguins) + aes(x = bill_length_mm) + \n  geom_histogram(bins = 30, fill = \"steelblue\", color = \"black\") +\n  theme_bw() +\n  labs(x = \"Näbblängd (mm)\", y = \"Antal\")\n\n\n\n\n\n\n\n\nFigur 2.1: Histogram över näbblängdens fördelning\n\n\n\n\n\nFigur 2.1 ger oss en bild av variabelns egenskaper och ifall materialet innehåller några extremvärden som kan vara svåra att plocka upp med en modell. Näbblängden verkar ha en bimodal struktur med två masscentrum vid 38-40 och 50 mm. Vi ser att majoriteten av observationerna ligger mellan ca 35-52 mm men det finns också enstaka observationer omkring 58-60 mm som verkar vara något avvikande stora näbbar.\nAtt fördelningen inte ser normalfördelad ut spelar ingen roll då vi måste titta på fördelningen av responsvariabeln med avseende på de förklarande variablerna för att kontrollera en regressionsmodells antaganden.",
    "crumbs": [
      "Regressionsanalys",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Utforska samband</span>"
    ]
  },
  {
    "objectID": "01-regression/01-explorative-analysis.html#sec-pairwise-visualization",
    "href": "01-regression/01-explorative-analysis.html#sec-pairwise-visualization",
    "title": "2  Utforska samband",
    "section": "2.3 Parvisa samband",
    "text": "2.3 Parvisa samband\nDatamaterialet innehåller ett flertal potentiella förklarande variabler som skulle kunna inkluderas i en modell. Beroende på hur en undersökning gått till kan variabler väljas bort om de inte anses ha ett logiskt samband med responsvariabeln, t.ex. id-variabler är inte relevanta att undersöka. I vårt exempel finns en variabel som beskriver årtal vilket vi i ett första skede kan anta inte har något logiskt samband med näbblängden. Då återstår sex andra variabler som skulle kunna inkluderas i modellanpassningen.\n\n2.3.1 Kvantitativa förklarande variabler\nFör kvantitativa förklarande variabler kan vi skapa ett spridningsdiagram där varje observation representeras med en punkt. Den förklarande variabeln placeras på x-axeln och responsvariabeln placeras på y-axeln. Med hjälp av punktsvärmen i spridningsdiagrammet kan vi få information om sambandet mellan de två variablerna. Det är fyra huvudsakliga punkter som vi fokuserar på:\n\nÄr sambandet linjärt?\nÄr sambandet positivt eller negativt?\nÄr sambandet starkt eller svagt?\nFörekommer det några extremvärden?\n\n\n\nVisa kod\nggplot(penguins) + aes(x = body_mass_g, y = bill_length_mm) +\n  geom_point(color = \"steelblue\") + \n  theme_bw() + \n  labs(x = \"Kroppsvikt (g)\", y = \"Näbblängd (mm)\")\n\n\n\n\n\n\n\n\nFigur 2.2: Spridningsdiagram som visar sambandet mellan kroppsvikt och näbblängd\n\n\n\n\n\nFigur 2.2 visar att sambandet ser till största del linjärt ut då en konstant förändring (ökning) av kroppsvikt leder till en konstant förändring (ökning) av näbblängden. Majoriteten av punkterna verkar följa denna trend, vilket tyder på ett relativt starkt samband, dock finns det ett flertal observationer (markerade i Figur 2.3) som avviker från detta. Dessa observationer har en lägre kroppsvikt men samma näbblängd som pingviner med en större kroppsvikt och påverkar styrkan av sambandet.\n\n\n\n\n\n\n\n\nFigur 2.3: Spridningsdiagram med markerat område i cirkeln\n\n\n\n\n\nVi kan beräkna Pearson’s korrelationskoefficient (\\(r\\)) för att inte behöva förlita oss på den subjektiva tolkningen av styrkan.1 Denna koefficient mäter styrkan på det linjära sambandet mellan två kvantitativa variabler och är ett lämpligt mått i just detta fall. Ett värde nära 0 tyder på inget eller ett svagt samband medan värden nära -1 eller +1 tyder på ett starkt negativt respektive positivt samband.\n\\[\nr = 0.589\n\\]\nDå korrelationskoefficienten är nära 0.6 tyder det på att sambandet är måttligt starkt.\n\n\n\n\n\n\nViktigt\n\n\n\nOm spridningsdiagrammet uppvisar ett icke-linjärt och icke-monotont (konstant) samband kommer koefficienten inte beskriva sambandets styrka på rätt sätt. Det är lätt hänt att korrelationskoefficienten används som den enda utforskande metoden då den är enkel att beräkna för flera olika par av variabler, men den kan ofta missa relevant information. Visualisering möjliggör identifieringen av komplexa samband som ofta medför att vi behöver hantera modellen på olika sätt.\n\n\nFigur 2.2 visar också vissa observationer som skulle kunna anses vara extremvärden. Till exempel skulle \\(\\{x = ~2700, y = ~47\\}\\) och \\(\\{x = ~3700, y = ~58\\}\\) vara observationer som avviker extremt från det tilltänkta sambandet och andra observationer. Detaljerad analys av extremvärden lämnar vi till senare kapitel, men i ett utforskande syfte noterar vi att vi kan ha observationer som kommer påverka modellanpassningen.\nSammanfattningsvis kan vi säga att sambandet mellan kroppsvikt och näbblängd är:\n\nlinjärt,\npositivt,\nmåttligt starkt,\nmed ev. några extremvärden.\n\nNär vi ska skapa vår första modell kommer det nog räcka med att inkludera en enkel \\(\\beta_1 \\cdot \\text{kroppsvikt}\\) term i modelleringen.\n\n2.3.1.1 Övriga kvantitativa variabler\nSamma utforskning bör genomföras för alla par av variabler, i detta fall också näbbredd och fenlängd:\n\n\n\n\n\n\n\n\nFigur 2.4: Spridningsdiagram som visar sambandet mellan näbbredd och näbblängd\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigur 2.5: Spridningsdiagram som visar sambandet mellan fenlängd och näbblängd\n\n\n\n\n\nBåda variablerna ser ut att ha linjärt samband med responsvariabeln. Figur 2.4 tyder på att sambandet mellan näbbredd och näbblängd är svagt negativt (\\(r = -0.229\\)) då punkterna är mycket utspridda medan Figur 2.5 tyder på ett lite starkare positivt samband (\\(r = 0.653\\)) i linje med Figur 2.2.\nEtt nytt fenomen som vi kan se i Figur 2.4 är att vi verkar ha flera punktsvärmar som var och en har ett positivt samband trots att vi tolkade det övergripande sambandet som svagt negativt. Om vi endast hade beräknat korrelationskoefficienten hade detta fenomen undgått vår analys. Figur 2.6 är ett exempel på Simpson’s Paradox som vi kommer undersöka närmare senare i detta underlag.\n\n\n\n\n\n\n\n\nFigur 2.6: Grupperingar av observationer\n\n\n\n\n\n\n\n\n2.3.2 Kvalitativa förklarande variabler\nVi kan inte använda spridningsdiagram för att visualisera sambandet mellan kvalitativa förklarande variabler och en kontinuerlig responsvariabel. Vi behöver istället använda visualiseringar som tar hänsyn till den kvalitativa skalan, vanligtvis ordinal eller nominalskala. Det finns flera olika sätt att visualisera fördelningen av responsvariabeln för de olika nivåerna av den förklarande, till exempel grupperade histogram eller lådagram, men en typ av visualisering som visar detaljerna i fördelningen är ett fioldiagram. Ett fioldiagram består utav en spegling av ett densitetsdiagram, där områden med många observationer har en större yta under kurvan.\nVia ggplot2 kan vi skapa ett sådant diagram genom geom_violin():\n\n\nVisa kod\nggplot(penguins) + \n  aes(x = species, y = bill_length_mm) +\n  geom_violin(fill = \"steelblue\") + \n  theme_bw() + \n  labs(x = \"Art\", y = \"Näbblängd (mm)\")\n\n\n\n\n\n\n\n\nFigur 2.7: Fördelningen av näbblängd uppdelat på art\n\n\n\n\n\nFigur 2.7 visar att Adelie-pingviner överlag har en kortare näbblängd jämfört med Chinstrap och Gentoo då fördelningens mittpunkt förhåller sig kring 38-40 mm. Chinstrap-pingviner har en något större andel pingviner med en längd större än 50 mm medan Gentoo har en större andel med en längd mindre än 50 mm.\n\n\n\n\n\n\n\n\nFigur 2.8: Fördelningen av näbblängd uppdelat på kön\n\n\n\n\n\nFigur 2.8 har en liten annorlunda form, med två stora massor för respektive kategori. Här har vi förmodligen en indikation på att kön inom de olika arterna har en påverkan och att hanar generellt har en större näbblängd än motsvarande honor av samma art.\n\n\n\n\n\n\n\n\nFigur 2.9: Fördelningen av näbblängd uppdelat på ö\n\n\n\n\n\nFigur 2.9 antyder att pingviner på ön Torgersen har en mindre näbblängd än vid övriga öar, men här behöver vi resonera huruvida denna variabel faktiskt beskriver sambandet eller om det finns något annat fenomen som kan förklara samma sak, till exempel om en ö endast har pingviner av en viss art. Mer om dessa sorters samband kommer senare i underlaget.\nSlutsatsen från dessa visualiseringar är att det verkar finnas ett samband mellan art och kön med näbblängd och de två variablerna bör inkluderas i modellen. Vi behöver nu fundera på hur vi på ett lämpligt sätt kan inkludera en kvalitativ variabel innehållande text i en matematisk modell som kräver siffror.",
    "crumbs": [
      "Regressionsanalys",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Utforska samband</span>"
    ]
  },
  {
    "objectID": "01-regression/01-explorative-analysis.html#sec-exercise-explore",
    "href": "01-regression/01-explorative-analysis.html#sec-exercise-explore",
    "title": "2  Utforska samband",
    "section": "2.4 Övningsuppgifter",
    "text": "2.4 Övningsuppgifter\nAnvänd datamaterialet marketing som går att hämta via:\n\ndevtools::install_github(\"kassambara/datarium\")\n\ndata(\"marketing\", package = \"datarium\")\n\nDatamaterialet innehåller tre variabler som beskriver reklambudget för YouTube, Facebook och nyhetstidningar (tusentals dollar) samt försäljningen (tusentals enheter). Vi vill modellera sambandet mellan försäljningen och de tre reklamkällorna.\n\nUndersöka variablernas typ och skala.\nSammanställ beskrivande statistik för respektive variabel.\nVisualisera fördelningen av respektive variabel.\nSkapa ett spridningsdiagram för varje förklarande variabel med responsvariabeln och tolka de utefter de fyra bitar information som ett spridningsdiagram visar.\nSammanfatta dina iakttagelser och motivera vilka förklarande variabler som bör inkluderas i en modell och hur de bör struktureras.\n\n\n\n\n\nHorst, Allison Marie, Alison Presmanes Hill, och Kristen B Gorman. 2020. palmerpenguins: Palmer Archipelago (Antarctica) penguin data. https://doi.org/10.5281/zenodo.3960218.\n\n\nKendall, Maurice G. 1955. Rank correlation methods, 2nd ed. Oxford, England: Hafner Publishing Co.\n\n\nSpearman, C. 1904. ”The Proof and Measurement of Association between Two Things”. The American Journal of Psychology 15 (1): 72–101. http://www.jstor.org/stable/1412159.",
    "crumbs": [
      "Regressionsanalys",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Utforska samband</span>"
    ]
  },
  {
    "objectID": "01-regression/01-explorative-analysis.html#footnotes",
    "href": "01-regression/01-explorative-analysis.html#footnotes",
    "title": "2  Utforska samband",
    "section": "",
    "text": "Eller andra mått för att beräkna styrkan på samband, t.ex. Kendall (Kendall 1955) eller Spearman (Spearman 1904).↩︎",
    "crumbs": [
      "Regressionsanalys",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Utforska samband</span>"
    ]
  },
  {
    "objectID": "01-regression/02-model-structure.html",
    "href": "01-regression/02-model-structure.html",
    "title": "3  Modellanpassning",
    "section": "",
    "text": "3.1 Indikatorvariabler\nEn regressionsmodell kan inte hantera kvalitativa variabler direkt, exempelvis \\(\\beta_4 \\cdot \\text{art}\\), då variabelns värden beskriver kategorier inte värden från en numerisk skala. Detta gäller även om den kvalitativa variabeln är kodad numerisk. En lutningsparameter beskriver den konstanta förändring i responsvariabeln när den tillhörande förklarande variabeln ökar med en enhet, men en kvalitativ variabel har oftast ingen enhet och inte heller konstanta förändringar mellan intilliggande värden. Istället måste vi transformera den kvalitativa variabeln numerisk genom indikatorvariabler (även kallad dummyvariabler).\nSom namnet antyder används indikatorvariabler för att indikera vilken kategori en observation har uppmätt på den kvalitativa variabeln. Vi behöver då skapa en begränsad mängd indikatorvariabler som på ett tydligt sätt visar exakt en kategori per observation.\nAnta att en kvalitativ variabel har 3 kategorier: \\[\n\\begin{bmatrix}A\\\\B\\\\C\\end{bmatrix}\n\\] Vi kan börja med att skapa en indikatorvariabel för kategori A som antar värdet 1 om observationen har uppmätt kategorin, 0 annars:\n\\[\n\\begin{bmatrix}A\\\\B\\\\C\\end{bmatrix} = \\begin{bmatrix}1\\\\0\\\\0\\end{bmatrix}\n\\] Med endast en indikatorvariabel kan vi inte tydligt identifiera om en observation har uppmätt kategori B eller C då de båda har värdet 0, så vi lägger till ytterligare en indikator som antar värdet 1 om observationen uppmätt kategori B, 0 annars:\n\\[\n\\begin{bmatrix}A\\\\B\\\\C\\end{bmatrix} = \\begin{bmatrix}1 & 0\\\\0 & 1\\\\0 & 0\\end{bmatrix}\n\\] Nu skulle det vara lätt att fortsätta, att skapa en indikatorvariabel även för den sista kategorin, men det behövs inte. Om båda indikatorvariablerna är 0 har vi lyckats identifiera att observationen uppmätt kategori C och ytterligare en variabel är bara onödig information.\nDen sista kategorin blir också vår referenskategori, den kategori som de andra indikatorvariablernas effekter tolkas gentemot. När vi tolkar lutningsparametrar för indikatorvariabler, till exempel indikatorvariabeln för A, mäts förändringen i \\(Y\\) när \\(X = A\\) jämfört med när \\(X = C\\).\nGenerellt skapas \\(\\text{antal kategorier} - 1\\) indikatorvariabler för varje kvalitativa variabel som ska inkluderas i en regressionsmodell. Valet av referenskategori för respektive är godtyckligt, men vanligtvis används den första eller sista kategorin för detta ändamål.\nFör att slutföra modelleringen av Ekvation 3.1 ska vi inkludera Art och Kön i modellen. Då behöver vi skapa två respektive en indikatorvariabel enligt:\n\\[\\begin{align*}\n  Gentoo &= \\begin{cases}\n            1 \\qquad \\text{om art Gentoo}\\\\\n            0 \\qquad \\text{annars}\n        \\end{cases}\\\\\n  Chinstrap &= \\begin{cases}\n      1 \\qquad \\text{om art Chinstrap}\\\\\n      0 \\qquad \\text{annars}\n  \\end{cases}\n\\end{align*}\\]\noch\n\\[\\begin{align*}\n  hane &= \\begin{cases}\n            1 \\qquad \\text{om hane}\\\\\n            0 \\qquad \\text{annars}\n        \\end{cases}\n\\end{align*}\\]\nför att till slut skapa följande modell:\n\\[\n\\text{näbblängd} = \\beta_0 + \\beta_1 \\cdot \\text{kroppsvikt} + \\beta_2 \\cdot \\text{fenlängd} + \\beta_3 \\cdot \\text{näbbredd} + \\beta_4 \\cdot \\text{Gentoo} + \\beta_5 \\cdot \\text{Chinstrap} + \\beta_6 \\cdot \\text{hane} + E\n\\tag{3.2}\\]\ndär Adelie och honor agerar referenskategori för respektive kvalitativ variabel.",
    "crumbs": [
      "Regressionsanalys",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Modellanpassning</span>"
    ]
  },
  {
    "objectID": "01-regression/02-model-structure.html#indikatorvariabler",
    "href": "01-regression/02-model-structure.html#indikatorvariabler",
    "title": "3  Modellanpassning",
    "section": "",
    "text": "Viktigt\n\n\n\nRent matematiskt kommer tre indikatorvariabler modellera ett perfekt samband och skapa problem med singularitet i beräkningarna.",
    "crumbs": [
      "Regressionsanalys",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Modellanpassning</span>"
    ]
  },
  {
    "objectID": "01-regression/02-model-structure.html#modellanpassning",
    "href": "01-regression/02-model-structure.html#modellanpassning",
    "title": "3  Modellanpassning",
    "section": "3.2 Modellanpassning",
    "text": "3.2 Modellanpassning\nEkvation 3.2 visar den sanna modell som utgår ifrån populationens alla observerade värden, men nästintill alla undersökningar utgår från någon form av urval. Även en totalundersökning under en viss period kan anses vara ett urval i tiden om modellen avses att användas efter undersökningsperioden är slutförd.\nVi kan beteckna den anpassade modellen med dess skattade parametrar enligt:\n\\[\n\\hat{y}_i = b_0 + b_1 \\cdot x_{1i} + b_2 \\cdot x_{2i} + b_3 \\cdot x_{3i} + b_4 \\cdot x_{4i} + b_5 \\cdot x_{5i} + b_6 \\cdot x_{6i}\n\\tag{3.3}\\] där \\[\\begin{align*}\n  \\hat{y}_i &= \\text{responsvariabelns skattade värde för observation i}\\\\\n  b_0 &= \\text{skattning av interceptet}\\\\\n  b_1 - b_6 &= \\text{skattning av lutningsparametrar}\n\\end{align*}\\]\n\n\n\n\n\n\nNotera\n\n\n\nViss litteratur använder \\(\\hat{\\beta}\\) som beteckning för skattade parametrar.\n\n\nModellen anpassas med hjälp av minsta kvadratskattningen (eng. Ordinary Least Squares, OLS), där syftet är att minimera modellens totala fel. Vi kan notera att Ekvation 3.3 saknar feltermen \\(E\\) som inkluderas tidigare, vilket kommer från att den anpassade modellen endast består av regressionslinjen. Kom ihåg att en regressionsmodell ämnar att ge en förenkling av verkligheten. Men \\(E\\) beskrev ju felet i modellen och om vi ska minimera det totala felet behöver vi på något sätt ta hänsyn till denna term i modellanpassningen.\nAnta att vi anpassar en modell enbart på kroppsvikt och näbblängd. Om vi skulle projicera den anpassade enkla linjära modellen i ett spridningsdiagram över de två variablerna (Figur 3.1) skulle linjen inte lyckas träffa alla punkter exakt, varje enskilda observation kommer ligga ett visst avstånd från regressionslinjen. Detta avstånd är observationens residual som betecknas med \\(e_i\\).\n\n\n\n\n\n\n\n\nFigur 3.1: Visualisering av regressionsmodellens residualer\n\n\n\n\n\nMatematiskt beräknar vi \\(e_i = Y_i - \\hat{Y}_i\\), där \\(Y_i\\) är det observerade värdet (punkten) och \\(\\hat{Y}_i\\) är modellens anpassade värde (linjen). Minsta kvadratskattningen beräknar modellens alla parametrar så att det totala felet (Sum of Squares of Error, SSE) för alla residualer blir så litet som möjligt.\n\\[\nSSE = \\sum_{i = 1}^n e_i^2 = \\sum_{i = 1}^n (Y_i - \\hat{Y}_i)^2\n\\tag{3.4}\\]\nI en enkel linjär regression går det att härleda fram analytiska lösningar för de två parameterskattningarna, \\(b_0\\) och \\(b_1\\), som minimerar SSE men så fort vi inkluderar flera variabler blir detta betydligt svårare. Istället förlitar vi (och R) oss på matrisberäkningar som presenteras mer i Avsnitt 3.3.\n\n\n\n\n\n\nViktigt\n\n\n\nFormler för parameterskattningarna i en enkel linjär regression är: \\[\\begin{align*}\n  b_1 &= \\frac{\\sum_{i=1}^n(X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum_{i=1}^n(X_i - \\bar{X})^2}\\\\\n  b_0 &= \\bar{Y} - b_1 \\cdot \\bar{X}\n\\end{align*}\\]\n\\(b_1\\) kan också omformuleras till beräkningsformeln: \\[\n\\begin{aligned}\n\\frac{\\sum_{i=1}^n(X_i \\cdot Y_i) - \\frac{\\sum_{i=1}^nX_i \\cdot \\sum_{i=1}^nY_i}{n}}{\\sum_{i=1}^nX_i^2 - \\frac{(\\sum_{i=1}^nX_i)^2}{n}}\n\\end{aligned}\n\\]\n\n\n\n3.2.1 Modellanpassning i R\nFör att anpassa en linjär regressionsmodell i R används funktionen lm() med följande argument:\n\nformula: modellens struktur som ett formelobjekt\ndata: datamaterialet som variablerna hittas\n\nEtt formelobjekt är ett speciellt format som R använder för att beskriva relationen mellan variabler. Generellt anges formatet som y ~ x där x består utav de olika förklarande variablerna, till exempel bill_length_mm ~ body_mass_g + bill_depth_mm. Det finns ett kortkommando (~ .) som används i exemplet nedan, där alla övriga variabler inkluderas i högerledet , men det kräver att vi först har ett datamaterial enbart bestående av de förklarande variablerna från Ekvation 3.2.\nVi måste också se till att alla variabler i datamaterialet har rätt variabeltyp som vi förväntar oss. Vi identifierade i Avsnitt 2.1 att vi hade tre kvantitativa variabler och två kvalitativa variabler som i R motsvarar typerna numeric och Factor. Att använda sig av Factor underlättar transformationen till indikatorvariabler eftersom R vet att den måste göra så för att modellen ska fungera. Om de kvalitativa variablerna var av typen character eller kodad numeric är det inte säkert att R skapar indikatorvariabler. Vi kan undersöka variabeltyperna för penguins med hjälp av str().\n\n# Tar endast med de variabler som vi ansåg ha ett samband med responsvariabeln\nmodelData &lt;- \n  penguins %&gt;% \n  select(\n    bill_length_mm,\n    body_mass_g,\n    flipper_length_mm,\n    bill_depth_mm,\n    species,\n    sex\n  )\n\n# Anpassar angiven modell\nsimpleModel &lt;- lm(formula = bill_length_mm ~ ., data = modelData)\n\nMed summary() får vi en detaljerad utskrift för modellen som inkluderar de anpassade regressionskoefficienterna. Vid presentation av en sådan utskrift kan vi använda kable() eller xtable() för att få en snyggare utskrift.\n\nVisa kod\nsummary(simpleModel)\n\n\n\n\n\n\n\nCall:\nlm(formula = bill_length_mm ~ ., data = modelData)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.3939 -1.3424 -0.0421  1.2695 11.4274 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       1.502e+01  4.374e+00   3.433 0.000674 ***\nbody_mass_g       1.084e-03  4.231e-04   2.562 0.010864 *  \nflipper_length_mm 6.856e-02  2.315e-02   2.961 0.003293 ** \nbill_depth_mm     3.130e-01  1.541e-01   2.032 0.043000 *  \nspeciesChinstrap  9.566e+00  3.497e-01  27.351  &lt; 2e-16 ***\nspeciesGentoo     6.404e+00  1.030e+00   6.215 1.56e-09 ***\nsexmale           2.030e+00  3.892e-01   5.215 3.27e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.217 on 326 degrees of freedom\nMultiple R-squared:  0.8386,    Adjusted R-squared:  0.8356 \nF-statistic: 282.3 on 6 and 326 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nFigur 3.2: Inte särskilt snygg utskrift\n\n\n\n\n\nVisa kod\nsummary(simpleModel) %&gt;% \n  coef() %&gt;% \n  as_tibble(rownames = NA) %&gt;% \n  rownames_to_column() %&gt;% \n  rename(\n    ` ` = rowname,\n    Skattning = Estimate,\n    Medelfel = `Std. Error`,\n    `t-värde` = `t value`,\n    `p-värde` = `Pr(&gt;|t|)`\n  ) %&gt;% \n  kable(\n    digits = 4\n  ) %&gt;% \n  kable_styling(\"striped\")\n\n\n\n\nTabell 3.1: En snygg utskrift av modellens anpassade parametrar\n\n\n\n\n \n  \n     \n    Skattning \n    Medelfel \n    t-värde \n    p-värde \n  \n \n\n  \n    (Intercept) \n    15.0166 \n    4.3742 \n    3.4330 \n    0.0007 \n  \n  \n    body_mass_g \n    0.0011 \n    0.0004 \n    2.5617 \n    0.0109 \n  \n  \n    flipper_length_mm \n    0.0686 \n    0.0232 \n    2.9608 \n    0.0033 \n  \n  \n    bill_depth_mm \n    0.3130 \n    0.1541 \n    2.0316 \n    0.0430 \n  \n  \n    speciesChinstrap \n    9.5655 \n    0.3497 \n    27.3508 \n    0.0000 \n  \n  \n    speciesGentoo \n    6.4044 \n    1.0304 \n    6.2154 \n    0.0000 \n  \n  \n    sexmale \n    2.0297 \n    0.3892 \n    5.2153 \n    0.0000 \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip 3.1\n\n\n\nFör att skapa denna snygga utskrift av koefficienterna behöver vi plocka ut en enskild del av summary() med hjälp av coef(). I dokumentationen för lm() finns mer information om vad som kan hämtas från det resulterande regressionsobjektet.\nR är ett objektorienterat programmeringsspråk, och funktionen lm() returnerar ett objekt av klassen ”lm”, vilket är en lista. Det är enkelt att plocka önskade delar från den listan vid behov. Det finns en mängd funktioner kopplade till objekt av klassen ”lm”:\n\ncoef(): Ger regressionskoefficienter\nresiduals(): Ger residualerna\nfitted(): Ger de anpassade värdena (\\(\\hat{Y}\\))\nsummary(): Ger en sammanfattande analys av regressionsmodellen. Funktionen returnerar ett objekt av klassen ”summary.lm”. Se ?summary.lm i dokumentationen. coef() funkar även på dessa objekt som vi såg ovan.\nanova(): Ger ANOVA-tabellen för modellen\npredict(): gör prediktioner för (nya) x-värden, alltså beräknar \\(\\hat{Y}\\) för givna x-värden. Kan även beräkna konfidensintervall och prediktionsintervall för \\(\\hat{Y}\\). Se ?predict.lm() för detaljer.\nplot(): Ger olika diagnostiska plottar, se ?plot.lm för detaljer.\nconfint(): Beräknar konfidensintervall för regressionskoefficienterna\nmodel.matrix(): skapar olika typer av designmatriser som kan användas i lm(), se Avsnitt 3.3.\n\nDet är också användbart att använda str() på lm-objekt. Kolla i ?lm() under Value rubriken för att se vilka olika delar som finns i objektet.\n\n\nTabell 3.1 visar de skattade lutningsparametrarna (koefficienterna). Exempelvis kan vi se att för varje gram mer en pingvin väger ökar näbbens längd med ca 0.0011 mm i genomsnitt givet att alla andra variabler hålls konstanta. Den sista delen av denna tolkning är viktig att inkludera då en förändring av flera variabler skulle medföra en annan förändring av responsvariabeln i relation till respektive koefficient.\nIndikatorvariablerna tolkas inom sin grupp jämfört med referenskategorin, till exempel har Gentoo-pingviner i genomsnitt en 6.4 mm större näbblängd än referenskategorin Adelie-pingviner givet att alla andra variabler hålls konstanta.\nInterceptet är endast relevant att tolka om värdemängden är alla 0, det vill säga att data täcker det område där alla förklarande variabler antar värdet 0. I just detta exempel finns det inte data över dessa områden vilket medför att värdet på interceptet inte har någon rimlig tolkning.\n\n\n\n\n\n\nViktigt\n\n\n\nÄven om tolkningen av interceptet inte blir rimlig måste interceptet inkluderas i modellanpassningen för att minsta kvadratskattningen ska minimera SSE. Om interceptet hade plockats bort motsvarar det en linje som tvingas att korsa y-axeln vid \\(y = 0\\) vilket resulterar i att modellen inte beskriver de fenomen som vi vill att den ska beskriva.\n\n\nDet är inte bara koefficienttabellen som är relevant att titta på i en modellanpassning och vi kommer tillbaka till de andra objekten som finns inuti lm senare.",
    "crumbs": [
      "Regressionsanalys",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Modellanpassning</span>"
    ]
  },
  {
    "objectID": "01-regression/02-model-structure.html#sec-matrices",
    "href": "01-regression/02-model-structure.html#sec-matrices",
    "title": "3  Modellanpassning",
    "section": "3.3 Matrisberäkningar",
    "text": "3.3 Matrisberäkningar\nMatriser underlättar de tunga beräkningar som krävs för att anpassa en regressionsmodell med flera förklarande variabler. Vi kan formulera en regressionsmodell i matrisform enligt: \\[\n\\mathbf{Y} = \\mathbf{X} \\boldsymbol{\\beta} + \\mathbf{E}\n\\tag{3.5}\\] där, \\[\n    \\mathbf{Y} = \\underset{n \\times 1}{\\begin{bmatrix}Y_1\\\\Y_2\\\\\\vdots\\\\Y_n\\end{bmatrix}} \\quad \\mathbf{X} = \\underset{n \\times p}{\\begin{bmatrix}1 & X_{11} & X_{12} & \\cdots & X_{1k}\\\\1 & X_{21} & X_{22} & \\cdots & X_{2k}\\\\\\vdots & \\vdots & \\vdots & \\ddots & \\vdots\\\\1 & X_{n1} & X_{n2} & \\cdots & X_{nk}\\end{bmatrix}\n} \\quad \\boldsymbol{\\beta} = \\underset{p \\times 1}{\\begin{bmatrix}\\beta_0\\\\\\beta_1\\\\\\vdots\\\\\\beta_k\\end{bmatrix}} \\quad \\mathbf{E} = \\underset{n \\times 1} {\\begin{bmatrix}E_1\\\\E_2\\\\\\vdots\\\\E_n\\end{bmatrix}}\n\\] \\(\\mathbf{X}\\) kallas för designmatrisen och innehåller alla \\(k\\) förklarande variabler, en kolumn för varje, samt en första kolumn med 1:or som motsvarar interceptet. Indikatorvariabler adderar till antalet förklarande variabler trots att de utgår ifrån samma kvalitativa variabel, se Ekvation 3.2 där vi totalt har 6 förklarande variabler. \\(p\\) beskriver antalet parametrar, motsvarande \\(k + 1\\) antalet lutningsparametrar + interceptet, och \\(n\\) är antalet observationer.\nSkattningen av \\(\\hat{\\boldsymbol{\\beta}}\\) minimerar fortfarande SSE där: \\[\nSSE = (\\mathbf{Y} - \\mathbf{X}\\boldsymbol{\\hat{\\beta}})'(\\mathbf{Y}-\\mathbf{X}\\boldsymbol{\\hat{\\beta}})\n\\tag{3.6}\\]\noch \\[\n\\boldsymbol{\\hat{\\beta}} = (\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\mathbf{Y}\n\\tag{3.7}\\]\n\n3.3.1 Matriser i R\nR använder sig av matriser i bakgrunden när vi använder lm() men vi kan också skapa våra egna utifrån datamaterialet och genomföra matrisberäkningen för \\(\\boldsymbol{\\hat{\\beta}}\\) eller SSE.\nDesignmatrisen är den mest komplexa att skapa, speciellt om vi har kvalitativa variabler med i data, men som tur är kan vi använda samma formelobjekt i funktionen model.matrix().\n\nX &lt;- \n  model.matrix(\n    bill_length_mm ~ ., \n    data = modelData\n  )\n\n# Visar de första fem raderna i matrisen\nX[1:5,]\n\n  (Intercept) body_mass_g flipper_length_mm bill_depth_mm speciesChinstrap\n1           1        3750               181          18.7                0\n2           1        3800               186          17.4                0\n3           1        3250               195          18.0                0\n4           1        3450               193          19.3                0\n5           1        3650               190          20.6                0\n  speciesGentoo sexmale\n1             0       1\n2             0       0\n3             0       0\n4             0       0\n5             0       1\n\nY &lt;- \n  modelData$bill_length_mm %&gt;% \n  as.matrix()\n\n# Visar de första fem raderna i vektorn\nY[1:5,]\n\n[1] 39.1 39.5 40.3 36.7 39.3\n\n\nDe fem första raderna i respektive matris är transformerade värden från Tabell 2.1 och designmatrisen innehåller indikatorvariabler enligt Ekvation 3.2.\n\n3.3.1.1 Skattning av \\(\\boldsymbol{\\hat{\\beta}}\\)\nNu kan vi med hjälp av matrisberäkningsformler i R beräkna koefficienterna:\n\nbetaHat &lt;- solve(t(X) %*% X) %*% t(X) %*% Y\n\n\n\n\n\nTabell 3.2: Skattade koefficienter från matrisberäkning avrundat till fyra decimaler\n\n\n\n\n \n  \n     \n    Koefficient \n  \n \n\n  \n    (Intercept) \n    15.0166 \n  \n  \n    body_mass_g \n    0.0011 \n  \n  \n    flipper_length_mm \n    0.0686 \n  \n  \n    bill_depth_mm \n    0.3130 \n  \n  \n    speciesChinstrap \n    9.5655 \n  \n  \n    speciesGentoo \n    6.4044 \n  \n  \n    sexmale \n    2.0297 \n  \n\n\n\n\n\n\n\n\nTabell 3.2 visar samma parameterskattningar som Tabell 3.1, eftersom det är samma beräkningar som genomförts. Vi ser dock fler värden i den tidigare tabellen vilket uppkommer från att lm() omfattar fler beräkningar som sedan sammanställs i ett och samma objekt.\nTill exempel beräknas även prediktioner och residualer, vilket vi också kan göra med matrisberäkningar enligt:\n\nYhat &lt;- X %*% betaHat\n\ne &lt;- Y - Yhat\n\n\n\n3.3.1.2 Kovariansmatris för \\(\\boldsymbol{\\hat{\\beta}}\\)\nVariansen för respektive parameter kan också beräknas med matriser, där medelfelet är roten ur diagonalelementen från kovariansmatrisen. \\[\ns^2_{\\boldsymbol{\\hat{\\beta}}} = (\\mathbf{X}'\\mathbf{X})^{-1}MSE\n\\] där MSE är \\(\\frac{SSE}{n - (k + 1)}\\).\nDå beräkningen av SSE och MSE utgår från matriser kommer även deras objekt vara en \\(1 \\times 1\\) matris, men i beräkningen av kovariansmatrisen är MSE endast en skalär. Vi behöver därför explicit ange att MSE inte längre är en matris för att undvika problem med matrisdimensioner.\n\n# Beräknar SSE\nSSE &lt;- t(Y - Yhat) %*% (Y - Yhat)\n\n# Beräknas MSE\nMSE &lt;- SSE / (nrow(X) - ncol(X))\n\n# Beräknar kovariansmatrisen för Beta\ns2Beta &lt;- solve(t(X) %*% X) * as.numeric(MSE)\n\n\n\n\n\nTabell 3.3: Skattade medelfel från matrisberäkning avrundat till fyra decimaler\n\n\n\n\n \n  \n     \n    Medelfel \n  \n \n\n  \n    (Intercept) \n    4.3742 \n  \n  \n    body_mass_g \n    0.0004 \n  \n  \n    flipper_length_mm \n    0.0232 \n  \n  \n    bill_depth_mm \n    0.1541 \n  \n  \n    speciesChinstrap \n    0.3497 \n  \n  \n    speciesGentoo \n    1.0304 \n  \n  \n    sexmale \n    0.3892 \n  \n\n\n\n\n\n\n\n\nVi ser även i Tabell 3.3 samma värden som Tabell 3.1.",
    "crumbs": [
      "Regressionsanalys",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Modellanpassning</span>"
    ]
  },
  {
    "objectID": "01-regression/02-model-structure.html#sec-exercise-fit",
    "href": "01-regression/02-model-structure.html#sec-exercise-fit",
    "title": "3  Modellanpassning",
    "section": "3.4 Övningsuppgifter",
    "text": "3.4 Övningsuppgifter\nVi kommer återigen använda datamaterialet marketing.\n\nAnpassa en linjär regressionsmodell med lm() som inkluderar de variabler som du valt ut i Avsnitt 2.4.\nSammanställ en tabell över de skattade koefficienterna och tolka respektive.\nSkapa designmatrisen och en matris för responsvariabeln och skatta lutningsparametrarna med hjälp av dessa. Kontrollera att du får samma värden som i tabellen från lm().\nAnvänd matrisberäkningar för att beräkna medelfelet för respektive parameter.",
    "crumbs": [
      "Regressionsanalys",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Modellanpassning</span>"
    ]
  },
  {
    "objectID": "01-regression/03-model-assessment.html",
    "href": "01-regression/03-model-assessment.html",
    "title": "4  Modellutvärdering",
    "section": "",
    "text": "4.1 Residualanalys\nResidualanalys innebär att beräkna och visuellt utforska residualerna från en modell gentemot modellantaganden \\(E\\overset{iid}{\\sim}N(0, \\sigma^2)\\), det vill säga att residualerna är oberoende, normalfördelade med väntevärde 0 och lika varians. Residualerna kan också användas för att undersöka ifall den linjära modell som anpassats är lämplig. Vi kommer titta närmare på mer detaljerad residualanalys i ett senare kapitel.\nFör enkelhetens skull kan vi plocka ut residualerna samt de observerade och skattade värdena på responsvariabeln från den skattade modellen (se Tip 3.1).\n# Skapa ett datamaterial för visualiseringar\n\nresidualData &lt;- \n  tibble(\n    residuals = residuals(simpleModel),\n    y = modelData$bill_length_mm,\n    yHat = fitted(simpleModel)\n  )\nVi kommer visualisera dessa variabler i olika former med hjälp av ggplot2 vilket kräver att vi har en data.frame eller tibble med data.",
    "crumbs": [
      "Regressionsanalys",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Modellutvärdering</span>"
    ]
  },
  {
    "objectID": "01-regression/03-model-assessment.html#residualanalys",
    "href": "01-regression/03-model-assessment.html#residualanalys",
    "title": "4  Modellutvärdering",
    "section": "",
    "text": "4.1.1 Normalfördelning\nVi kan undersöka antagandet om normalfördelade residualer genom ett histogram och ett QQ-diagram (quantile-quantile diagram).\n\n\nVisa kod\nggplot(residualData) + \n  aes(x = residuals, y = after_stat(density)) +\n  geom_histogram(binwidth = 1, fill = \"steelblue\", color = \"black\") + \n  theme_bw() + \n  labs(x = \"Residualer\", y = \"Densitet\")\n\n\n\n\n\nResidualernas fördelning\n\n\n\n\n\n\nVisa kod\nggplot(residualData) + \n  # Använder standardiserade residualer\n  aes(sample = scale(residuals)) + \n  geom_qq_line() +\n  geom_qq(color = \"steelblue\") +\n  theme_bw() + \n  labs(x = \"Teoretiska kvantiler\", y = \"Observerade kvantiler\")\n\n\n\n\n\nResidualernas observerade kvantiler jämfört med teoretiska normalfördelade kvantiler.\n\n\n\n\nI histogrammet vill vi se normalfördelningens symmetriska och klockliknande form centrerad kring 0 vilket ibland kan vara svårt att utläsa speciellt om datamaterialet är litet. QQ-diagrammet visar de observerade och de teoretiska kvantilerna där vi vill att punkterna ska följa den inritade linjen för en “perfekt” normalfördelning.\nFör denna modell ser vi inga tydliga avvikelser från det mönster vi vill se, men vi kan utläsa ett fåtal avvikande observationer som skulle kunna betraktas som extremvärden. Två stora positiva residualer kan identifieras i diagrammen men det finns även enstaka negativa som ligger långt från de övriga.\n\n\n\n\n\n\nViktigt\n\n\n\nVi kan betrakta antagandet om normalfördelning som inte uppfyllt om dessa diagram visar på starka avvikelser från det vi vill se. Även när vi vet att ett urval är draget från en normalfördelning är det inte alltid som histogrammet visar den form som vi söker.\n\n\nVisa kod\nset.seed(1234)\n\ntibble(\n  x = rnorm(30)\n) %&gt;% \n ggplot() + \n  aes(x = x, y = after_stat(density)) +\n  geom_histogram(bins = 10, fill = \"steelblue\", color = \"black\") + \n  theme_bw() + \n  labs(x = \"x\", y = \"Densitet\")\n\n\n\n\n\nFördelning av ett urval från den faktiska normalfördelningen\n\n\n\n\nStarka avvikelser från normalfördelningen innebär exempelvis att vi ser flera områden med hög densitet:\n\n\nVisa kod\nset.seed(1234)\n\ntibble(\n  x = runif(30)\n) %&gt;% \n ggplot() + \n  aes(x = x, y = after_stat(density)) +\n  geom_histogram(bins = 10, fill = \"steelblue\", color = \"black\") + \n  theme_bw() + \n  labs(x = \"x\", y = \"Densitet\")\n\n\n\n\n\nFördelning av ett urval från andra fördelningar\n\n\n\n\neller en väldigt skev fördelning:\n\n\nVisa kod\nset.seed(1234)\n\ntibble(\n  x = rchisq(30, df = 2)\n) %&gt;% \n ggplot() + \n  aes(x = x, y = after_stat(density)) +\n  geom_histogram(bins = 10, fill = \"steelblue\", color = \"black\") + \n  theme_bw() + \n  labs(x = \"x\", y = \"Densitet\")\n\n\n\n\n\nFördelning av ett urval från andra fördelningar\n\n\n\n\nDessa diagram indikerar att modellen saknar en förklarande variabel eller måste transformeras på något sätt för att uppfylla antagandet.\nOm QQ-diagrammet uppvisar tydliga mönster, till exempel om punkterna är krökta runt linjen, betyder det att modellen inte uppfyller antagandet om linjärt samband.\n\n\n\n\n\nExempel på mönster i QQ-diagram\n\n\n\n\n\n\n\n\n4.1.2 Lika varians\nVi kan kontrollera antagandet om residualernas lika varians genom ett spridningsdiagram med residualerna på y-axeln och någon av anpassade värden eller observerade värden på förklarande eller responsvariabeln. Vanligtvis används de anpassade värdena för att x-axeln ska beskriva hela modellen, men andra variabler kan vara användbara att visualisera för att identifiera potentiella orsaker till ett brustet antagande.\n\n\nVisa kod\nggplot(residualData) + \n  aes(x = yHat, y = residuals) + \n  geom_point(color = \"steelblue\") + \n  theme_bw() +\n  labs(x = \"Anpassade värden\", y = \"Residualer\") + \n  geom_hline(\n    aes(yintercept = 0)\n  ) + \n  # Imaginära gränser\n  geom_hline(\n    aes(yintercept = -5),\n    color = \"#d9230f\",\n    linetype = 2\n  ) + \n  geom_hline(\n    aes(yintercept = 5),\n    color = \"#d9230f\",\n    linetype = 2\n  )\n\n\n\n\n\n\n\n\nFigur 4.1: Residualernas spridning mot anpassade värden.\n\n\n\n\n\nFör att uppfylla antagandet om lika varians, ska punkterna i varje tvärsnitt av värden på x-axeln vara jämnt utspridda. Tänk som att vi vill placera två stycken parallella linjer längs med maximum och minimum-värden för residualerna (de två rödstreckade linjerna i Figur 4.1) och en stor majoritet av punkterna bör ligga utspridda emellan dessa. Vi ser i Figur 4.1 att några enstaka observationer faktiskt hamnar utanför och ökar variationen i vissa tvärsnitt, men då det inte är tydliga avvikelser kan vi anse att residualerna har uppfyllt antagandet om lika varians.\n\n\n\n\n\n\nViktigt\n\n\n\nOm linjerna som täcker maximum och minimum-värden för residualerna inte är parallella uppfyller inte modellen kravet om lika varians.\n\n\n\n\n\nExempel på icke-konstant varians i residualerna\n\n\n\n\n\n\n\nExempel på icke-konstant varians i residualerna\n\n\n\n\nDessa fenomen betyder oftast att hela eller delar av modellen behöver transformeras för att uppfylla antagandet om lika varians.\nVi kan också identifiera problem med linjäritet i detta spridningsdiagram. Figuren nedan uppvisar någorlunda konstant varians i avseende på variationen i varje tvärsnitt av x-axeln, men det finns ett tydligt mönster i residualerna. Detta betyder att modellen inte lyckats modellera sambandet på rätt sätt. I detta läge vore det lämpligt att visualisera residualerna mot respektive förklarande variabel för att identifiera vilken/vilka utav de som verkar bidra med det icke-linjära sambandet.\n\n\n\n\n\nMönster i residualerna som tyder på ett icke-linjärt samband\n\n\n\n\n\n\n\n\n4.1.3 Oberoende\nOfta är det svårt eller omöjligt att undersöka om observationerna är oberoende med avseeende på alla ordningar som data kan samlas in på. Undantaget är ifall vi vet hur datainsamlingen har gått till och om det finns någon tydlig tidsaspekt, till exempel i tidsseriedata, eller att samma enhet har uppmätts flera gånger som gör att vi vet att observationerna blir beroende. Vi vill att den modell som anpassas tar hänsyn till det beroende som finns i data så att de efterföljande residualerna endast uppvisar oberoende.\nEtt linjediagram över residualerna i observationsordning kan användas för att undersöka oberoende, men det är som sagt endast i specialfall som denna visualisering används. Linjediagrammet ska uppvisa “slump”, det vill säga inga tydliga mönster i residualerna.\n\n\nVisa kod\nggplot(residualData) + \n  aes(x = 1:nrow(residualData), y = residuals) + \n  geom_line(color = \"steelblue\") + \n  theme_bw() +\n  labs(x = \"Obs. index\", y = \"Residualer\") + \n  geom_hline(\n    aes(yintercept = 0),\n    color = \"black\"\n  )\n\n\n\n\n\nResidualer i observationsordning.\n\n\n\n\nAndra exempel på data som har ett beroende är:\n\nVi samlar in data från personer, men vissa personer kommer ifrån samma famlij, detta kan göra att det finns ett beroende mellan dessa personer.\nVi samlar in spatiala (rumsliga) data, till exempel temperatur eller regnmängd på olika platser i Östergötland. Då är det vanligt att det finns en positiv korrelation mellan geografiskt närliggande observationer.\n\n\n\n4.1.4 Funktion med alla diagram\nDessa diagram kommer vara återkommande i regressionsmodellering så vi kan skapa en funktion för att automatiskt generera alla fyra diagram samtidigt. Vi får genom paketet cowplot tillgång till en funktion (plot_grid) som kan kombinera flera diagram till en och samma.\n\n\nVisa kod\n# Funktionen kräver två argument, modellen som anpassats och bredden på staplarna i histogrammet.\nresidualPlots &lt;- function(model) {\n  \n  residualData &lt;- \n    data.frame(\n      residuals = residuals(model),\n      # Responsvariabeln finns som första kolumn i modellens model-objekt\n      y = model$model[,1],\n      yHat = fitted(model)\n    )\n  \n  \n  p1 &lt;- ggplot(residualData) + \n    aes(x = residuals, y = after_stat(density)) +\n    geom_histogram(bins = 20, fill = \"steelblue\", color = \"black\") + \n    theme_bw() + \n    labs(x = \"Residualer\", y = \"Densitet\")\n  \n  p2 &lt;- ggplot(residualData) + \n    aes(x = yHat, y = residuals) + \n    geom_hline(aes(yintercept = 0)) + \n    geom_point(color = \"steelblue\") + \n    theme_bw() +\n    labs(x = \"Anpassade värden\", y = \"Residualer\")\n    \n  \n  p3 &lt;- ggplot(residualData) + \n    # Använder standardiserade residualer\n    aes(sample = scale(residuals)) + \n    geom_qq_line() + \n    geom_qq(color = \"steelblue\") +\n    theme_bw() + \n    labs(x= \"Teoretiska kvantiler\", y = \"Observerade kvantiler\")\n  \n  cowplot::plot_grid(p1, p2, p3, nrow = 2)\n  \n}\n\nresidualPlots(simpleModel)\n\n\n\n\n\n\n\n\nFigur 4.2: Residualdiagrammen i en och samma bild\n\n\n\n\n\nSammanfattningsvis visar Figur 4.2 att residualerna uppfyller antagandet om normalfördelning med väntevärde 0 och lika varians. Det finns inga tydliga mönster i något diagram som indikerar på motsatsen eller att modellen missar att plocka upp något av sambandet. Några enstaka extremvärden har identifierats, specifikt två stycken stora positiva residualer som kommer undersökas mer i senare kapitel. Slutsatsen är att modellen är en lämplig förenkling av verkligheten.",
    "crumbs": [
      "Regressionsanalys",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Modellutvärdering</span>"
    ]
  },
  {
    "objectID": "01-regression/03-model-assessment.html#sec-exercise-evaluate",
    "href": "01-regression/03-model-assessment.html#sec-exercise-evaluate",
    "title": "4  Modellutvärdering",
    "section": "4.2 Övningsuppgifter",
    "text": "4.2 Övningsuppgifter\nAnvänd återigen marketing från Avsnitt 2.4.\n\nGenomför en enkel residualanalys med hjälp av residualdiagrammen. Kontrollera respektive antagande och bedöm ifall modellen uppfyller modellantaganden.",
    "crumbs": [
      "Regressionsanalys",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Modellutvärdering</span>"
    ]
  },
  {
    "objectID": "01-regression/04-statistical-inference.html",
    "href": "01-regression/04-statistical-inference.html",
    "title": "5  Statistisk inferens",
    "section": "",
    "text": "5.1 ANOVA\nAnalysis of Variance är en samling metoder som beräknar variationen av olika modellkomponenter. Målet med en modell är att förklara den totala variationen i responsvariabeln på bästa sätt. Allting som de förklarande variablerna hjälper till att beskriva kallas för den förklarade variationen och det som modellen inte lyckas förklara (felet) är den oförklarade variationen.\n\\[\n\\underbrace{\\mathbf{Y}}_\\text{total variation} = \\underbrace{\\mathbf{X} \\boldsymbol{\\beta}}_\\text{förklarad variation} + \\underbrace{\\mathbf{E}}_\\text{oförklarad variation}\n\\tag{5.1}\\]\nEkvation 5.1 visar att den totala variationen är en summa av den förklarade och oförklarade variationen vilket också ses i formlerna för dessa. Respektive komponent beräknas enligt:\n\\[\n  \\text{total variation} = SST = \\mathbf{Y}'\\mathbf{Y} - \\left(\\frac{1}{n}\\right)\\mathbf{Y}'\\mathbf{J}\\mathbf{Y}\n\\] där \\(\\mathbf{J}\\) är enhetsmatrisen, en \\(n \\times n\\) matris endast innehållande 1:or.\nDet kanske inte är så lätt att se vad dessa matrisberäkningar faktiskt beskriver men beräkningen motsvarar \\(\\sum_{i=1}^n(Y_i - \\bar{Y})^2\\), alltså täljaren i en variansberäkning för \\(Y\\). Den vänstra termen (\\(\\mathbf{Y}'\\mathbf{Y}\\)) motsvarar \\(Y_i\\) och den högra termen (\\(\\left(\\frac{1}{n}\\right)\\mathbf{Y}'\\mathbf{J}\\mathbf{Y}\\)) motsvarar \\(\\bar{Y}\\), responsvariabelns medelvärde. Den totala variationen beskriver hur mycket variation som uppkommer ifall vi skulle använda medelvärdet av \\(Y\\) som modell.\n\\[\n  \\text{oförklarad variation} = SSE = \\mathbf{Y}'\\mathbf{Y} - \\boldsymbol{\\hat{\\beta}}'\\mathbf{X}'\\mathbf{Y}\n\\] SSE har vi tidigare använt som ett mått på felet i modellen, se Ekvation 3.4, vilket betyder att \\(\\boldsymbol{\\hat{\\beta}}'\\mathbf{X}'\\mathbf{Y}\\) motsvarar \\(\\hat{Y}_i\\).\n\\[\n  \\text{förklarad variation} = SSR = \\boldsymbol{\\hat{\\beta}}'\\mathbf{X}'\\mathbf{Y} - \\left(\\frac{1}{n}\\right)\\mathbf{Y}'\\mathbf{J}\\mathbf{Y}\n\\]\nSSR beskriver variationen mellan modellens anpassade värde och medelvärdet av \\(Y\\). Det kan i sin tur kan tolkas som hur mycket mer variation som modellen bidrar med jämfört med medelvärdet, eller kort sagt hur mycket bättre modellen är på att förklara variationen i \\(Y\\).\nVi har tidigare använt en annan matrisformel för SSE men med hjälp av omformuleringen kan vi tydligt se hur SST = SSR + SSE: \\[\n\\mathbf{Y}'\\mathbf{Y} - \\left(\\frac{1}{n}\\right)\\mathbf{Y}'\\mathbf{J}\\mathbf{Y} = \\mathbf{Y}'\\mathbf{Y} \\underbrace{-  \\boldsymbol{\\hat{\\beta}}'\\mathbf{X}'\\mathbf{Y} + \\boldsymbol{\\hat{\\beta}}'\\mathbf{X}'\\mathbf{Y}}_\\text{summerar till 0} - \\left(\\frac{1}{n}\\right)\\mathbf{Y}'\\mathbf{J}\\mathbf{Y}\n\\] Vi kan också visualisera denna relation i ett stackat stapeldiagram. Den totala höjden av stapeln är SST medan de olika delarna beskriver hur stor del av den totala variationen som är förklarad eller oförklarad i en viss modell.\nVisualisering av de olika källor av variation",
    "crumbs": [
      "Regressionsanalys",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Statistisk inferens</span>"
    ]
  },
  {
    "objectID": "01-regression/04-statistical-inference.html#anova",
    "href": "01-regression/04-statistical-inference.html#anova",
    "title": "5  Statistisk inferens",
    "section": "",
    "text": "5.1.1 ANOVA-tabellen\nEn ANOVA-tabell är ett sätt att effektivt få en översikt av dessa olika komponenter samt visa ytterligare information, såsom frihetsgraderna (\\(df\\)) för respektive komponent och medelkvadratsummor.\nFrihetsgrader beskriver hur många lutningsparametrar som skattas för respektive del1 och medelkvadratsummor visar den genomsnittliga variationen per frihetsgrad, \\(\\frac{SS}{df}\\).\n\n\n\n\nTabell 5.1: Enkel ANOVA-tabell\n\n\n\n\n\n\n\n\n\n\n\n\nSource\nDF\nSum of Squares\nMean Square\n\n\n\n\nModel (Regression)\n\\(df_R = k\\)\n\\(SSR = \\boldsymbol{\\hat{\\beta}}' \\mathbf{X}' \\mathbf{Y} - \\frac{1}{n} \\mathbf{Y}' \\mathbf{J} \\mathbf{Y}\\)\n\\(MSR = \\frac{SSR}{df_R}\\)\n\n\nError\n\\(df_E = n - (k + 1)\\)\n\\(SSE = \\mathbf{Y}' \\mathbf{Y} - \\boldsymbol{\\hat{\\beta}}' \\mathbf{X}' \\mathbf{Y}\\)\n\\(MSE = \\frac{SSE}{df_E}\\)\n\n\nTotal\n\\(df_T = n - 1\\)\n\\(SSY = \\mathbf{Y}' \\mathbf{Y} - \\frac{1}{n} \\mathbf{Y}' \\mathbf{J} \\mathbf{Y}\\)\n\n\n\n\n\n\n\n\n\nEn enkel ANOVA-tabell som Tabell 5.1 visar endast de tre huvudsakliga komponenterna, men olika programvaror kan ibland visa andra uppdelningar som standard. I en multipel linjär regressionsmodell är det vanligt att dela upp den förklarade variationen ytterligare, exempelvis i sekventiella kvadratsummor.\n\n\n5.1.2 Sekventiella kvadratsummor\nBeräkningarna för en ANOVA-tabell sker automatiskt i R när vi använder lm() och vi kan plocka ut tabellen från modellobjektet med hjälp av anova(), (se Tip 3.1).\n\n\nVisa kod\nanova(simpleModel) %&gt;% \n  round(4) %&gt;% \n  kable() %&gt;% \n  kable_styling(\"striped\")\n\n\n\n\nTabell 5.2: ANOVA-tabell från R\n\n\n\n\n \n  \n     \n    Df \n    Sum Sq \n    Mean Sq \n    F value \n    Pr(&gt;F) \n  \n \n\n  \n    species \n    2 \n    7015.3857 \n    3507.6929 \n    713.4929 \n    0 \n  \n  \n    bill_depth_mm \n    1 \n    818.5050 \n    818.5050 \n    166.4905 \n    0 \n  \n  \n    flipper_length_mm \n    1 \n    198.2269 \n    198.2269 \n    40.3210 \n    0 \n  \n  \n    body_mass_g \n    1 \n    160.3760 \n    160.3760 \n    32.6218 \n    0 \n  \n  \n    sex \n    1 \n    133.7191 \n    133.7191 \n    27.1995 \n    0 \n  \n  \n    Residuals \n    326 \n    1602.6899 \n    4.9162 \n     \n     \n  \n\n\n\n\n\n\n\n\nSom standard, delar R upp modellens kvadratsumma (SSR) i de enskilda förklarande variablerna med hjälp av sekventiella (även kallad betingade) kvadratsummor. En sekventiell kvadratsumma beskriver hur mycket variation en förklarande variabel bidrar med givet att modellen redan innehåller andra förklarande variabler.\nOrdningen som presenteras i Tabell 5.2 är ordningen som variablerna läggs till i modellen, till exempel visar andra raden \\(SS(\\text{bill\\_depth\\_mm} | \\text{species})\\), att näbbredden bidrar med 818.505 ytterligare unik förklarad variation av responsvariabeln som art inte redan har förklarat. Den tredje raden visar \\(SS(\\text{flipper\\_length\\_mm} | \\text{species}, \\text{bill\\_depth\\_mm})\\), det vill säga hur mycket ytterligare unik variation som fenlängden förklarar i en modell som inkluderar näbbredd och art.\nRent matematiskt beräknas den sekventiella kvadratsumman som en summa av antingen SSE eller SSR mellan två olika modeller, en utan den tillagda variabeln och en med variabeln inkluderad. Anta att vi vill lägga till variabel \\(X^*\\) till en modell som har \\(k\\) andra variabler, då ser beräkningen ut som följer:\n\\[\n\\begin{aligned}\nSS(X^*|X_1, \\ldots, X_k) &= SSE_{X_1, \\ldots, X_k} - SSE_{X_1, \\ldots, X_k, X^*} = \\\\\n&= SSR_{X_1, \\ldots, X_k, X^*} - SSR_{X_1, \\ldots, X_k}\n\\end{aligned}\n\\tag{5.2}\\]\nNotera att SSR ökar för varje ytterligare variabel som läggs till i modellen, medan SSE alltid minskar. En variation måste alltid vara positiv, därav beräknas \\(SSE_{reducerad} - SSE_{komplett}\\) eller \\(SSR_{komplett} - SSR_{reducerad}\\).\nSekventiella kvadratsummor påverkas av ordningen variablerna läggs till i modellen. Låt oss byta ordning på de förklarande variablerna när vi anpassar modellen:\n\n\nVisa kod\nmodel &lt;- lm(formula = bill_length_mm ~ sex + ., data = modelData)\n\nanova(model) %&gt;% \n  round(4) %&gt;% \n  kable() %&gt;% \n  kable_styling(\"striped\")\n\n\n\n\nTabell 5.3: Annan ordning på modellernas variabler\n\n\n\n\n \n  \n     \n    Df \n    Sum Sq \n    Mean Sq \n    F value \n    Pr(&gt;F) \n  \n \n\n  \n    sex \n    1 \n    1175.4780 \n    1175.4780 \n    239.1017 \n    0.0000 \n  \n  \n    species \n    2 \n    6975.5916 \n    3487.7958 \n    709.4457 \n    0.0000 \n  \n  \n    bill_depth_mm \n    1 \n    64.4987 \n    64.4987 \n    13.1196 \n    0.0003 \n  \n  \n    flipper_length_mm \n    1 \n    78.3815 \n    78.3815 \n    15.9434 \n    0.0001 \n  \n  \n    body_mass_g \n    1 \n    32.2629 \n    32.2629 \n    6.5625 \n    0.0109 \n  \n  \n    Residuals \n    326 \n    1602.6899 \n    4.9162 \n     \n     \n  \n\n\n\n\n\n\n\n\nI Tabell 5.3 ser vi att \\(SS(\\text{sex}) = 1175.478\\) vilket är betydligt högre än \\(SS(\\text{sex}|\\text{species}, \\text{bill\\_depth\\_mm}, \\text{flipper\\_length\\_mm}, \\text{body\\_mass\\_g}) = 133.7191\\) från Tabell 5.2. Variabeln kön bidrar med mycket variation när den är ensam i en modell, men när den läggs till i en modell som redan har andra variabler bidrar den inte med lika mycket unik information. Detta betyder att den förklarade variationen som variabeln bidrar med verkar finnas i övriga variabler också. Denna iakttagelse kommer vi komma tillbaka till i ett senare kapitel.\nNågonting som är lika i de två tabellerna är SSE. Vi har i båda modellerna inkluderad samma variabler vilket innebör att SST, SSR, och SSE överlag är densamma. Summan av alla sekventiella kvadratsummor ska fortfarande bli SSR oavsett ordningen på variablerna och på grund av den additiva egenskapen hos variationen har SST och SSE inte heller förändrats.",
    "crumbs": [
      "Regressionsanalys",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Statistisk inferens</span>"
    ]
  },
  {
    "objectID": "01-regression/04-statistical-inference.html#sec-stat-inference",
    "href": "01-regression/04-statistical-inference.html#sec-stat-inference",
    "title": "5  Statistisk inferens",
    "section": "5.2 Statistisk inferens",
    "text": "5.2 Statistisk inferens\nMed hjälp av de olika källorna av variation kan vi beräkna tester för hela eller delar av modellen i olika F-test, medan de enskilda parameterskattningarna och dess tillhörande medelfel kan användas i tester för enskilda lutningsparametrar.\n\n5.2.1 F-test för modellen\nI en multipel linjär regression är ett F-test för hela modellen bra att börja med för att se ifall minst en lutningsparameter är signifikant. Vi undersöker hypoteserna:\n\\[\\begin{align*}\nH_0&: \\beta_1 = \\beta_2 = \\beta_3 = \\cdots = \\beta_k = 0\\\\\nH_a&: \\text{Minst en av } \\beta_j \\text{ i } H_0 \\text{ är skild från } 0\n\\end{align*}\\]\nOm minst en lutningsparameter är signifikant betyder det att det finns åtminstone en variabel som bidrar med förklarad variation, att modellen är bättre än att använda enbart \\(\\bar{Y}\\). Testvariabeln undersöker relationen mellan den förklarande och oförklarande variationen genom dess medelkvadratsummor.\n\\[\nF_{test} = \\frac{SSR / k}{SSE / (n - (k+1))} = \\frac{MSR}{MSE}\n\\]\nTestvariabeln följer en F-fördelning som styrs av två frihetsgrader; \\(df1\\) från täljaren och \\(df2\\) från nämnaren i beräkningen, det vill säga modellens och felets frihetsgrader. Om \\(H_0\\) är sann kommer testvariabeln bli 0, medan om \\(H_a\\) är sann kommer testvariabeln bli ett stort positivt tal. Eftersom båda medelkvadratsummorna är positiva tal innebär det att kvoten alltid kommer vara positiv och vi kan förkasta \\(H_0\\) om testvariabeln befinner sig nog långt från 0.\n\n\nVisa kod\n# Skapar en funktion för att generera olika F-fördelningar\ngenerateFdistribution &lt;- function(df1, df2, n = 1000) {\n  x &lt;- seq(0, 5, length.out = n)  \n  y &lt;- df(x, df1, df2)  \n  tibble(x = x, y = y, df1 = df1, df2 = df2)  \n}\n\n# Skapar en lista med olika frihetsgrader\ndfs &lt;- list(c(5, 30), c(10, 100), c(20, 50), c(30, 300))\n\n# Genererar data\nFdistributions &lt;- dfs %&gt;%\n  purrr::map_df(~generateFdistribution(.x[1], .x[2]), .id = \"Distribution\") %&gt;%\n  mutate(Distribution = paste0(\"df1 = \", df1, \", df2 = \", df2))\n\n# Plot the F-distributions using ggplot2\nggplot(Fdistributions) + \n  aes(x = x, y = y, color = Distribution) +\n  geom_line(linewidth = 1) +\n  labs(\n    x = \"F-värde\",\n    y = \"Densitet\",\n    color = \"Frihetsgrader\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 14, face = \"bold\"),\n    legend.position = \"right\",\n    legend.title = element_text(face = \"bold\")\n  ) +\n  scale_color_manual(values = c(\"steelblue\", \"#d9230f\", \"black\", \"grey50\"))\n\n\n\n\n\nOlika F-fördelningar och deras frihetsgrader\n\n\n\n\nFör att få fram SSR från en ANOVA-tabell i R behöver vi summera de sekventiella kvadratsummorna. Vi kan sedan bearbeta tabellen för att få fram testvariabeln och använda frihetsgraderna för respektive källa i pf(lower.tail = FALSE) för att få fram p-värdet för testet.\n\n\nVisa kod\nanovaTable &lt;- anova(simpleModel)\n\n# Beräknar raden för SSR utifrån alla rader förutom SSE\nSSR &lt;- anovaTable[-nrow(anovaTable),] %&gt;% \n  summarize(across(Df:`Sum Sq`, ~sum(.x))) %&gt;% \n  mutate(`Mean Sq` = `Sum Sq` / Df,\n         `F value` = NA,\n         `Pr(&gt;F)` = NA)\n\n# Kombinerar SSR med SSE från ursprungliga tabellen\nsimpleAnova &lt;- SSR %&gt;% \n  add_row(anovaTable[nrow(anovaTable),]) %&gt;% \n  mutate(\n    `F value` = \n      ifelse(row_number() == 1,\n            `Mean Sq`[1] / `Mean Sq`[2], \n            NA),\n    `Pr(&gt;F)` = \n        ifelse(row_number() == 1, \n              pf(q = `F value`[1], df1 = Df[1], df2 = Df[2], lower.tail = FALSE), \n              NA)\n    )\n\nrownames(simpleAnova) &lt;- c(\"Model\", \"Residuals\")\n\nkable(simpleAnova, digits = 4) %&gt;% \n  kable_styling(\"striped\")\n\n\n\n\nTabell 5.4: Bearbetad och förenklad ANOVA tabell\n\n\n\n\n \n  \n     \n    Df \n    Sum Sq \n    Mean Sq \n    F value \n    Pr(&gt;F) \n  \n \n\n  \n    Model \n    6 \n    8326.213 \n    1387.7021 \n    282.2698 \n    0 \n  \n  \n    Residuals \n    326 \n    1602.690 \n    4.9162 \n     \n     \n  \n\n\n\n\n\n\n\n\nEftersom p-värdet är mindre än 5 procent, kan \\(H_0\\) förkastas och minst en av variablerna har ett samband med responsvariabeln.2\n\n\n5.2.2 Partiella F-test för grupper av parametrar\nIbland är vi intresserade att undersöka delar av modellen, en grupp med lutningsparametrar. Ett sådant fall är om vi vill undersöka en kvalitativ variabels påverkan eftersom den kan ha transformerats till flera indikatorvariabler alla med en tillhörande lutningsparameter. Ett annat tillfälle är om vi vill undersöka om flera variabler tillsammans bidrar med förklarad variation till modellen.\nIstället för att undersöka alla lutningsparametrar undersöks nu ett urval: \\[\\begin{align*}\nH_0&: \\beta_1 = \\beta_2 = \\beta_3 = \\cdots = \\beta_s = 0\\\\\nH_a&: \\text{Minst en av } \\beta_j \\text{ i } H_0 \\text{ är skild från } 0\n\\end{align*}\\] där \\(s\\) är antalet parametrar som undersöks.\nTestvariabeln för ett partiellt F-test kräver en komplett (betecknad \\(_F\\)) och en reducerad modell (betecknad \\(_R\\)). Den kompletta modellen består av alla variabler medan den reducerade modellen utgår från att \\(H_0\\) är sann och variablerna som undersöks har plockats bort från anpassningen. Vi kan välja att antingen använda SSR eller SSE för att beräkna hur mycket förklarad variation som försvinner mellan de två modellerna enligt samma princip som Ekvation 5.2.\n\\[\nF_{test} = \\frac{(SSR_F - SSR_R) / s}{SSE_F / (n - (k+1))} = \\frac{(SSE_R - SSE_F) / s}{SSE_F / (n - (k+1))}\n\\tag{5.3}\\]\nTestvariabeln är fortfarande F-fördelat med \\(s\\) respektive \\(n - (k+1)\\) frihetsgrader.\n\n5.2.2.1 Räkneknep för partiella F-test\nMed hjälp av Ekvation 5.2 kan Ekvation 5.3 formuleras på ett tredje sätt som underlättar vår analysprocess. Vi kan skriva om skillnaden i förklarad variation mellan den kompletta och reducerade modellen som en sekventiell kvadratsumma. Exempelvis kan vi vilja undersöka om variabeln art har ett samband med responsvariabeln. Eftersom den variabeln transformeras till två indikatorvariabler omfattar hypoteserna två lutningsparametrar.\n\\[\\begin{align*}\nH_0&: \\beta_{Chinstrap} = \\beta_{Gentoo} = 0\\\\\nH_a&: \\text{Minst en av } \\beta_j \\text{ i } H_0 \\text{ är skild från } 0\n\\end{align*}\\]\nDen reducerade modellen skapas utifrån att \\(H_0\\) är sann, det vill säga \\(\\beta_{Chinstrap} = \\beta_{Gentoo} = 0\\) och de två modellernas förklarade variation skulle betecknas som: \\[\n\\begin{aligned}\n  SSR_{R} &= SSR_{bill\\_depth\\_mm, flipper\\_length\\_mm, body\\_mass\\_g, sex} \\\\\n  SSR_{F} &= SSR_{bill\\_depth\\_mm, flipper\\_length\\_mm, body\\_mass\\_g, sex, species}\n\\end{aligned}\n\\]\nVi kan omformulera täljaren i Ekvation 5.3 till: \\[\nSS(species|bill\\_depth\\_mm, flipper\\_length\\_mm, body\\_mass\\_g, sex)\n\\] I de ANOVA-tabeller som presenterats tidigare kan vi få fram denna kvadratsumma direkt om art läggs till som den sista variabeln i modellen.\n\n\nVisa kod\nmodel &lt;- lm(bill_length_mm ~ bill_depth_mm + flipper_length_mm + body_mass_g + sex + species, data = modelData)\n\nanova(model) %&gt;% \n  round(4) %&gt;% \n  kable() %&gt;% \n  kable_styling(\"striped\")\n\n\n\n\nTabell 5.5: ANOVA-tabell från en modell där art läggs till sist\n\n\n\n\n \n  \n     \n    Df \n    Sum Sq \n    Mean Sq \n    F value \n    Pr(&gt;F) \n  \n \n\n  \n    bill_depth_mm \n    1 \n    518.9806 \n    518.9806 \n    105.5648 \n    0.0000 \n  \n  \n    flipper_length_mm \n    1 \n    4045.7248 \n    4045.7248 \n    822.9329 \n    0.0000 \n  \n  \n    body_mass_g \n    1 \n    6.1329 \n    6.1329 \n    1.2475 \n    0.2649 \n  \n  \n    sex \n    1 \n    68.4245 \n    68.4245 \n    13.9181 \n    0.0002 \n  \n  \n    species \n    2 \n    3686.9500 \n    1843.4750 \n    374.9776 \n    0.0000 \n  \n  \n    Residuals \n    326 \n    1602.6899 \n    4.9162 \n     \n     \n  \n\n\n\n\n\n\n\n\nEn ANOVA-tabell med sekventiella kvadratsummor beräknar ett partiellt F-test för respektive variabel (och dess parameter/parametrar) som undersöker huruvida variabeln bidrar med en signifikant ökning av den förklarade variationen till en modell som redan inkluderar variablerna ovanför. Tabell 5.5 beräknar nu det partiella F-test för art (\\(F_{test} = 374.9776\\)) som vi var intresserade av och vi kan direkt tolka p-värdet för testet (\\(p-värde &lt; 0.001\\)) som att minst en av lutningsparametrarna är signifikant skild från 0.\nOm vi genomför ett partiellt F-test för flera variabler kan vi inte använda p-värden som anges i tabellen då hypoteserna omfattar fler lutningsparametrar/variabler än vad de sekventiella kvadratsummorna visar. Anta att vi vill undersöka om art och kön tillsammans bidrar något till modellen. Hypotesprövningen skulle då omfatta:\n\\[\n\\begin{aligned}\nH_0&: \\beta_{sexMale} = \\beta_{Chinstrap} = \\beta_{Gentoo} = 0\\\\\nH_a&: \\text{Minst en av } \\beta_j \\text{ i } H_0 \\text{ är skild från } 0\n\\end{aligned}\n\\]\nDen sekventiella kvadratsumman som vi vill använda anges som \\(SS(species, sex|bill\\_depth\\_mm, flipper\\_length\\_mm, body\\_mass\\_g)\\) och vi kan beräkna fram detta värde genom att summera de två variablernas SS från Tabell 5.5.\n\\[\n\\begin{aligned}\nSS(species, sex|bill\\_depth\\_mm, flipper\\_length\\_mm, body\\_mass\\_g) = \\\\\nSS(species|bill\\_depth\\_mm, flipper\\_length\\_mm, body\\_mass\\_g, sex) + \\\\\nSS(sex|bill\\_depth\\_mm, flipper\\_length\\_mm, body\\_mass\\_g)\n\\end{aligned}\n\\] Alternativet är att anpassa två modeller i R, den kompletta och reducerade och läsa av SSE eller summera SSR från respektive ANOVA-tabell.\n\n\n5.2.2.2 Partiellt F-test för specifika värden\nVi kan ställa upp en generell modell som: \\[\\begin{align*}\n  Y = \\beta_0 + \\beta_1 \\cdot X_1 + \\beta_2 \\cdot X_2 + \\beta_3 \\cdot X_3 +\\beta_4 \\cdot X_4 +\\beta_5 \\cdot X_5+ E\n\\end{align*}\\]\nOm vi ska undersöka specifika parametrars värden (som inte är 0) kan vi genomföra följande härledning. Anta \\(H_0:\\) \\(\\beta_2=4\\) och \\(\\beta_5 = -2\\) som ska undersökas med ett test.\n\\[\\begin{align*}\nY &= \\beta_0 + \\beta_1 \\cdot X_1 + 4 \\cdot X_2 + \\beta_3 \\cdot X_3 +\\beta_4 \\cdot X_4 - 2 \\cdot X_5+ E\\\\\nY - 4 \\cdot X_2 + 2 \\cdot X_5 &= \\beta_0 + \\beta_1 \\cdot X_1  + \\beta_3 \\cdot X_3 +\\beta_4 \\cdot X_4 + E \\\\\nY^* &= \\beta_0 + \\beta_1 \\cdot X_1  + \\beta_3 \\cdot X_3 +\\beta_4 \\cdot X_4 + E\n\\end{align*}\\]\n\\(Y^*\\) kan anses vara en reducerad modell för ett F-test. I R kan detta inte lösas genom anova() utan måste beräknas ‘’för hand’’ genom att anpassa två modeller, den kompletta och den reducerade.\n\n\n\n5.2.3 t-test för enskilda parametrar\nAtt använda ANOVA-tabellen för att undersöka enskilda parametrar är inte lämpligt då det kräver att variabeln anges sist i modelleringen för att det partiella F-testet undersöker just den enskilda variabeln i relation till övriga modellen. Istället bör vi använda t-test för respektive parameter.\nFormellt undersöks hypoteserna: \\[\n\\begin{aligned}\n  H_0&: \\beta_j = 0\\\\\n  H_a&: \\beta_j \\ne 0\n\\end{aligned}\n\\] där \\(j\\) är någon av lutningsparametrarna i en anpassad modell.\nTestvariabeln beräknas utifrån den skattade lutningsparametern och dess medelfel: \\[\n\\begin{aligned}\nt_{test} = \\frac{b_j - 0}{s_{b_j}}\n\\end{aligned}\n\\]\nTestvariabeln är t-fördelad givet \\(H_0\\) med \\(n-(k+1)\\) frihetsgrader.\nI R används t-test i koefficienttabellen som vi kan plocka ut ur summary()-objektet genom coef().\n\n\nVisa kod\nsummary(simpleModel) %&gt;% \n  coef() %&gt;% \n  round(4) %&gt;% \n  kable(format = \"markdown\",\n        col.names = c(\"Parameter\", \"Estimate\", \"Std. Error\", \"t value\", \"Pr(&gt;t)\"), \n        parse = TRUE) %&gt;% \n  kable_styling(\"striped\")\n\n\n\n\nTabell 5.6: Koefficienttabell för en modell med tillhörande t-test för enskilda parametrar\n\n\n\n\n \n  \n    Parameter \n    Estimate \n    Std. Error \n    t value \n    Pr(&gt;t) \n  \n \n\n  \n    (Intercept) \n    15.0166 \n    4.3742 \n    3.4330 \n    0.0007 \n  \n  \n    speciesChinstrap \n    9.5655 \n    0.3497 \n    27.3508 \n    0.0000 \n  \n  \n    speciesGentoo \n    6.4044 \n    1.0304 \n    6.2154 \n    0.0000 \n  \n  \n    bill_depth_mm \n    0.3130 \n    0.1541 \n    2.0316 \n    0.0430 \n  \n  \n    flipper_length_mm \n    0.0686 \n    0.0232 \n    2.9608 \n    0.0033 \n  \n  \n    body_mass_g \n    0.0011 \n    0.0004 \n    2.5617 \n    0.0109 \n  \n  \n    sexmale \n    2.0297 \n    0.3892 \n    5.2153 \n    0.0000 \n  \n\n\n\n\n\n\n\n\nI Tabell 5.6 ser vi att p-värdet för alla t-testen är väldigt låga (nära 0). För varje enskilda hypotesprövning kan vi på fem procents signifikans förkasta \\(H_0\\) vilket betyder att variabeln har en signifikant påverkan på responsvariabeln.\n\n\n\n\n\n\nViktigt\n\n\n\nOm en parameter inte anses signifikant är det en motivering till att variabeln kan plockas bort, vi anpassar en reducerad modell och en ny analys påbörjas. Om en variabel plockas bort kommer de övriga parameterskattningarna förändras och tolkningar samt inferens behöver uppdateras.\n\n\n\n\n5.2.4 Konfidensintervall för \\(\\beta\\)\nSlutsatsen vi kan dra från dessa hypotesprövningar är att modellen innehåller variabler som alla har ett signifikant samband med responsvariabeln. Om vi vill tolka magnituden av effekten gentemot populationen, inte bara om sambandet är signifikant, behöver vi beräkna intervallskattningar.\n\\[\n\\begin{aligned}\nb_j \\pm t_{n - (k+1); 1- \\alpha/2} \\cdot s_{b_j}\n\\end{aligned}\n\\]",
    "crumbs": [
      "Regressionsanalys",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Statistisk inferens</span>"
    ]
  },
  {
    "objectID": "01-regression/04-statistical-inference.html#enkla-utvärderingsmått",
    "href": "01-regression/04-statistical-inference.html#enkla-utvärderingsmått",
    "title": "5  Statistisk inferens",
    "section": "5.3 Enkla utvärderingsmått",
    "text": "5.3 Enkla utvärderingsmått\nBara för att en modell är lämplig, uppfyller modellantaganden och innehåller signifikanta parametrar, betyder det inte att modellen är den bästa som kan skapas eller överhuvudtaget bra. Med hjälp av olika utvärderingsmått kan vi få en överblick på hur bra modellen är.\nFörklaringsgraden (\\(R^2\\)) beskriver hur stor andel av den totala variationen som förklaras av modellens förklarande variabler. Med denna beskrivning kan vi beräkna \\(R^2\\) som: \\[\n\\begin{aligned}\n  R^2 = \\frac{SSR}{SST} = 1 - \\frac{SSE}{SST}\n\\end{aligned}\n\\] På grund av att SSR alltid blir större ju fler variabler som en modell innehåller, behöver vi justera måttet för att kunna jämföra modeller av olika storlekar. Istället bör vi titta på den justerade förklaringsgraden (\\(R^2_{a}\\)) för att se vilken modell som är bäst. En förbättrad \\(R^2_{a}\\) betyder att modellen har tagit bort onödig komplexitet.\n\\[\n\\begin{aligned}\n  R^2_a = 1 - \\frac{SSE / (n - (k+1))}{SST / (n - 1)}\n\\end{aligned}\n\\]",
    "crumbs": [
      "Regressionsanalys",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Statistisk inferens</span>"
    ]
  },
  {
    "objectID": "01-regression/04-statistical-inference.html#footnotes",
    "href": "01-regression/04-statistical-inference.html#footnotes",
    "title": "5  Statistisk inferens",
    "section": "",
    "text": "Frihetsgrader beskriver egentligen hur många bitar oberoende information som finns för en beräkning. Tänk tillbaka på beräkningen av en stickprovsstandardavvikelse vars frihetsgrader är \\(n - 1\\), antalet observationer - 1, för att vi skattar medelvärdet när vi beräknar standardavvikelsen.↩︎\nOm vi hade tagit ett annat beslut (att inte förkasta nollhypotesen) hade det inte varit relevant att fortsätta med analysen, eller åtminstone att fokusera resterande analys på att undersöka varför en multipel linjär regressionsmodell som vi förväntar har ett samband utifrån parvisa spridningsdiagram inte visar på det tillsammans.↩︎",
    "crumbs": [
      "Regressionsanalys",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Statistisk inferens</span>"
    ]
  },
  {
    "objectID": "01-regression/05-using-the-model.html",
    "href": "01-regression/05-using-the-model.html",
    "title": "6  Använda modellen",
    "section": "",
    "text": "6.1 Simultan inferens\nNär en modell innehåller flera förklarande variabler, alla med minst en tillhörande lutningsparameter, kommer vi genomföra flera inferensberäkningar om vi vill dra slutsatser om flera parametrar eller intervallskatta prediktioner för flera nya observationer. Varje hypotesprövning eller intervallskattning som beräknas utgår från att vi alltid har en risk att fatta fel beslut, vilket innebär att denna risk inflateras ju fler parametrar eller värden som undersöks med enskilda beräkningar. Typ I felet (signifikansnivån) beskriver risken att förkasta en sann \\(H_0\\).\nDen satiriska vetenskapliga serietecknaren xkcd har publicerat en serie som berör signifikansen av hypotesprövningar.\nEn grupp forskare, som hellre vill spela Minecraft, har fått i uppdrag att undersöka effekten av Jelly Beans på förekomsten av acne. Forskarna undersöker 20 olika färger av Jelly Beans med en signifikansnivå på fem procent. En utav färgerna visade sig ha en signifikant påverkan på förekomsten av acne och som vi ser i nyhetsartikeln allra sist i serien blir denna upptäckt innehållet på första sidan.\nVad är det som faktiskt har hänt här och är det korrekt att annonsera ut detta för världen? Vi kan skapa ett liknande simulerat exempel genom att genomföra 20 stycken urval från en population där vi vet det sanna medelvärdet.",
    "crumbs": [
      "Regressionsanalys",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Använda modellen</span>"
    ]
  },
  {
    "objectID": "01-regression/05-using-the-model.html#simultan-inferens",
    "href": "01-regression/05-using-the-model.html#simultan-inferens",
    "title": "6  Använda modellen",
    "section": "",
    "text": "Figur 6.1: xkcd, CC BY-NC 2.5\n\n\n\n\n\n\n6.1.1 Simulering\n\n\nVisa kod\n## Anger ett seed\nset.seed(12345)\n\n## Simulerar ett datamaterial från en given population med sanna värden\ndata &lt;- \n  replicate(\n    n = 20,\n    expr = rnorm(n = 30, mean = 0, sd = 1)\n  )\n\n\nObjektet data innehåller nu 20 stycken kolumner med 30 observationer i varje och varje kolumn är ett nytt urval (motsvarande en undersökning av en färg Jelly Bean). Alla dessa urval kommer från en population där \\(\\mu = 0\\) och det skulle vi kunna undersöka genom ett enkelt t-test för ett medelvärde. Formellt undersöks hypoteserna:\n\\[\n\\begin{aligned}\n  H_0 &: \\mu = 0\\\\\n  H_A &: \\mu \\ne 0\n\\end{aligned}\n\\]\ndär testvariabeln är \\[\nt_{test} = \\frac{\\bar{x} - 0}{ \\frac{s}{\\sqrt{n}}}\n\\]\n\n\nVisa kod\n## Testvariabeln för första urvalet\ntTest &lt;- (mean(data[,1]) - 0) / (sd(data[,1]) / sqrt(nrow(data))) \n\n\np-värdet för testet kan beräknas utifrån t-fördelningen med \\(n - 1\\) frihetsgrader. Vi måste dock ta hänsyn till att vi undersöker en dubbelsidig mothypotes vilket innebär att p-värdet är både större än den positiva testvariabeln och mindre än den negativa testvariabeln. Med hjälp av symmetrin i t-fördelningen kan vi räkna ut detta genom att beräkna absolutbeloppet av testvariabeln och multiplicera ytan större än detta värde med \\(2\\).\n\n\nVisa kod\n## Beräkning av p-värdet\npValue &lt;- 2 * pt(q = abs(tTest), df = nrow(data) - 1, lower.tail = FALSE) \n\n\nVi kan också använda inbyggda funktionen t.test() för att få fram samma resultat, där standardargumentet är att vi undersöker om populationsmedelvärdet är skilt från 0.\n\n\nVisa kod\nt.test(data[,1])\n\n\n\n    One Sample t-test\n\ndata:  data[, 1]\nt = 0.46006, df = 29, p-value = 0.6489\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n -0.2715323  0.4291463\nsample estimates:\n mean of x \n0.07880701 \n\n\nVi kan replikera detta test och spara p-värdet för alla 20 olika utval med hjälp av apply(). Funktionen genomför en angiven funktion för varje element i ett objekt, i detta fall anger vi att den ska genomföra beräkningen för varje kolumn (MARGIN = 2) i objektet data.\n\n\nVisa kod\n## Kör samma funktion (t.test) för varje kolumn (MARGIN = 2) av datamaterialet\npValues &lt;- \n  apply(\n    X = data,\n    MARGIN = 2,\n    FUN = function(x){\n      t.test(x)$p.value\n    }\n  )\n\n\n\n\n\n\nTabell 6.1: p-värden för t-testet i respektive urval\n\n\n\n\n \n  \n    p-värde \n  \n \n\n  \n    0.649 \n  \n  \n    0.195 \n  \n  \n    0.036 \n  \n  \n    0.895 \n  \n  \n    0.806 \n  \n  \n    0.978 \n  \n  \n    0.341 \n  \n  \n    0.765 \n  \n  \n    0.981 \n  \n  \n    0.269 \n  \n  \n    0.092 \n  \n  \n    0.497 \n  \n  \n    0.228 \n  \n  \n    0.467 \n  \n  \n    0.114 \n  \n  \n    0.333 \n  \n  \n    0.268 \n  \n  \n    0.790 \n  \n  \n    0.290 \n  \n  \n    0.896 \n  \n\n\n\n\n\n\n\n\nI Tabell 6.1 är det endast ett test vars tillhörande p-värde är mindre än \\(\\alpha = 0.05\\) (urval 3), motsvarande den gröna Jelly Bean. Eftersom vi skapat dessa urval från en sann fördelning, vet vi att populationen har medelvärde 0 vilket innebär att vi gör ett fel av typ I, förkastar en sann \\(H_0\\). Med en signifikansnivå på 5 procent räknar vi att i 1 av 20 fall (5%) så fattar vi fel beslut utav ren slump. Desto fler tester som genomförs desto större risk att minst en av dessa tester kommer generera ett typ I-fel och att tolka ett enskilt test med 5 procents signifikans är missvisande.\nUnder antagandet att de olika urvalen är oberoende av varandra kan vi räkna ut den slutgiltiga risken för typ-I fel i minst en av testerna med hjälp av multiplikationssatsen för oberoende händelser:\n\\[\n\\begin{aligned}\n1 - (0.95 \\cdot 0.95 \\cdot  0.95 \\cdot  0.95 \\cdot  0.95 \\cdot  0.95 \\cdot  0.95 \\cdot  0.95 \\cdot  0.95 &\\cdot  0.95 \\cdot  0.95 \\cdot  0.95 \\cdot  0.95 \\cdot  0.95 \\cdot  0.95 \\cdot  0.95 \\cdot  0.95 \\cdot  0.95 \\cdot  0.95 \\cdot  0.95)\\\\\n&1 - (0.95^{20})\\\\\n&0.6415141\n\\end{aligned}\n\\] Risken att minst en utav 20 tester har förkastat en sann \\(H_0\\) är istället ca 64% istället för 5%.\n\n\n6.1.2 Familjekonfidens\nNär vi genomför flera tester kan vi istället justera konfidensgraden för varje individuella test så att vi får en sammanfattande konfidensnivå som vi också använder i tolkningen. Den enklaste varianten är att beräkna Bonferronis familjekonfidens där konfidensgraden beräknas enligt: \\[\n\\begin{aligned}\n  1 - \\alpha/2 \\Rightarrow 1 - \\alpha/(2 \\cdot g)\n\\end{aligned}\n\\] där \\(g\\) är antalet tester som genomförs.\n\n\nVisa kod\n# Plockar ur testvariabeln\nstatistic &lt;- \n  apply(\n    X = data,\n    MARGIN = 2,\n    FUN = function(x){\n      t.test(x)$statistic\n    }\n  )\n\ntibble(`Testvariabel` = statistic) %&gt;% \n  kable(digits = 3, caption.placement = \"top\") %&gt;% \n  kable_styling(\"striped\", full_width = FALSE) \n\n\n\n\nTabell 6.2: Beräknad testvariabel för respektive urval\n\n\n\n\n \n  \n    Testvariabel \n  \n \n\n  \n    0.460 \n  \n  \n    1.328 \n  \n  \n    2.196 \n  \n  \n    0.134 \n  \n  \n    -0.248 \n  \n  \n    0.028 \n  \n  \n    0.969 \n  \n  \n    0.302 \n  \n  \n    0.024 \n  \n  \n    -1.127 \n  \n  \n    1.744 \n  \n  \n    0.689 \n  \n  \n    1.231 \n  \n  \n    -0.737 \n  \n  \n    1.629 \n  \n  \n    -0.984 \n  \n  \n    -1.129 \n  \n  \n    0.268 \n  \n  \n    1.077 \n  \n  \n    0.132 \n  \n\n\n\n\n\n\n\n\nDet justerade kritiska tabellvärdet är \\(\\pm t_{30 - 1, 1 - 0.05 / (2 \\cdot 20)} = \\pm\\) 3.31 istället för \\(\\pm t_{30 - 1, 1 - 0.05 / (2)} = \\pm\\) 2.045 vilket innebär att ingen testvariabel i Tabell 6.2 ligger extremare än det justerade. Med en simultan signifikansnivå på fem procent förkastas ingen av testernas nollhypoteser.\n\n\n6.1.3 Familjekonfidens för lutningsparametrar\nMed hjälp av Bonferroni:s familjekonfidens kan vi justera formlerna för intervallskattning av lutningsparametrarna. En generell formel för en intervallskattning kan skrivas såsom: \\[\n\\text{punktskattning} \\pm \\text{tabellvärde} \\cdot \\text{medelfel}\n\\tag{6.1}\\]\nI fallet för \\(\\beta_1\\) blir formeln: \\[\n    b_1 \\pm t_{n - (k+1); 1- \\alpha/2} \\cdot s_{b_1}\n\\] Bonferroni:s familjekonfidens leder till att formeln justeras enligt: \\[\n    b_1 \\pm t_{n - (k+1); 1- \\alpha/(2\\cdot g)} \\cdot s_{b_1}\n\\] , där \\(g\\) är antalet lutningsparametrar som ska undersökas samtidigt.\n\\(t_{n-(k+1); 1 - \\alpha/(2\\cdot g)}\\) brukar också anges som \\(B\\). När \\(g\\) är stort, kommer värden från \\(t\\)-fördelningen också vara stora vilket i slutändan kan leda till intervall som inte ger någon vettig information. Därför följer att Bonferronis metod är lämpligast att användas när antalet intervall eller tester är få.",
    "crumbs": [
      "Regressionsanalys",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Använda modellen</span>"
    ]
  },
  {
    "objectID": "01-regression/05-using-the-model.html#prediktioner",
    "href": "01-regression/05-using-the-model.html#prediktioner",
    "title": "6  Använda modellen",
    "section": "6.2 Prediktioner",
    "text": "6.2 Prediktioner\nPrediktioner innebär att vi skattar värdet på Y givet observerade värden på X med hjälp av den anpassade regressionslinjen. Dessa prediktioner kommer falla längsmed linjen vilket ytterligare motiverar att modellen behöver vara lämplig och bra. Vi vill inte att en prediktion i ett område är mer träffsäker än en prediktion i en annan eller att regressionslinjen generellt är en dålig representation av responsvariabeln.\nModellen utgår från en specifik definitionsmängd, de observerade värdena på \\(\\mathbf{X}\\), och det är även inom denna mängd som prediktioner bör göras. Det finns vissa tillfällen, till exempel inom tidsserieanalys, där prediktioner görs utanför definitionsmängden men där finns ett beroende i tiden som möjliggör dessa extrapoleringar. Inom “vanlig” regression bör vi undvika att extrapolera regressionslinjen utanför definitionsmängden.\n\n6.2.1 Medelvärdet av Y för givna \\(\\mathbf{X}\\)\nOm vi är intresserad av det genomsnittliga värdet på responsvariabeln för alla nya observationer med givna värden på \\(\\mathbf{X}\\) kan vi skatta \\(\\mu_{Y|{\\mathbf{X}_0}}\\) där \\(\\mathbf{X}_0\\) innehåller värden för den nya observationen.\n\\[\n\\mathbf{X}_0 = \\begin{bmatrix}\n    1 \\\\\n    X_{1,0}\\\\\n    \\vdots\\\\\n    X_{k,0}\n    \\end{bmatrix}\n\\] Vi utgår från den anpassade regressionsmodellen och beräknar en punktprediktion av responsvariabeln enligt: \\[\n\\hat{Y}_{\\mathbf{X}_0} = \\mathbf{X}_0'\\boldsymbol{\\hat{\\beta}}\n\\]\nMedelfelet för skattningen tar hänsyn till:\n\\[\ns^2_{\\hat{Y}_{\\mathbf{X}_0}} = \\mathbf{X}_0'\\mathbf{s}^2_{\\boldsymbol{\\hat{\\beta}}}\\mathbf{X}_0\n\\]\nIntervallskattningen för ett genomsnitt blir ett konfidensintervall.\n\n\n6.2.2 Enskild prediktion av Y för givna \\(\\mathbf{X}\\)\nOm vi istället är intresserad av ett enskilt värde på Y med givna värden på \\(\\mathbf{X}\\), kan vi skatta \\(Y_{\\mathbf{X}_0}\\).\n\\[\ns^2_{pred} = MSE + s^2_{\\hat{Y}_{\\mathbf{X}_0}}\n\\] Intervallskattningen för ett värde av Y blir ett prediktionsintervall.\n\n\n6.2.3 Skattning av flera konfidensintervall\nOm vi vill beräkna konfidensband för hela regressionslinjen, i.e. \\(\\mu_Y\\) för alla punkter av \\(\\mathbf{X}\\), kan Working-Hotelling:s metod skapa denna region. Strukturen från formel Ekvation 6.1 ger då följande formel:\n\\[\n\\begin{aligned}\n  \\hat{Y}_{\\mathbf{X}_0} \\pm W \\cdot s_{\\hat{Y}_{\\mathbf{X}_0}}\n\\end{aligned}\n\\tag{6.2}\\]\ndär \\(W^2 = 2\\cdot F_{k+1; n-(k+1); 1-\\alpha}\\).\nDenna beräkning tar redan hänsyn till alla möjliga punkter av \\(\\mathbf{X}\\) som regressionslinjen täcker, vilket innebär att samma formler kan användas för enstaka nya observationer, \\(\\mathbf{X}_{\\mathbf{X}_0}\\).\nÄven för prediktioner av responsvariabeln kan Bonferroni användas enligt: \\[\n  \\hat{Y}_{\\mathbf{X}_0} \\pm B \\cdot s_{\\hat{Y}_{\\mathbf{X}_0}}\n\\tag{6.3}\\] där \\(B = t_{n-(k+1); 1 - \\alpha/(2\\cdot g)}\\)\nEkvation 6.2 ger generellt smalare intervall än Ekvation 6.3 då \\(W\\) är samma värde oavsett hur många intervall som skattas medan \\(B\\) ökar med antalet \\(g\\).\n\n\n6.2.4 Skattning av flera prediktionsintervall\nFör enstaka nya observationer beräknas istället prediktionsintervall vilket innebär att skattningens medelfel förändras. Metoder för att beräkna \\(g\\) prediktionsintervall är Scheffé och Bonferronis metoder.\nScheffés metod: \\[\\begin{align}\n   \\hat{Y}_{\\mathbf{X}_0} \\pm S \\cdot s_{pred}\n\\end{align}\\] där \\[\\begin{align*}\n    S &= g \\cdot F_{g; n-k+1; 1 - \\alpha}\\\\\n    s_{pred} &= \\sqrt{MSE + s^2_{\\hat{Y}_{\\mathbf{X}_0}}}\n\\end{align*}\\]\nBonferronis metod: \\[\\begin{align}\n    \\hat{Y}_{\\mathbf{X}_0} \\pm B \\cdot s_{pred}\n\\end{align}\\] där \\[\\begin{align*}\n    B = t_{n-k+1; 1 - \\alpha/(2\\cdot g)}\n\\end{align*}\\]\nBåda dessa metoder kommer skapa bredare intervall i relation till antalet intervall som skattas eftersom \\(g\\) inkluderas i båda beräkningarna. Detta skiljer sig från Ekvation 6.2 där \\(W\\) inte blir större desto fler intervall.",
    "crumbs": [
      "Regressionsanalys",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Använda modellen</span>"
    ]
  },
  {
    "objectID": "01-regression/06-complex-predictors.html",
    "href": "01-regression/06-complex-predictors.html",
    "title": "7  Komplexa samband",
    "section": "",
    "text": "7.1 Interaktioner",
    "crumbs": [
      "Regressionsanalys",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Komplexa samband</span>"
    ]
  },
  {
    "objectID": "01-regression/06-complex-predictors.html#sec-komplex-modell",
    "href": "01-regression/06-complex-predictors.html#sec-komplex-modell",
    "title": "7  Komplexa samband",
    "section": "",
    "text": "7.1.1 Simpson’s paradox",
    "crumbs": [
      "Regressionsanalys",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Komplexa samband</span>"
    ]
  },
  {
    "objectID": "01-regression/06-complex-predictors.html#polynom",
    "href": "01-regression/06-complex-predictors.html#polynom",
    "title": "7  Komplexa samband",
    "section": "7.2 Polynom",
    "text": "7.2 Polynom",
    "crumbs": [
      "Regressionsanalys",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Komplexa samband</span>"
    ]
  },
  {
    "objectID": "01-regression/06-complex-predictors.html#multikollinearitet",
    "href": "01-regression/06-complex-predictors.html#multikollinearitet",
    "title": "7  Komplexa samband",
    "section": "7.3 Multikollinearitet",
    "text": "7.3 Multikollinearitet\nBla",
    "crumbs": [
      "Regressionsanalys",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Komplexa samband</span>"
    ]
  }
]