[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistisk teori och tillämpningar i R",
    "section": "",
    "text": "Inledning\nOm du har några kommentarer, synpunkter eller vill meddela något fel? Maila mig på isak.hietala@liu.se.",
    "crumbs": [
      "Regressionsanalys",
      "Inledning"
    ]
  },
  {
    "objectID": "00-programming/00-basics-R.html",
    "href": "00-programming/00-basics-R.html",
    "title": "Grunderna i R",
    "section": "",
    "text": "Introduktion\nR kännetecknas av sina paket med funktioner som vi kan använda för att genomföra olika operationer. Vi kan se på funktionerna som olika verktyg ämnade att lösa specifika problem, till exempel om vi vill slå in en spik i en bräda kan vi använda en hammare, och paketen som olika verktygslådor fokuserade på att genomföra en viss form utav arbete. När vi installerar R för första gången får vi tillgång till en stor mängd olika paket, men vi har möjligheten att utöka dessa väldigt enkelt. Vem som helst skapa funktioner och paket för allmänheten och ibland finns det ett behov av att ladda ner nya paket (motsv. köpa in nya verktygslådor) för att genomföra en viss sorts analys.\nDet finns flertalet gränssnitt som hjälper till att programmera med R, men vi rekommenderar att använda RStudio. Detta visuella gränssnitt medför att vi inte behöver programmera med en kommandotolk. För att installera R och RStudio kan du följa instruktionerna här. Detta underlag är skapad med R version 4.4.1 (2024-06-14 ucrt).",
    "crumbs": [
      "Regressionsanalys",
      "**DEL I - Programmering i R**",
      "Grunderna i R"
    ]
  },
  {
    "objectID": "00-programming/00-basics-R.html#introduktion",
    "href": "00-programming/00-basics-R.html#introduktion",
    "title": "Grunderna i R",
    "section": "",
    "text": "Viktigt\n\n\n\nNär vi programmerar i R, sparas objekt som vi skapar i datorns arbetsminne (RAM-minne). Arbetsminnet kan vi säga motsvarar ett korttidsminne, vilket innebär att vi har ett mindre utrymme att arbeta med än datorns hårddisk och information finns endast där temporärt. När vi startar upp R börjar vi en session som avslutas när vi stänger ner programmet. Vi kan spara objekt för att använda de senare i samma session, men dessa rensas när vi stänger ner programmet. Om vi vill använda objekt mellan sessioner kan vi spara ner information till datorns hårddisk, eller dess långtidsminne, på några olika sätt.\n\n\n\nStarta RStudio\nNär vi startar RStudio för första gången, delas programmet in i tre olika fönster. Det är starkt rekommenderat att alltid börja med att öppna eller skapa ett dokument där kod sparas (även kallad ett R Script eller skript) genom File -&gt; New file -&gt; R Script (eller kortkommandot Ctrl el. Command + Shift + N). När detta skript öppnas delas RStudio in i fyra olika fönster. Ett skript har ändelsen .R men är i grunden en textfil och kan öppnas i alla enkla texthanterare, t.ex. Notepad, om vi vill redigera koden utanför R eller RStudio.\n\nKort genomgång av RStudio\nNär ett skript skapats innehåller nu det övre vänstra fönstret denna fil. Här kan vi skriva kod som kan sparas ned och användas igen nästa gång vi öppnar programmet. Skriptet kan också skickas mellan datorer och är ett bra sätt att kunna dela med sig utav vad det är man jobbar med. Det rekommenderas starkt att alltid skriva kod i ett skript för att ha möjligheten att återanvända och redigera koden på ett lätt sätt.\nDet nedre vänstra fönstret innehåller en konsol. Här visas all kod som vi kör och tillhörande utskrifter och vi kan också skriva kod direkt i konsollen. Det som skiljer användandet av konsollen med ett skript är att all kod som vi skriver i konsollen inte sparas någonstans och kommer försvinna nästa gång vi startar upp programmet. Det är också svårt att redigera kod, till exempel om man gjort något stavfel eller måste revidera kod som man skrivit för länge sedan.\nI det nedre högra fönstret kan vi se olika flikar som visar filer från datorn, skapade visualiseringar, en översikt av de paket som vi har tillgängliga, samt få hjälp om olika funktioner i hjälpdokumentationen. Paket i R har inbyggda hjälpfiler som kan underlätta förståelsen av vad en funktion gör, hur den arbetar, vad den behöver för att köras med mera. Det rekommenderas att alltid börja med att titta i dokumentationen för en funktion om man är osäker på vad det är den gör.\nDet övre högra fönstret innehåller en översikt av alla objekt som vi skapar under en session och som R lagrar i korttidsminnet. Objekten som vi ser i RStudios Environment-flik kan vi klicka på för att antingen få mer detaljer om objektet, exempelvis hur stort ett visst objekt är eller om den innehåller fler delobjekt.\nVi kommer återkomma till dessa olika fönster senare i underlaget.\n\n\n\nR kod i underlaget\nI detta underlag kommer kodexempel (ljus bakgrund) visas som du kan kopiera till din egna dator. En knapp finns överst i det högra hörnet som kopierar allt innehåll i kodblocket. Från exemplena visas också utskrifter (mörk bakgrund) från koden som vi i RStudio ser i konsollen.\n\n\nVisa kod\n# Exempelkod som kan kopieras med en knapp i det övra högra hörnet.\n\n\n\n\nUtskrift från koder.",
    "crumbs": [
      "Regressionsanalys",
      "**DEL I - Programmering i R**",
      "Grunderna i R"
    ]
  },
  {
    "objectID": "00-programming/00-basics-R.html#sec-basics-R",
    "href": "00-programming/00-basics-R.html#sec-basics-R",
    "title": "Grunderna i R",
    "section": "Grundläggande programmering i R",
    "text": "Grundläggande programmering i R\nR är ett kraftfullt språk som kan genomföra väldigt komplexa och avancerade beräkningar. Innan vi kommer dit måste vi första börja med grunderna i R och en utav de vanligaste användningsområdena för R är att använda språket som en vanlig miniräknare. R kommer läsa varje rad kod som en enskild operation och vi kan köra kod på flera olika sätt.\nVi kan antingen använda knappen uppe till höger av skriptet som heter Run eller använda kortkommandot Ctrl el. Command + Enter för att köra enstaka rader kod. R kan också köra flera rader kod samtidigt om vi markerar flera rader och klickar på Run eller använder kortkommandot.\n\nR som miniräknare\nTill att börja med kan vi använda matematiska operatorer såsom +, -, *, och /.\n\n\nVisa kod\n## Addition\n2 + 3\n\n\n[1] 5\n\n\nVisa kod\n## Subtraktion\n5 - 2\n\n\n[1] 3\n\n\nVisa kod\n## Multiplikation\n4 * 5\n\n\n[1] 20\n\n\nVisa kod\n## Division\n20 / 4\n\n\n[1] 5\n\n\nR kommer genomföra dessa operationer mellan siffrorna och skriva ut svaret i konsollen. De vanliga räknereglerna för matematiska operatorer gäller här vilket innebär att t.ex. kommer:\n\n\nVisa kod\n## Räkneregler\n1 + 2 * 3\n\n\n[1] 7\n\n\ngenerera ett annorlunda svar än:\n\n\nVisa kod\n## Räkneregler forts.\n(1 + 2) * 3\n\n\n[1] 9\n\n\n\nKommentarer i koden\nI ovanstående kodexempel ser vi också något speciellt. Om du kopierar all text från ett utav blocken och kör den på egen hand, kommer R endast skriva ut resultat från raden som inte innehåller #. En rad som börjar med # kommer R istället läsa som en kommentar. Detta medför att vi kan kommentera kod som vi skriver vilket har många fördelar.\nDels kan vi i anslutning till specifika bitar kod själva beskriva vad koden gör och vad för resultat som vi förväntar oss att få. Det är väldigt lätt att man i ett stort skript tappar bort sig själv, eller när man öppnar skriptet vid nästa session har glömt bort vad det är koden gör. Med kommentarer kan vi lättare komma ihåg vad de olika delarna gör, men framförallt om vi skickar koden till någon annan kan vi skicka med tillhörande instruktioner så att det blir lättare att följa vad det är som händer. Försök att få in en rutin att kommentera kod som du skriver redan från början!\n\n\n\nObjekt och dess typer\nI tidigare exempel kommer svaret på beräkningarna endast skrivas ut i konsollen, men om vi hade varit intresserade av att spara ner informationen och använda den senare behöver vi specifikt säga till R att göra det.\nVi kan spara resultat av en operation till ett objekt med hjälp av &lt;-. När vi vill ge ett objekt ett värde är det praxis att använda &lt;- och vi kan skriva detta som en kombination av symbolerna, &lt; och -. Vi kan läsa det som ’‘ett värde ges till objektet i pilens riktning’’.\nObjekt kan heta nästintill vadsomhelst men vi måste förhålla oss till ett par begränsningar:\n\nett objektnamn kan inte börja med en siffa, ex. 9tal &lt;- 9 är inte tillåtet.\nett objektnamn kan inte innehålla mellanslag, ex. ett tal &lt;- 2 är inte tillåtet. 1\n\nIstället kan vi använda siffror och andra symboler inuti namnet, ex. tal9. Istället för mellanslag för att separera ord kan vi använda ett system kallad camelCase. Detta sätt att namnge objekt medför att vi tar bort alla mellanslag, slår ihop orden, och använder stor bokstav för att urskilja enskilda delar, ex. ettTal.\nVi kan spara några matematiska operationer för att använda de senare.\n\n\nVisa kod\n## Summan av 2 + 3 tilldelas till ett objekt, döpt till a\na &lt;- 2 + 3\n\n## Vi kan använda en högerriktad pil, men detta är inte praxis\n2 + 3 -&gt; a\n\n\nVi kan nu använda a i senare delar av vår kod och R kommer veta att objektet innehåller summan av 2 + 3.\n\n\nVisa kod\n## a (5) + 3\na + 3\n\n\n[1] 8\n\n\nVisa kod\n## a (5) / 5\na / 5\n\n\n[1] 1\n\n\n\nTyper\nR kan utföra olika operationer beroende på vilken typ av objekt som vi har sparat. En fördel jämfört med andra programmeringsspråk är att vi, i majoriteten av fallen, inte behöver säga till R vilken typ ett objekt är, R kan läsa av kontexten relativt bra. Till exempel kommer objektet a vara numeriskt (mer specifikt typen numeric) och vi kan kontrollera detta genom funktionen class().\n\n\nVisa kod\nclass(a)\n\n\n[1] \"numeric\"\n\n\nUtöver numeriska typer finns även:\n\ntextsträng (character): ex. \"hej\", \"Anna\",\nlogisk (logical): ex. TRUE eller FALSE,\n\nsamt två specialfall av siffror:\n\nheltal (integer), ett specialfall av numeriskt när det bara förekommer heltal: 2L där L anger till R att objektet ska vara av typen integer,\nkomplex (complex) innehållande komplexa tal\n\nVanligtvis räcker det med att använda numeric, character och logical-typer. I praktiken använder sig R utav ytterligare ett specialfall för textsträngar, Factor. Ett Factor-objekt är en blandning av en siffra och text där R lagrar en siffra men har ett lexikon där varje siffra egentligen representerar en textsträng.\n\n\nVisa kod\n## Siffror anges bara som de är\nvalfrittNummer &lt;- 3\n\n## För att man ska ange text måste de omges av \" \" (citattecken) för att R ska läsa de som text\nvalfriText &lt;- \"Hello world!\"\n\n## Om något är sant \ntest &lt;- TRUE\n\n\nMed hjälp av dessa objekt kan vi använda dess värden senare.\n\n\nVisa kod\n# a är summan av 2 + 3, och valfrittNummer har värdet 3\na + valfrittNummer\n\n\n[1] 8\n\n\n\n\n\n\n\n\nViktigt\n\n\n\nLägg märke till att om vi genomför någon matematisk beräkning på ett sparat objekt, till exempel a + 3 så kommer objektet a fortfarande ha samma värde som när vi skapade det. Vi ser enbart resultatet av beräkningen i konsollen och måste tilldela värdet på nytt om vi vill spara det. R uppdaterar inte värdet på ett objekt såvida vi inte ‘’skriver över’’ det.\n\n\n\n\n\nFelsökning\nVad händer om vi försöker genomföra en matematisk operation på två objekt som inte båda är numeriska?\n\n\nVisa kod\n## Ett numeriskt objekt - text\nvalfrittNummer - valfriText\n\n\nError in valfrittNummer - valfriText: non-numeric argument to binary operator\n\n\nVi får nu inte ut något värde i konsollen utan får istället ett felmeddelande. Felmeddelanden och felsökning är en stor del av programmering och kommer vara mycket frustrerande till en början. R ger oss dock (ofta) en indikation på vad som har gått fel och efter ett tag kommer du börja känna igen vanligt förekommande fel och hur man ska lösa dem.\nI detta fall säger R att vi har en ’‘non-numeric argument to binary operator’’ vilket kanske inte är jättetydligt vad det egentligen betyder, men den första delen av meddelandet är viktigt. Vi har ett icke-numeriskt objekt i en operation som kräver numeriska värden. Detta vet vi ju eftersom valfriText är en textsträng och vi kan då lätt lösa felet genom att ange ett annat numeriskt objekt istället.\n\n\nDatastrukturer\nI de tidigare exemplen har ett objekt endast haft ett värde, men vi kan tillåta att ett objekt har en samling med värden istället. Detta kallar vi för datastrukturer då vi på olika sätt strukturerar data. Vanligen kräver en datastruktur att värdena är av samma typ, men vissa strukturer tillåter en blandning av typer.\n\nVektorer\nLåt oss säga att vi har en massa kläder som ligger kaotiskt utspritt i ett rum. För att strukturera dessa kan vi exempelvis ta alla sockar och lägga dem i en låda. Lådan innehåller därmed en grupp av liknande objekt, som i R motsvarar en vektor.\nEn vektor måste innehålla värden av samma typ, siffror, text, eller logiska värden. För att skapa en vektor använder vi funktionen c(), där varje delobjekt kallas för element. Vi separarar elementen i funktionen med ett kommatecken.\n\n\nVisa kod\n## En vektor med numeriska värden\nA &lt;- c(3, 5, 3, 7)\n\nB &lt;- c(1, 3, 2, 4)\n\n## En vektor med textstränger\nord &lt;- c(\"Apelsin\", \"Banan\", \"Citron\")\n\n## Skriver ut respektive vektor\nA\n\n\n[1] 3 5 3 7\n\n\nVisa kod\nB\n\n\n[1] 1 3 2 4\n\n\nVisa kod\nord\n\n\n[1] \"Apelsin\" \"Banan\"   \"Citron\" \n\n\nVektor A och B har fyra element och vektor ord har tre element, alla av samma typ. Om vi vill titta på ett element kan vi indexera vektorn. Indexering innebär att vi säger till R att plocka ut angivna element ur objektets dimensionser. En vektor har bara en dimension vilket innebär att indexfunktionen [] endast behöver ett värde.\n\n\n\n\n\n\nViktigt\n\n\n\nR börjar räkna från 1 när det kommer till index. Detta skiljer sig från andra programmeringsspråk som oftast börjar på 0.\n\n\n\n\nVisa kod\n# Plockar ut det fjärde elementet ur vektorn A\nA[4]\n\n\n[1] 7\n\n\nVisa kod\n# Plockar ut det andra elementet ur vektorn ord\nord[2]\n\n\n[1] \"Banan\"\n\n\nVi kan också plocka ut flera element genom att i indexeringen ange en vektor med numeriska index.\n\n\nVisa kod\n## Plockar ut det andra och fjärde elementet ur vektorn B\nB[c(2, 4)]\n\n\n[1] 3 4\n\n\n\n\nMatriser\nLåt oss anta att vi har två vektorer med lika många element. Vi kan slå ihop dessa till en matris genom funktionen matrix(). En matris måste, likt vektorer, innehålla värden av samma typ, vi kan alltså inte kombinera en numerisk vektor och en textvektor oavsett om de har lika många element.\n\n\nVisa kod\n## En matris med värdena från vektor A och B där vi anger att vi har 2 kolumner\nmatris &lt;- matrix(c(A,B), ncol = 2)\n\n## Skriver ut matrisen\nmatris\n\n\n     [,1] [,2]\n[1,]    3    1\n[2,]    5    3\n[3,]    3    2\n[4,]    7    4\n\n\nVi ser nu i marginalerna (raderna och kolumnerna) att R använder två dimensioner för att peka på ett element. Indexeringen behöver nu två värden [radindex, kolumnindex] för att plocka ut enskilda element. Ordningen av dessa värden spelar roll eftersom R först letar efter en specifik rad (före ,) och sedan en specifik kolumn (efter ,). Om inget värde anges som rad- eller kolumnindex förutsätter R att vi vill se alla index från den dimensionen.\n\n\nVisa kod\n## Plockar ut den andra kolumnen ur matris, vektor B\nmatris[,2]\n\n\n[1] 1 3 2 4\n\n\nVisa kod\n## Plockar ut elementet från den andra raden och första kolumnen, värdet 5\nmatris[2,1]\n\n\n[1] 5\n\n\nVisa kod\n## Plockar ut elementet från den första raden och andra kolumnen, värdet 1\nmatris[1,2]\n\n\n[1] 1\n\n\n\n\nListor och data frames\nDet är ibland begränsande att kräva samma typ av objekt i en vektor eller matris, men som tur är finns det en datastruktur som faktiskt tillåter oss att kombinera objekt av olika typ. Denna struktur kallas för en lista och vi kan se en lista lite som en byrålåda. I en byrålåda kan vi samla ihop lådor av olika storlek och form och detsamma kan vi göra i en lista. Vi kan kombinera vektorer av olika typer med matriser eller enskilda värden i en och samma lista, vi kan till och med lägga in en lista inuti en annan lista.\nFöljande exempel är en lista med fyra olika objekt.\n\n\nVisa kod\nlista &lt;- list(A, B, ord, matris)\n\nlista\n\n\n[[1]]\n[1] 3 5 3 7\n\n[[2]]\n[1] 1 3 2 4\n\n[[3]]\n[1] \"Apelsin\" \"Banan\"   \"Citron\" \n\n[[4]]\n     [,1] [,2]\n[1,]    3    1\n[2,]    5    3\n[3,]    3    2\n[4,]    7    4\n\n\nFör att plocka ut ett element från listan behöver vi använda ytterligare en form av indexering. Vi behöver först ta oss in i en utav lådorna för att indexera den som vanligt, och det gör vi genom [[element]].\n\n\nVisa kod\n## Plockar ut det fjärde elementet ur vektorn A, listans första element\nlista[[1]][4]\n\n\n[1] 7\n\n\nVisa kod\n## Plockar ut elementet från den andra raden och första kolumnen i listans fjärde element\nlista[[4]][2,1]\n\n\n[1] 5\n\n\nFlexibiliteten i en lista gör det däremot väldigt svårt att arbeta med den, mer specifikt att en lista kan innehålla objekt av olika storlekar. En mer strukturerad datastruktur som tillåter olika typer av objekt är en data frame. Likt en matris innehåller en data frame vektorer av samma längd och likt en lista tillåter en data frame att vektorerna är av olika typ, vi kombinerar alltså strukturen från en matris med flexibiliteten från en lista. Ytterligare en fördel med en data frame är att kolumnerna (vektorerna) kan ha namn för att lättare urskilja vad objektet innehåller.\n\n\n\nVisa kod\n## Skapar en data frame med hjälp av funktionen data.frame() och döper de två kolumnerna till Var1 och Var2\ndata.frame(Var1 = A, Var2 = B)\n\n\n  Var1 Var2\n1    3    1\n2    5    3\n3    3    2\n4    7    4\n\n\nNär vi samlar in data inom statistik brukar vi ställa upp informationen med samma struktur som en data frame, där varje rad är en observerad enhet och varje kolumn är en variabel som vi gör mätningar på. Varje enskilda cell blir då enhetens uppmätta värde på den angivna variabeln, till exempel halten kväveoxid i Vänern. Detta kallar vi ofta för rådata-format.\n\n\nVisa kod\n## Skapar data\nstudent &lt;- \n  data.frame(\n    namn = c(\"Anna\", \"Oscar\", \"Jakob\", \"Noor\"),\n    längd = c(158, 164, 180, 174),\n    utbildning = c(\"Biologi\", \"Biologi\", \"Matematik\", \"Statistik\")\n  )\n\n## Skriver ut data\nstudent\n\n\n   namn längd utbildning\n1  Anna   158    Biologi\n2 Oscar   164    Biologi\n3 Jakob   180  Matematik\n4  Noor   174  Statistik\n\n\nVi kan i objektet student till exempel läsa att den första studenten Anna är 158 cm lång och studerar Biologi. Om vi är intresserade av en specifik variabel ur denna data frame kan vi återigen indexera med hjälp av [] men det vanligaste är att istället använda $.\n\n\nVisa kod\n## Indexerar variabeln \"utbildning\" ur data frame \"student\"\nstudent$utbildning\n\n\n[1] \"Biologi\"   \"Biologi\"   \"Matematik\" \"Statistik\"\n\n\nVisa kod\n## Gör samma indexering med hjälp av [], där \"utbildning\" är den tredje kolumnen\nstudent[,3]\n\n\n[1] \"Biologi\"   \"Biologi\"   \"Matematik\" \"Statistik\"\n\n\nVisa kod\n## Gör återigen samma men använder namnet på variabeln\nstudent[, \"utbildning\"]\n\n\n[1] \"Biologi\"   \"Biologi\"   \"Matematik\" \"Statistik\"\n\n\nFör att plocka ut enskilda rader (observationer) eller celler kan vi indexera med [radindex, kolumnindex] likt en matris.",
    "crumbs": [
      "Regressionsanalys",
      "**DEL I - Programmering i R**",
      "Grunderna i R"
    ]
  },
  {
    "objectID": "00-programming/00-basics-R.html#sammanfattning",
    "href": "00-programming/00-basics-R.html#sammanfattning",
    "title": "Grunderna i R",
    "section": "Sammanfattning",
    "text": "Sammanfattning\nNu har vi fått en genomgång av grunderna i R, hur man kodar enkla matematiska operationer, vilka typer av objekt som R kan arbeta med, och hur vi sparar information till arbetsminnet på olika sätt som kan användas senare i samma session. Vi kommer i nästkommande kapitel gå igenom olika hjälpmedel som R och RStudio erbjuder så att vi inte alltid behöver koda allting från grunden.",
    "crumbs": [
      "Regressionsanalys",
      "**DEL I - Programmering i R**",
      "Grunderna i R"
    ]
  },
  {
    "objectID": "00-programming/00-basics-R.html#övningsuppgifter",
    "href": "00-programming/00-basics-R.html#övningsuppgifter",
    "title": "Grunderna i R",
    "section": "Övningsuppgifter",
    "text": "Övningsuppgifter\nMed hjälp av R på din egna dator försök att lösa följande uppgifter. Efter varje uppgift får du en utskrift som visar hur resultatet ska se ut från din kod.\n\nSpara en numerisk vektor med alla värden från 1 till och med 6 i ökande ordning.\n\n\n\n[1] 1 2 3 4 5 6\n\n\n\nSpara en textvektor med sex element där den första, tredje och fjärde elementet är Hund och resterande är Katt.\n\n\n\n[1] \"Hund\" \"Katt\" \"Hund\" \"Hund\" \"Katt\" \"Katt\"\n\n\n\nSkapa en data frame med två kolumner. Den första kolumnen ska heta “ID” och innehålla vektorn från a) och den andra kolumnen ska heta “Art” och innehålla vektorn från b).\n\n\n\n  ID  Art\n1  1 Hund\n2  2 Katt\n3  3 Hund\n4  4 Hund\n5  5 Katt\n6  6 Katt\n\n\n\nPlocka ut den fjärde observationen.\n\n\n\n  ID  Art\n4  4 Hund",
    "crumbs": [
      "Regressionsanalys",
      "**DEL I - Programmering i R**",
      "Grunderna i R"
    ]
  },
  {
    "objectID": "00-programming/00-basics-R.html#footnotes",
    "href": "00-programming/00-basics-R.html#footnotes",
    "title": "Grunderna i R",
    "section": "",
    "text": "Det finns ett sätt att tillåta detta som vi kommer till i kapitel ?sec-tidyverse.↩︎",
    "crumbs": [
      "Regressionsanalys",
      "**DEL I - Programmering i R**",
      "Grunderna i R"
    ]
  },
  {
    "objectID": "00-programming/01-packages-and-functions.html",
    "href": "00-programming/01-packages-and-functions.html",
    "title": "Paket och funktioner",
    "section": "",
    "text": "Paket och funktioner\nInom programmering är det vanligt att man genomför samma komplexa operation flera gånger och ett enkelt sätt att underlätta kodningen är att skapa en funktion som förkortar flera rader kod till en. Flera liknande funktioner samlas i olika paket som antingen redan är installerade på datorn eller måste installeras en gång.\nInstallerade paket måste också laddas till arbetsminnet för att R ska kunna använda sig utav dem. Vi kan visualisera detta som att även fast vi har köpt in en verktygslåda, måste vi ställa upp den på arbetsbänken för att kunna använda verktygen däri. Om du inte har verktygslådan bredvid dig kommer det bli svårt att använda något av verktygen. Vi kan ladda upp ett paket till arbetsminnet med hjälp av library() eller require(), där namnet på paketet anges inuti parenteserna.\nVisa kod\n## Laddar paketet dplyr med dess tillhörande funktioner\nrequire(dplyr)\nOm du har en ny installation av R på datorn måste du ibland installera paket, det vill säga ladda ner paketets information från internet. Som tur är finns det en funktion för detta också, nämligen install.packages(). Skillnaden här är att namnet på paketet måste anges inom citationstecken, ex. \"PAKET\", istället för bara namnet som vi gjorde i require(). Om du en gång redan installerat paketet till din version av R behöver du inte installera paketetet för varje ny session utan kan direkt ladda det genom library() eller require().\nVisa kod\n## Om paketen inte finns på datorn måste de installeras. \n## KÖRS ENDAST EN GÅNG I SAMBAND MED EN NY INSTALLATION ELLER UPPDATERING AV R!\ninstall.packages(\"ggplot2\")\ninstall.packages(\"RColorBrewer\")\n\n## Laddar paketen innehållande de funktioner som vi vill använda\nrequire(ggplot2)\nrequire(RColorBrewer)\nTermen funktion har använts frekvent tidigare och kan behöva förtydligas. En funktion är en större operation, oftast flera rader kod, som sammanfattas med en rad. Vi kan använda redan inbyggda funktioner från olika paket (som vi gjort i tidigare kapitel) eller skapa en egen funktion, men vi kommer i detta underlag fokusera på att använda redan skapade funktioner. För att använda en funktion behöver vi veta namnet på funktionen och eventuella argument som funktionen behöver. Argument motsvarar inställningar eller information som behövs i operationerna som funktionen genomför. För att tilldela ett argument ett värde eller information används = till skillnad från &lt;- i “vanlig” kodning.\nVisa kod\nfunktion(argument1 = värde1, argument2 = värde2)\nAlla argument i en funktion omsluts av parenteser, ( och ), och detta betyder att man kan skriva funktioner på en eller flera rader. Fördelen med att skriva en funktion på flera rader är att göra det lättare att läsa av vad som egentligen händer. Notera att R dock måste ha någon form utav indikation att funktionen fortsätter, t.ex. ett , för att ange att flera argument tillkommer eller &lt;- som tilldelar ett värde till ett objekt.1\nVisa kod\n## Skapar en numerisk vektor \nvektor &lt;- \n  c(\n    2,\n    3,\n    4,\n    5,\n    6\n  )\nR fortsätter att läsa på nästa rad om programmet inte anser att raden avslutas, i detta fall med att funktionen avslutas med ). Mer specifikt kan vi läsa av raderna en och en likt:\nEn annan fördel med att skriva funktioner på flera rader är att vi kan kommentera varje enskilda argument som används. Detta kommer vi se exempel på senare.",
    "crumbs": [
      "Regressionsanalys",
      "**DEL I - Programmering i R**",
      "Paket och funktioner"
    ]
  },
  {
    "objectID": "00-programming/01-packages-and-functions.html#paket-och-funktioner",
    "href": "00-programming/01-packages-and-functions.html#paket-och-funktioner",
    "title": "Paket och funktioner",
    "section": "",
    "text": "&lt;- behöver ett värde att tilldela till objektet vektor, men eftersom det inte anges något värde på samma rad fortsätter R läsa av nästa rad,\nFunktionen c får inget avslutande ) så R fortsätter läsa av nästa rad,\nVarje , separerar element i vektorn, men eftersom det inte anges något ytterligare värde fortsätter R läsa av nästa rad,\nDet är först när inget ytterligare element läggs till och funktionen stängs med ) som R är nöjd och inte läser vidare på efterföljande rader.",
    "crumbs": [
      "Regressionsanalys",
      "**DEL I - Programmering i R**",
      "Paket och funktioner"
    ]
  },
  {
    "objectID": "00-programming/01-packages-and-functions.html#sec-readData",
    "href": "00-programming/01-packages-and-functions.html#sec-readData",
    "title": "Paket och funktioner",
    "section": "Ladda in datamaterial",
    "text": "Ladda in datamaterial\nVid datainsamling, eller vid inhämtning av material som någon annan har samlat in, är det vanligt att vi sammanställer informationen utanför R, till exempel skulle ett arbetsblad i Microsoft Excel ge oss ett lätt sätt att strukturera information. Istället för att behöva återskapa materialet från grunden i R kan vi läsa in information från andra typer av filer på datorn.\nDen enklaste formen av fil som R läser in är rena textfiler (med filändelser .txt, .csv, .dat) som på olika sätt sparat ner datans struktur på ett organiserat sätt. Vi kan läsa in informationen från dessa sorters filer till R med hjälp av specifika funktioner.\nInom statistik är det vanligt att använda kommaseparerade filer, .csv, som vi kan skapa från Excel genom menyn Spara som.... Med hjälp av funktionen read.csv och dess argument kan vi säga till R vilken struktur som filen har och få samma struktur i en data frame. Ett tips är att öppna datafilen i Notepad, Anteckningar eller annan enkel ordbehandlare för att se vilka symboler som används för att beskriva strukturen.\nExempelvis skulle en textfil kunna se ut så här:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigur 1: Exempel på en fil öppnad i Notepad\n\n\n\nsom laddas in i R som följer:\n\n\nVisa kod\n## Laddar in datamaterialet från fil och sparar data frame i objektet ekar\nekar &lt;- \n  read.csv(\n    ## Sökvägen till filen på datorn\n    \"dataOmEkar.csv\", \n    ## Argument för vilken symbol som används för decimaler i filen\n    dec = \",\", \n    ## Argument för vilken symbol som används för att separera kolumner i filen\n    sep = \";\"\n  )",
    "crumbs": [
      "Regressionsanalys",
      "**DEL I - Programmering i R**",
      "Paket och funktioner"
    ]
  },
  {
    "objectID": "00-programming/01-packages-and-functions.html#arbetsmappar",
    "href": "00-programming/01-packages-and-functions.html#arbetsmappar",
    "title": "Paket och funktioner",
    "section": "Arbetsmappar",
    "text": "Arbetsmappar\nAtt importera datamaterial är endast ett sätt där vi interagerar med datorns hårddisk. Vi kan ibland vilja spara ner objekt eller visualiseringar mer permanent och då kommer vi återigen behöva ange en sökväg till någonstans på datorn. För att underlätta denna process kan vi använda oss utav arbetsmappar (eng. working directory).\nEn arbetsmapp är ett sätt för oss att snabbt och enkelt ge information till R att den ska leta i en specifik del av datorns hårddisk. Om vi arbetar med ett projekt, till exempel datorlaborationer i en specifik kurs, kan det vara lämpligt att skapa en mapp på datorn där all information som hör till kursen sparas ner. Då är det också lämpligt att i R säga att denna mapp är den arbetsmapp som vi ska importera från och exportera till under en session. Vi behöver dock först veta vilken sökväg som leder oss in till mappen.\nFör att hitta denna sökväg, kan du öppna upp mappen i ditt operativssystems filhanterare. Du kan följa lämplig instruktion för just ditt operativsystem via någon av följande länkar:\n\ninstruktioner för Windows\ninstruktioner för Mac\ninstruktioner för Linux\n\nNär du väl har hittat din sökväg ska du kopiera den i sin helhet och sedan använda funktionen setwd() för att R ska spara mappen i arbetsminnet. Notera att alla \\ måste bytas ut med antingen \\\\ eller / för att R ska kunna läsa av sökvägen korrekt. Exempelvis kan det se ut så här:\n\n\nVisa kod\n## Arbetsmappen anger en sökväg till en mapp på datorn med alla filer som ska användas \nsetwd(\"C:/Users/MIN ANVÄNDARE/MAPPENS NAMN\")\n\n\nVi måste lägga in sökvägen inom \"sökväg\" för att R ska kunna läsa av den som en textsträng och inte en massa objekt. Låt oss säga att vi har en fil i denna mapp som heter dataOmEkar.csv. Om vi inte har en arbetsmapp i sessionen behöver vi ange hela sökvägen till filen om vi ska importera den till R via read.csv(\"C:/Users/MIN ANVÄNDARE/MAPPENS NAMN/dataOmEkar.csv)\". Däremot efter att ha kört raden kod vet R att den ska leta efter filen i arbetsmappen och vi behöver därför bara använda filens namn likt read.csv(\"dataOmEkar.csv\"). I det långa loppet kommer arbetsmappar underlätta kodningen avsevärt om vi organiserar filer på datorn på ett strukturerat sätt. Om vi endast är intresserade av någon enstaka fil kanske vi kan klara oss att skriva den långa sökvägen en eller två gånger, men det blir lätt mycket text om fler filer behöver importeras.",
    "crumbs": [
      "Regressionsanalys",
      "**DEL I - Programmering i R**",
      "Paket och funktioner"
    ]
  },
  {
    "objectID": "00-programming/01-packages-and-functions.html#help",
    "href": "00-programming/01-packages-and-functions.html#help",
    "title": "Paket och funktioner",
    "section": "Hjälpdokumentation",
    "text": "Hjälpdokumentation\nAtt alltid komma ihåg vad R kan göra, vilka funktioner som finns, vilka argument som de behöver, är omöjligt. Som tur är finns många resurser att hjälpa oss, inte minst den interna dokumentationen.\nGenom att använda ? och ett funktionsnamn letar R upp den tillhörande hjälpdokumentationen för funktionen. Den innehåller en detaljerad beskrivning av funktionens syfte, hur man använder den, vilka argument som finns och vad respektive styr, mer djupgående detaljer om funktionen och dess resultat, ytterligare referenser, och allra sist exempel när funktionen används som man kan ta inspiration från.\nLäs hjälpdokumentationen för funktionen att skapa en vektor och testa några av de exempel som visas allra sist i artikeln.\n\n\nVisa kod\n?c\n\n\nEftersom R är open-source finns det väldigt mycket material tillgänglig på internet, detta underlag kan ses som en sådan resurs. Genom att använda en sökmotor där man beskriver det problem som man stött på eller fråga hur en funktion fungerar kommer du hitta en stor samling av sidor som säkerligen beskriver exakt det du är ute efter.",
    "crumbs": [
      "Regressionsanalys",
      "**DEL I - Programmering i R**",
      "Paket och funktioner"
    ]
  },
  {
    "objectID": "00-programming/01-packages-and-functions.html#chat-GPT",
    "href": "00-programming/01-packages-and-functions.html#chat-GPT",
    "title": "Paket och funktioner",
    "section": "ChatGPT eller annan AI",
    "text": "ChatGPT eller annan AI\nYtterligare en resurs som kan både hjälpa och stjälpa är ChatGPT och andra liknande AI-program. Genom att ställa frågor av olika slag till programmet kan man få resultat som dels ger kommenterad kod som (förhoppningsvis) löser det problem som man frågar efter och beskriver mer i detalj koden som den producerat.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigur 2: Exempel på en fråga och svar med ChatGPT\n\n\n\nI exemplet ovan producerar ChatGPT några rader kod som ser ut att fungera, vi måste ange sökvägen till filen i file_path och sedan används funktionen read.csv() för att läsa in datamaterialet till objektet data. ChatGPT är dock inte bra på att kontrollera sin kod och inte heller att följa viss praxis vid kodning, vilket innebär att vi aldrig får ta koden som produceras som hel sanning. Istället kan vi använda AI-program som ett sätt att inspireras eller på annat sätt läsa oss olika programmeringskoncept som vi sedan tillämpar på egen hand.",
    "crumbs": [
      "Regressionsanalys",
      "**DEL I - Programmering i R**",
      "Paket och funktioner"
    ]
  },
  {
    "objectID": "00-programming/01-packages-and-functions.html#footnotes",
    "href": "00-programming/01-packages-and-functions.html#footnotes",
    "title": "Paket och funktioner",
    "section": "",
    "text": "Vi kommer senare se andra exempel på symboler som visar R att kodningen fortsätter på nästa rad.↩︎",
    "crumbs": [
      "Regressionsanalys",
      "**DEL I - Programmering i R**",
      "Paket och funktioner"
    ]
  },
  {
    "objectID": "00-programming/02-tidyverse.html",
    "href": "00-programming/02-tidyverse.html",
    "title": "tidyverse",
    "section": "",
    "text": "Läsa in filer med readr\nR innehåller många inbyggda funktioner (se Ladda in datamaterial) för hur man kan läsa in filer av olika format från hårddisken. Vi har tidigare tittat på read.csv2() som är en grundfunktion i R, men readr har liknande funktioner utöver flera andra specialfall som utgår från vissa standardformat för filer.\nDet är starkt rekommenderat att läsa in text-filer i R, inte Excelfiler då Excel kan innehålla specialformat som R inte kan läsa in korrekt. Vi kan från Excel spara om en .xlsx fil till en .txt-, .tsv- eller .csv-fil.\nVisa kod\n## Läser in en textfil där kolumner separeras med en (eller flera) mellanslag (.txt)\nread_table(file = \"sökväg\")\n\n## Läser in en textfil där kolumner separeras med tab (.tsv)\nread_tsv(file = \"sökväg\")\n\n## Läser in en textfil där kolumner separeras med , (read_csv) eller ; (read_csv2)\nread_csv(file = \"sökväg\")\nread_csv2(file = \"sökväg\")\nResultatet av dessa funktioner blir en formatterad tibble.\nSe till att datamaterialet som laddats in ser ut som vi förväntar att det ska göra, exempelvis är decimaler korrekt angivna, har vi lika många variabler i R som i originalfilen och liknande.",
    "crumbs": [
      "Regressionsanalys",
      "**DEL I - Programmering i R**",
      "tidyverse"
    ]
  },
  {
    "objectID": "00-programming/02-tidyverse.html#sec-tibble",
    "href": "00-programming/02-tidyverse.html#sec-tibble",
    "title": "tidyverse",
    "section": "tibble",
    "text": "tibble\nI Listor och data frames presenterades datastrukturen data frame som ett sätt för oss att i R spara information i ett standard format. En tibble är en utveckling av denna datastruktur som rensat bort egenskaper som inte längre används och lägger till egenskaper som ofta vill användas vid behandling av data.\nLikt en data frame, innehåller en tibble en samling lika långa kolumner och dessa kolumner kan vara av olika objekttyper. Till exempel kan vi blanda listor och vektorer så länge de innehåller lika många element.\n\n\n\n\n\n\nViktigt\n\n\n\nEn tibble konverterar inte textvektorer till faktorer per automatik. Det vill säga om ett datamaterial som läses in via read_csv2() innehåller text, kommer denna variabel vara ett character-objekt, inte ett Factor-objekt.\n\n\nVi kan skapa en tibble från grunden genom att ange vilka objekt vi vill inkludera och vad de ska heta:\n\n\nVisa kod\na &lt;- c(1, 2, 3)\nb &lt;- c(4, 5, 6)\nc &lt;- c(\"a\", \"b\", \"c\")\n\n## Skapar en tibble utifrån angivna vektorer\ntibble(\n  Var1 = a,\n  Var2 = b,\n  Var3 = c\n)\n\n\n# A tibble: 3 × 3\n   Var1  Var2 Var3 \n  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;\n1     1     4 a    \n2     2     5 b    \n3     3     6 c    \n\n\nOm vi har en matris eller något annat objekt som ser ut som en tibble men inte är det, kan vi konvertera den till en tibble med hjälp av as_tibble().\n\n\nVisa kod\n## Tar objektet a (en vektor) och gör om den till en tibble med en kolumn\na %&gt;% \n  as_tibble()\n\n\n# A tibble: 3 × 1\n  value\n  &lt;dbl&gt;\n1     1\n2     2\n3     3",
    "crumbs": [
      "Regressionsanalys",
      "**DEL I - Programmering i R**",
      "tidyverse"
    ]
  },
  {
    "objectID": "00-programming/02-tidyverse.html#sec-tidyr",
    "href": "00-programming/02-tidyverse.html#sec-tidyr",
    "title": "tidyverse",
    "section": "tidyr",
    "text": "tidyr\nOm data inte är på det format som vi önskar eller om någon statistisk metod behöver ett material som har ett annat format än tidy-data. Detta paket kommer vi inte titta på just nu i detalj men följande Cheat Sheets innehåller exempel på olika bearbetningar som vi kan vara intresserade av att genomföra med data.",
    "crumbs": [
      "Regressionsanalys",
      "**DEL I - Programmering i R**",
      "tidyverse"
    ]
  },
  {
    "objectID": "00-programming/02-tidyverse.html#programmeringsrör-pipes",
    "href": "00-programming/02-tidyverse.html#programmeringsrör-pipes",
    "title": "tidyverse",
    "section": "Programmerings”rör” (pipes)",
    "text": "Programmerings”rör” (pipes)\nDet är vanligt att vi vill genomföra flera operationer på ett och samma objekt, speciellt när vi har att göra med databearbetning. Att nästla funktioner i varandra blir lätt väldigt krångligt att båda skriva och läsa av. Om argument ska läggas till i de olika funktionerna är det svårt att läsa av exakt vilken funktion de tillhör, och det blir i princip bara programmering med parenteser. Alternativt kan objektet sparas om vid varje steg men det skapas många onödiga objekt som man måste hålla koll på när det egentligen bara är det sista steget som är relevant.\n\n\nVisa kod\n## Fyra nästlade funktioner som ska appliceras på ett objekt med olika argument\nfunktion4(funktion3(funktion2(funktion1(objekt), argument2 = X)), argument4 = Y)\n\n## Fyra funktioner som sparas vid varje steg\nA &lt;- funktion1(objekt)\n\nB &lt;- funktion2(A, argument2 = X)\n\nC &lt;- funktion3(B)\n\nD &lt;- funktion4(C, argument4 = Y)\n\n\nIstället kan vi använda en pipe operator för att skapa sekvenser av funktioner. Från paketet dplyr används operatorn %&gt;% för detta. Denna operator läses som att resultatet innan %&gt;% skickas vidare till det första argumentet i funktionen efter %&gt;%.\n\n\nVisa kod\nobjekt %&gt;% \n  funktion1() %&gt;% \n  funktion2(argument2 = X) %&gt;% \n  funktion3() %&gt;% \n  funktion4(argument4 = Y)\n\n\nFunktioner som inte behöver fler än ett argument skrivs bara som funktionen med tomma parenteser. Funktioner som har ytterligare argument anger bara dessa nya.\nDet kan finnas funktioner vars argument vi vill lägga in resultatet från en tidigare funktion inte är det första argumentet. Som tur är kan vi lösa detta genom att använda . som symbolen för det tidigare resultatet och ange det vid argumentet som är aktuellt.\n\n\nVisa kod\n## Objektet (.) läggs här in som värde för det andra argumentet i funktionen\nobjekt %&gt;% \n  funktion5(argument1 = X, argument2 = .)\n\n\nI resterande kodexempel kommer denna pipe operator användas frekvent för att underlätta läsningen av koden.",
    "crumbs": [
      "Regressionsanalys",
      "**DEL I - Programmering i R**",
      "tidyverse"
    ]
  },
  {
    "objectID": "00-programming/02-tidyverse.html#databearbetning-med-dplyr",
    "href": "00-programming/02-tidyverse.html#databearbetning-med-dplyr",
    "title": "tidyverse",
    "section": "Databearbetning med dplyr",
    "text": "Databearbetning med dplyr\nDatamaterialet behöver rensas från saknade värden för dessa kommer ställa till problem. I denna kod används filter() från paketet dplyr där datamaterialet penguins rensas från observationer där sex är NA, Rs sätt att ange saknade värden (not available). Mer om databearbetning kommer visas i kapitel ?sec-tidyverse.\n\n\nVisa kod\ninstall.packages(\"dplyr\")\nrequire(dplyr)\n\n## Filtrerar bort observationer som saknar information\npenguins &lt;- filter(penguins, !is.na(sex))\n\n\nselect filter\nrename relocate arrange\nmutate transmute\ngroup_by summarize",
    "crumbs": [
      "Regressionsanalys",
      "**DEL I - Programmering i R**",
      "tidyverse"
    ]
  },
  {
    "objectID": "00-programming/02-tidyverse.html#hantering-av-text-med-stringr-och-forcats",
    "href": "00-programming/02-tidyverse.html#hantering-av-text-med-stringr-och-forcats",
    "title": "tidyverse",
    "section": "Hantering av text med stringr och forcats",
    "text": "Hantering av text med stringr och forcats",
    "crumbs": [
      "Regressionsanalys",
      "**DEL I - Programmering i R**",
      "tidyverse"
    ]
  },
  {
    "objectID": "01-regression/index.html#footnotes",
    "href": "01-regression/index.html#footnotes",
    "title": "Förord",
    "section": "",
    "text": "Detta material använder sig av %&gt;% men |&gt; bör kunna användas istället i de allra flesta exemplen.↩︎",
    "crumbs": [
      "Regressionsanalys",
      "**DEL II - Regressionsanalys**",
      "Förord"
    ]
  },
  {
    "objectID": "01-regression/00-intro-regression.html",
    "href": "01-regression/00-intro-regression.html",
    "title": "1  Introduktion till regression",
    "section": "",
    "text": "1.1 Den linjära regressionsmodellen\nDet finns många olika sorters modeller inom regressionsanalys, men i det allra enklaste fallet anpassas en linjär modell där variablernas samband antas enkelriktat och konstant. \\[\nY = \\beta_0 + \\beta_1 \\cdot X + E\n\\tag{1.1}\\]\ndär:\nOm flera förklarande variabler antas påverka responsvariabeln utökas den linjära modellen med flera \\(\\beta_j\\), en för varje förklarande variabel \\(X_j\\). Senare kapitel kommer titta närmare på utökningar av denna linjära modell.",
    "crumbs": [
      "Regressionsanalys",
      "**DEL II - Regressionsanalys**",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduktion till regression</span>"
    ]
  },
  {
    "objectID": "01-regression/00-intro-regression.html#den-linjära-regressionsmodellen",
    "href": "01-regression/00-intro-regression.html#den-linjära-regressionsmodellen",
    "title": "1  Introduktion till regression",
    "section": "",
    "text": "\\(Y\\) är den beroende/respons- variabeln som antas påverkas av \\(X\\).\n\\(X\\) är den oberoende/förklarande variabeln som antas påverka \\(Y\\).\n\\(\\beta_0\\) är modellens intercept där linjen skär y-axeln när \\(X = 0\\).\n\\(\\beta_1\\) är lutningen som beskriver det enkelriktade samband mellan \\(X\\) och \\(Y\\). Mer specifikt beskriver parametern förändringen i \\(Y\\) när \\(X\\) ökar med en enhet.\n\\(E\\) är modellens felterm, avståndet mellan det observerade värdet på \\(Y\\) och modellens skattade värde \\(\\hat{Y}\\).",
    "crumbs": [
      "Regressionsanalys",
      "**DEL II - Regressionsanalys**",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduktion till regression</span>"
    ]
  },
  {
    "objectID": "01-regression/00-intro-regression.html#sec-model-assumptions",
    "href": "01-regression/00-intro-regression.html#sec-model-assumptions",
    "title": "1  Introduktion till regression",
    "section": "1.2 Modellens antaganden",
    "text": "1.2 Modellens antaganden\nSyftet med en modell är att ge en lämplig förenkling av verkligheten. En linjär regressionsmodell kan vara en lämplig förenkling av ett samband om följande antaganden uppfylls:\n\natt det för varje \\(X\\) finns en slumpvariabel \\(Y\\) med ett ändligt medelvärde och varians,\nobservationerna är oberoende av varandra,\nmedelvärdet, \\(\\mu_{Y|X}\\), kan modelleras linjärt,\nvariansen för \\(Y\\) är lika för alla värden av \\(X\\), \\(\\sigma^2_{Y|X} \\equiv \\sigma^2\\),\nslumpvariabeln \\(Y\\) är normalfördelad för alla värden av \\(X\\).\n\nVi kan sammanfatta majoriteten av dessa antaganden med: \\[\n   Y|X \\overset{\\mathrm{iid}}{\\sim} N(\\mu_{Y|X}, \\sigma^2_{Y|X})\n\\] där \\(\\mathrm{iid}\\) betyder “independent and identically distributed” motsvarande antagande 2.\n\n\n\n\n\n\nViktigt\n\n\n\nDet finns inget antagande om att \\(Y \\sim N(\\mu_Y, \\sigma^2_Y)\\)! Alla antaganden för en linjär regressionsmodell fokuserar på att vi med hjälp av \\(X\\) har en normalfördelad slumpvariabel \\(Y\\).\n\n\nOm det tredje antagandet uppfylls kan vi modellera väntevärdet av \\(Y|X\\) med den linjära modellen: \\[\n  E[Y|X] = \\beta_0 + \\beta_1 \\cdot X\n\\tag{1.2}\\] så att: \\[\nY|X \\overset{\\mathrm{iid}}{\\sim} N(\\beta_0 + \\beta_1 \\cdot X, \\sigma^2_{Y|X})\n\\]\nTill skillnad från Ekvation 1.1 saknar Ekvation 1.2 modellens felterm på grund av att vi nu modellerar endast medelvärdet av slumpvariabelns fördelning, \\(\\mu_{Y|X}\\). Osäkerheten runtomkring medelvärdet är variansen av fördelningen.\nNär vi modellerar varje enskilda observation inkluderas \\(E\\) vilket innebär att vi kan flytta modellens antaganden från \\(Y|X\\) till \\(E\\). \\[\n  E \\overset{\\mathrm{iid}}{\\sim} N(0, \\sigma^2)\n\\tag{1.3}\\]\nDenna omskrivning ger oss en bra utgångspunkt att utvärdera lämpligheten av en anpassad modell.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigur 1.2: Ej lämplig flygplansmodell, (”Create a model of a commercial airplane where some parts are taken from a car or a boat” 2024)\n\n\n\nOm en flygplansmodell ser endast till viss del ut som ett flygplan kommer modellen inte vara lämplig att använda för att förstå eller förenkla verkligheten. Detsamma gäller för regressionsmodeller; om modellen inte uppfyller dess antaganden riskerar slutsatser som dras inte stämma överens med verkligheten.",
    "crumbs": [
      "Regressionsanalys",
      "**DEL II - Regressionsanalys**",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduktion till regression</span>"
    ]
  },
  {
    "objectID": "01-regression/00-intro-regression.html#studier-och-andra-variabler",
    "href": "01-regression/00-intro-regression.html#studier-och-andra-variabler",
    "title": "1  Introduktion till regression",
    "section": "1.3 Studier och andra variabler",
    "text": "1.3 Studier och andra variabler\nEn regressionsmodell behöver nödvändigtvis inte beskriva ett “orsak-och-verkan” samband, eller som vi brukar benämna det, ett kausalt samband. Samband kan ibland uppstå utav ren slump där det inte finns någon logisk koppling mellan variablerna. Denna typ av samband benämns som korrelationssamband. Trots att korrelationssamband rent matematiskt beskriver en relation mellan den ena variabeln och den andra, är det i vissa fall inte lämpligt eller relevant att använda eller tolka modellen i verkligheten.\n\n\n\n\n\n\n\n\nFigur 1.3: Sambandet mellan antalet filmer som Nicolas Cage medverkade i och antalet dödsfall av drunkning mellan 1999 och 2009 i USA (”CDC - NCHS - National Center for Health Statistics — cdc.gov”; ”Nicolas Cage | Actor, Producer, Director — imdb.com”)\n\n\n\n\n\nFigur 1.3 uppvisar ett exempel på korrelationssamband där de två variablerna inte har någon logisk koppling till varandra utan endast har observerats ha en positiv korrelation. Att beskriva detta samband skulle inte ge någon information om verkligheten så en viktig del av regressionsanalys är att bedöma lämpligheten och relevansen av utvalda variabler. Rent matematiskt kan inte heller en regressionsmodell urskilja mellan kausala eller korrelationssamband vilket innebär att vi som analytiker måste ta hänsyn till vilken sorts data och hur data har samlats in för att använda och tolka modellerna på rätt sätt.\nExemplet i figuren är insamlad som en observationsstudie där mätvärden (antal dödsfall och filmer) på enheterna (år) har observerats från olika registerdata. Vi har inte kunnat styra vilken relation dessa variabler har till varandra och studien i sig har inte tagit hänsyn till någon specifik orsak och verkan mellan de två. Vi kan därför endast dra slutsatser om korrelationssamband från en observationsstudie, vi kan säga att desto fler filmer Nicolas Cage medverkar i medför ett större antal dödsfall, vilket egentligen inte är relevant, men vi kan inte säga något om den kausala effekten.\nFör att kunna dra slutsatser om kausala samband behöver vi genomföra en experimentell studie där vi styr vilka mätvärden som enheter får eller har och responsvariablen antas vara en direkt effekt från de förklarande variablerna. Medicinska studier, till exempel studier om Covid-vaccinets effektivitet på att motverka en infektion, är typiska exempel på experimentella studier där en förklarande variabel (dos) ges till vissa grupper av enheter där andra påverkande effekter kontrolleras för att justera den förklarande variabelns verkliga påverkan.\n\n1.3.1 Kontrollvariabler\nI en observationsstudie kan vi ibland observera kontrollvariabler som kan justera den förklarande variabelns faktiska påverkan men det är främst i experimentella studier som dessa typer av variabler kan användas. Okända variabler kallas för confounding-effekter och antas påverka både den förklarande och responsvariabeln.\nI följande två figurer visas den huvudsakliga förklarande variabeln och den valda responsvariabeln med ovaler. De kända (heldragen) och okända (streckade) kontrollvariablerna visas som rektanglar.\n\n\n\n\n\n\n\n\n\n\nG\n\n\n\nOkänd\n\nOkänd\n\n\n\nFilmer\n\nFilmer\n\n\n\nOkänd-&gt;Filmer\n\n\n\n\n\nDödsfall\n\nDödsfall\n\n\n\nOkänd-&gt;Dödsfall\n\n\n\n\n\nFilmer-&gt;Dödsfall\n\n\n\n\n\n\n\n\n\n\n\n\nFigur 1.4: Exempel på relationen mellan variabler i en observationsstudie.\n\n\n\nEftersom sambandet mellan filmer och dödsfall förmodligen endast uppkommit av slumpen kan det finnas andra okända variabler som påverkar de båda.\n\n\n\n\n\n\n\n\n\n\nG\n\n\n\nÅlder\n\nÅlder\n\n\n\nBlodtryck\n\nBlodtryck\n\n\n\nÅlder-&gt;Blodtryck\n\n\n\n\n\nKön\n\nKön\n\n\n\nKön-&gt;Blodtryck\n\n\n\n\n\nMedicin\n\nMedicin\n\n\n\nMedicin-&gt;Blodtryck\n\n\n\n\n\n\n\n\n\n\n\n\nFigur 1.5: Exempel på relationen mellan variabler i en experimentell studie.\n\n\n\nEffekten av en medicin på blodtryck kan också påverkas av personens ålder och kön (Gu m.fl. 2008) vilka inkluderas i modellen för att isolera den förklarande variabelns effekt. I alla dessa exempel är det endast en effekt som är av intresse att undersöka, trots att modellen innehåller flera variabler och tillhörande lutningsparametrar.\n\n\n\n\n”CDC - NCHS - National Center for Health Statistics — cdc.gov”. https://www.cdc.gov/nchs/.\n\n\n”Create a model of a commercial airplane where some parts are taken from a car or a boat”. 2024. OpenAI. https://chat.openai.com/chat.\n\n\nGu, Qiuping, Vicki L. Burt, Ryne Paulose-Ram, och Charles F. Dillon. 2008. ”Gender Differences in Hypertension Treatment, Drug Utilization Patterns, and Blood Pressure Control Among US Adults With Hypertension: Data From the National Health and Nutrition Examination Survey 1999–2004”. American Journal of Hypertension 21 (7): 789–98. https://doi.org/10.1038/ajh.2008.185.\n\n\n”Nicolas Cage | Actor, Producer, Director — imdb.com”. https://www.imdb.com/name/nm0000115/.",
    "crumbs": [
      "Regressionsanalys",
      "**DEL II - Regressionsanalys**",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduktion till regression</span>"
    ]
  },
  {
    "objectID": "01-regression/01-explorative-analysis.html",
    "href": "01-regression/01-explorative-analysis.html",
    "title": "2  Utforska samband",
    "section": "",
    "text": "2.1 Pingviner vid Antarktis\nÅterkommande i underlaget kommer ett insamlat datamaterial från ett forskarteam vid Antarktis användas. Teamet har mellan 2007 och 2009 samlat in information om 333 pingviner vid tre öar runtomkring Palmer Research Station. Datamaterialet kan hämtas via paketet palmerpenguins (Horst, Hill, och Gorman 2020) och läsas in i R via följande kod:\n# Glöm inte att installera paketet om du inte har gjort det förut\n# install.packages(\"palmerpenguins\")\n\n# Laddar paketet med datamaterialet\nrequire(palmerpenguins)\n\n# Filtrerar bort observationer med saknade värden\npenguins &lt;- \n  penguins %&gt;% \n  filter(!is.na(sex))\nVi kan titta närmare på ett urval av datamaterialet i Tabell 2.1.\nVisa kod\n# Generera en formaterad tabell med hjälp av kable() och formatera den med hjälp av kable_styling()\npenguins %&gt;% \n  slice_head(n = 5) %&gt;% \n  kable() %&gt;% \n  kable_styling(\"striped\")\n\n\n\n\nTabell 2.1: Urval av observationer från datamaterialet.\n\n\n\n\n \n  \n    species \n    island \n    bill_length_mm \n    bill_depth_mm \n    flipper_length_mm \n    body_mass_g \n    sex \n    year \n  \n \n\n  \n    Adelie \n    Torgersen \n    39.1 \n    18.7 \n    181 \n    3750 \n    male \n    2007 \n  \n  \n    Adelie \n    Torgersen \n    39.5 \n    17.4 \n    186 \n    3800 \n    female \n    2007 \n  \n  \n    Adelie \n    Torgersen \n    40.3 \n    18.0 \n    195 \n    3250 \n    female \n    2007 \n  \n  \n    Adelie \n    Torgersen \n    36.7 \n    19.3 \n    193 \n    3450 \n    female \n    2007 \n  \n  \n    Adelie \n    Torgersen \n    39.3 \n    20.6 \n    190 \n    3650 \n    male \n    2007\nFrån tabellen kan vi utläsa följande variabler:\nVi kommer fokusera på näbblängden som vår responsvariabel i efterföljande exempel. I och med att datamaterialet är en observationsstudie kommer vi inte kunna dra slutsatser om kausala samband, utan kan endast undersöka korrelationssamband mellan pingvinernas olika egenskaper.",
    "crumbs": [
      "Regressionsanalys",
      "**DEL II - Regressionsanalys**",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Utforska samband</span>"
    ]
  },
  {
    "objectID": "01-regression/01-explorative-analysis.html#sec-example-data",
    "href": "01-regression/01-explorative-analysis.html#sec-example-data",
    "title": "2  Utforska samband",
    "section": "",
    "text": "Notera\n\n\n\nSom en del av utforskningen kan vi identifiera saknade värden på vissa variabler och väljer att filtrera bort dessa variabler i just detta exempel. Hantering av saknade värden är ett stort fält inom statistiken och det finns flertalet metoder som kan imputera (skatta det saknade värdet) värden så att vi inte behöver ta bort hela observationer från undersökningen.\nEn enkel imputeringsmetod är medelvärdesimputering där vi byter ut det saknade värdet med medelvärdet av de övriga mätvärdena eller typvärdet ifall en kvalitativ variabel ska imputeras. I praktiken används mer avancerade metoder som kan tillämpas i många olika fall där vi också tar hänsyn till annan information om observationerna.\n\n\n\n\n\n\n\n\n\n\nNotera\n\n\n\nFunktionen kable() kommer från paketet knitr som måste (installeras och) laddas in innan vi kan använda den. Funktionen kable_styling() kommer från paketet kableExtra som på samma vis måste (installeras och) laddas in innan vi kan använda den.\n\n\n\n\nspecies: Pingvinens art mäts som en kvalitativ variabel och vi kan inte säga att en art är “bättre” eller “större” än någon annan. Vi kan alltså inte rangordna kategorierna och denna variabel följer då en nominalskala.\nisland: Vilken ö pingvinen har befunnit sig på vid mättidpunkten är också en kvalitativ variabel som inte går att rangordna. Därav följer även denna variabel en nominalskala.\nbill_length_mm: En kvantitativ variabel som mäter längden på näbben i millimeter (mm). Längd är en typisk variabel som följer en kvotskala eftersom det finns en tydlig nollpunkt.\nbill_depth_mm: Mäter näbbens djup i millimeter och följer samma resonemang som näbblängden.\nflipper_length_mm: Ytterligare en variabel som mäter en längd, nu längden av pingvinens fena. Samma resonemang som näbbens olika längder kan föras.\nbody_mass_g: Vikt av pingvinen mätt i gram. Även vikt har en tydlig nollpunkt och variabeln anses vara kvantitativ och följa en kvotskala.\nsex: Pingvinens biologiska kön vilket är en kvalitativ variabel som inte går att rangordna, nominalskala.\nyear: Denna variabel är lite svårare att bedöma då den mäter året då pingvinen är mätt som en numerisk variabel (heltal så R har sparat det som en int), men variabeln i sig behöver inte bedömas vara kvantitativ i denna kontext. Vi går inte in vidare på detta utan för enkelhetens skull kan vi säga att eftersom det går att beräkna differenser mellan åren, (det är 1 år mellan 2007 och 2008) men ingen tydlig nollpunkt finns på skalan, så kan vi anse denna variabel vara en kvantitativ variabel som följer en intervallskala.",
    "crumbs": [
      "Regressionsanalys",
      "**DEL II - Regressionsanalys**",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Utforska samband</span>"
    ]
  },
  {
    "objectID": "01-regression/01-explorative-analysis.html#visualisera-responsvariabeln",
    "href": "01-regression/01-explorative-analysis.html#visualisera-responsvariabeln",
    "title": "2  Utforska samband",
    "section": "2.2 Visualisera responsvariabeln",
    "text": "2.2 Visualisera responsvariabeln\nSom ett första steg i den explorativa analysen kan vi visualisera fördelningen av responsvariabeln med ett histogram.\n\n\nVisa kod\nggplot(penguins) + aes(x = bill_length_mm) + \n  geom_histogram(bins = 30, fill = \"steelblue\", color = \"black\") +\n  theme_bw() +\n  labs(x = \"Näbblängd (mm)\", y = \"Antal\")\n\n\n\n\n\n\n\n\nFigur 2.1: Histogram över näbblängdens fördelning\n\n\n\n\n\nFigur 2.1 ger oss en bild av variabelns egenskaper och ifall materialet innehåller några extremvärden som kan vara svåra att plocka upp med en modell. Näbblängden verkar ha en bimodal struktur med två masscentrum vid 38-40 och 50 mm. Vi ser att majoriteten av observationerna ligger mellan ca 35-52 mm men det finns också enstaka observationer omkring 58-60 mm som verkar vara något avvikande stora näbbar.\nAtt fördelningen inte ser normalfördelad ut spelar ingen roll då vi måste titta på fördelningen av responsvariabeln med avseende på de förklarande variablerna för att kontrollera en regressionsmodells antaganden.",
    "crumbs": [
      "Regressionsanalys",
      "**DEL II - Regressionsanalys**",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Utforska samband</span>"
    ]
  },
  {
    "objectID": "01-regression/01-explorative-analysis.html#sec-pairwise-visualization",
    "href": "01-regression/01-explorative-analysis.html#sec-pairwise-visualization",
    "title": "2  Utforska samband",
    "section": "2.3 Parvisa samband",
    "text": "2.3 Parvisa samband\nDatamaterialet innehåller ett flertal potentiella förklarande variabler som skulle kunna inkluderas i en modell. Beroende på hur en undersökning gått till kan variabler väljas bort om de inte anses ha ett logiskt samband med responsvariabeln, t.ex. id-variabler är inte relevanta att undersöka. I vårt exempel finns en variabel som beskriver årtal vilket vi i ett första skede kan anta inte har något logiskt samband med näbblängden. Då återstår sex andra variabler som skulle kunna inkluderas i modellanpassningen.\n\n2.3.1 Kvantitativa förklarande variabler\nFör kvantitativa förklarande variabler kan vi skapa ett spridningsdiagram där varje observation representeras med en punkt. Den förklarande variabeln placeras på x-axeln och responsvariabeln placeras på y-axeln. Med hjälp av punktsvärmen i spridningsdiagrammet kan vi få information om sambandet mellan de två variablerna. Det är fyra huvudsakliga punkter som vi fokuserar på:\n\nÄr sambandet linjärt?\nÄr sambandet positivt eller negativt?\nÄr sambandet starkt eller svagt?\nFörekommer det några extremvärden?\n\n\n\nVisa kod\nggplot(penguins) + aes(x = body_mass_g, y = bill_length_mm) +\n  geom_point(color = \"steelblue\") + \n  theme_bw() + \n  labs(x = \"Kroppsvikt (g)\", y = \"Näbblängd (mm)\")\n\n\n\n\n\n\n\n\nFigur 2.2: Spridningsdiagram som visar sambandet mellan kroppsvikt och näbblängd\n\n\n\n\n\nFigur 2.2 visar att sambandet ser till största del linjärt ut då en konstant förändring (ökning) av kroppsvikt leder till en konstant förändring (ökning) av näbblängden. Majoriteten av punkterna verkar följa denna trend, vilket tyder på ett relativt starkt samband, dock finns det ett flertal observationer (markerade i Figur 2.3) som avviker från detta. Dessa observationer har en lägre kroppsvikt men samma näbblängd som pingviner med en större kroppsvikt och påverkar styrkan av sambandet.\n\n\n\n\n\n\n\n\nFigur 2.3: Spridningsdiagram med markerat område i cirkeln\n\n\n\n\n\nVi kan beräkna Pearson’s korrelationskoefficient (\\(r\\)) för att inte behöva förlita oss på den subjektiva tolkningen av styrkan.1 Denna koefficient mäter styrkan på det linjära sambandet mellan två kvantitativa variabler och är ett lämpligt mått i just detta fall. Ett värde nära 0 tyder på inget eller ett svagt samband medan värden nära -1 eller +1 tyder på ett starkt negativt respektive positivt samband.\n\\[\nr = 0.589\n\\]\nDå korrelationskoefficienten är nära 0.6 tyder det på att sambandet är måttligt starkt.\n\n\n\n\n\n\nViktigt\n\n\n\nOm spridningsdiagrammet uppvisar ett icke-linjärt och icke-monotont (konstant) samband kommer koefficienten inte beskriva sambandets styrka på rätt sätt. Det är lätt hänt att korrelationskoefficienten används som den enda utforskande metoden då den är enkel att beräkna för flera olika par av variabler, men den kan ofta missa relevant information. Visualisering möjliggör identifieringen av komplexa samband som ofta medför att vi behöver hantera modellen på olika sätt.\n\n\nFigur 2.2 visar också vissa observationer som skulle kunna anses vara extremvärden. Till exempel skulle \\(\\{x = ~2700, y = ~47\\}\\) och \\(\\{x = ~3700, y = ~58\\}\\) vara observationer som avviker extremt från det tilltänkta sambandet och andra observationer. Detaljerad analys av extremvärden lämnar vi till senare kapitel, men i ett utforskande syfte noterar vi att vi kan ha observationer som kommer påverka modellanpassningen.\nSammanfattningsvis kan vi säga att sambandet mellan kroppsvikt och näbblängd är:\n\nlinjärt,\npositivt,\nmåttligt starkt,\nmed ev. några extremvärden.\n\nNär vi ska skapa vår första modell kommer det nog räcka med att inkludera en enkel \\(\\beta_1 \\cdot \\text{kroppsvikt}\\) term i modelleringen.\n\n2.3.1.1 Övriga kvantitativa variabler\nSamma utforskning bör genomföras för alla par av variabler, i detta fall också näbbredd och fenlängd:\n\n\n\n\n\n\n\n\nFigur 2.4: Spridningsdiagram som visar sambandet mellan näbbredd och näbblängd\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigur 2.5: Spridningsdiagram som visar sambandet mellan fenlängd och näbblängd\n\n\n\n\n\nBåda variablerna ser ut att ha linjärt samband med responsvariabeln. Figur 2.4 tyder på att sambandet mellan näbbredd och näbblängd är svagt negativt (\\(r = -0.229\\)) då punkterna är mycket utspridda medan Figur 2.5 tyder på ett lite starkare positivt samband (\\(r = 0.653\\)) i linje med Figur 2.2.\nEtt nytt fenomen som vi kan se i Figur 2.4 är att vi verkar ha flera punktsvärmar som var och en har ett positivt samband trots att vi tolkade det övergripande sambandet som svagt negativt. Om vi endast hade beräknat korrelationskoefficienten hade detta fenomen undgått vår analys. Figur 2.6 är ett exempel på Simpson’s Paradox som vi kommer undersöka närmare senare i detta underlag.\n\n\n\n\n\n\n\n\nFigur 2.6: Grupperingar av observationer\n\n\n\n\n\n\n\n\n2.3.2 Kvalitativa förklarande variabler\nVi kan inte använda spridningsdiagram för att visualisera sambandet mellan kvalitativa förklarande variabler och en kontinuerlig responsvariabel. Vi behöver istället använda visualiseringar som tar hänsyn till den kvalitativa skalan, vanligtvis ordinal eller nominalskala. Det finns flera olika sätt att visualisera fördelningen av responsvariabeln för de olika nivåerna av den förklarande, till exempel grupperade histogram eller lådagram, men en typ av visualisering som visar detaljerna i fördelningen är ett fioldiagram. Ett fioldiagram består utav en spegling av ett densitetsdiagram, där områden med många observationer har en större yta under kurvan.\nVia ggplot2 kan vi skapa ett sådant diagram genom geom_violin():\n\n\nVisa kod\nggplot(penguins) + \n  aes(x = species, y = bill_length_mm) +\n  geom_violin(fill = \"steelblue\") + \n  theme_bw() + \n  labs(x = \"Art\", y = \"Näbblängd (mm)\")\n\n\n\n\n\n\n\n\nFigur 2.7: Fördelningen av näbblängd uppdelat på art\n\n\n\n\n\nFigur 2.7 visar att Adelie-pingviner överlag har en kortare näbblängd jämfört med Chinstrap och Gentoo då fördelningens mittpunkt förhåller sig kring 38-40 mm. Chinstrap-pingviner har en något större andel pingviner med en längd större än 50 mm medan Gentoo har en större andel med en längd mindre än 50 mm.\n\n\n\n\n\n\n\n\nFigur 2.8: Fördelningen av näbblängd uppdelat på kön\n\n\n\n\n\nFigur 2.8 har en liten annorlunda form, med två stora massor för respektive kategori. Här har vi förmodligen en indikation på att kön inom de olika arterna har en påverkan och att hanar generellt har en större näbblängd än motsvarande honor av samma art.\n\n\n\n\n\n\n\n\nFigur 2.9: Fördelningen av näbblängd uppdelat på ö\n\n\n\n\n\nFigur 2.9 antyder att pingviner på ön Torgersen har en mindre näbblängd än vid övriga öar, men här behöver vi resonera huruvida denna variabel faktiskt beskriver sambandet eller om det finns något annat fenomen som kan förklara samma sak, till exempel om en ö endast har pingviner av en viss art. Mer om dessa sorters samband kommer senare i underlaget.\nSlutsatsen från dessa visualiseringar är att det verkar finnas ett samband mellan art och kön med näbblängd och de två variablerna bör inkluderas i modellen. Vi behöver nu fundera på hur vi på ett lämpligt sätt kan inkludera en kvalitativ variabel innehållande text i en matematisk modell som kräver siffror.",
    "crumbs": [
      "Regressionsanalys",
      "**DEL II - Regressionsanalys**",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Utforska samband</span>"
    ]
  },
  {
    "objectID": "01-regression/01-explorative-analysis.html#sec-exercise-explore",
    "href": "01-regression/01-explorative-analysis.html#sec-exercise-explore",
    "title": "2  Utforska samband",
    "section": "2.4 Övningsuppgifter",
    "text": "2.4 Övningsuppgifter\nAnvänd datamaterialet marketing som går att hämta via:\n\ndevtools::install_github(\"kassambara/datarium\")\n\ndata(\"marketing\", package = \"datarium\")\n\nDatamaterialet innehåller tre variabler som beskriver reklambudget för YouTube, Facebook och nyhetstidningar (tusentals dollar) samt försäljningen (tusentals enheter). Vi vill modellera sambandet mellan försäljningen och de tre reklamkällorna.\n\nUndersöka variablernas typ och skala.\nSammanställ beskrivande statistik för respektive variabel.\nVisualisera fördelningen av respektive variabel.\nSkapa ett spridningsdiagram för varje förklarande variabel med responsvariabeln och tolka de utefter de fyra bitar information som ett spridningsdiagram visar.\nSammanfatta dina iakttagelser och motivera vilka förklarande variabler som bör inkluderas i en modell och hur de bör struktureras.\n\n\n\n\n\nHorst, Allison Marie, Alison Presmanes Hill, och Kristen B Gorman. 2020. palmerpenguins: Palmer Archipelago (Antarctica) penguin data. https://doi.org/10.5281/zenodo.3960218.\n\n\nKendall, Maurice G. 1955. Rank correlation methods, 2nd ed. Oxford, England: Hafner Publishing Co.\n\n\nSpearman, C. 1904. ”The Proof and Measurement of Association between Two Things”. The American Journal of Psychology 15 (1): 72–101. http://www.jstor.org/stable/1412159.",
    "crumbs": [
      "Regressionsanalys",
      "**DEL II - Regressionsanalys**",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Utforska samband</span>"
    ]
  },
  {
    "objectID": "01-regression/01-explorative-analysis.html#footnotes",
    "href": "01-regression/01-explorative-analysis.html#footnotes",
    "title": "2  Utforska samband",
    "section": "",
    "text": "Eller andra mått för att beräkna styrkan på samband, t.ex. Kendall (Kendall 1955) eller Spearman (Spearman 1904).↩︎",
    "crumbs": [
      "Regressionsanalys",
      "**DEL II - Regressionsanalys**",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Utforska samband</span>"
    ]
  },
  {
    "objectID": "01-regression/02-model-structure.html",
    "href": "01-regression/02-model-structure.html",
    "title": "3  Modellanpassning",
    "section": "",
    "text": "3.1 Indikatorvariabler\nEn regressionsmodell kan inte hantera kvalitativa variabler direkt, exempelvis \\(\\beta_4 \\cdot \\text{art}\\), då variabelns värden beskriver kategorier inte värden från en numerisk skala. Detta gäller även om den kvalitativa variabeln är kodad numerisk. En lutningsparameter beskriver den konstanta förändring i responsvariabeln när den tillhörande förklarande variabeln ökar med en enhet, men en kvalitativ variabel har oftast ingen enhet och inte heller konstanta förändringar mellan intilliggande värden. Istället måste vi transformera den kvalitativa variabeln numerisk genom indikatorvariabler (även kallad dummyvariabler).\nSom namnet antyder används indikatorvariabler för att indikera vilken kategori en observation har uppmätt på den kvalitativa variabeln. Vi behöver då skapa en begränsad mängd indikatorvariabler som på ett tydligt sätt visar exakt en kategori per observation.\nAnta att en kvalitativ variabel har 3 kategorier: \\[\n\\begin{bmatrix}A\\\\B\\\\C\\end{bmatrix}\n\\] Vi kan börja med att skapa en indikatorvariabel för kategori A som antar värdet 1 om observationen har uppmätt kategorin, 0 annars:\n\\[\n\\begin{bmatrix}A\\\\B\\\\C\\end{bmatrix} = \\begin{bmatrix}1\\\\0\\\\0\\end{bmatrix}\n\\] Med endast en indikatorvariabel kan vi inte tydligt identifiera om en observation har uppmätt kategori B eller C då de båda har värdet 0, så vi lägger till ytterligare en indikator som antar värdet 1 om observationen uppmätt kategori B, 0 annars:\n\\[\n\\begin{bmatrix}A\\\\B\\\\C\\end{bmatrix} = \\begin{bmatrix}1 & 0\\\\0 & 1\\\\0 & 0\\end{bmatrix}\n\\] Nu skulle det vara lätt att fortsätta, att skapa en indikatorvariabel även för den sista kategorin, men det behövs inte. Om båda indikatorvariablerna är 0 har vi lyckats identifiera att observationen uppmätt kategori C och ytterligare en variabel är bara onödig information.\nDen sista kategorin blir också vår referenskategori, den kategori som de andra indikatorvariablernas effekter tolkas gentemot. När vi tolkar lutningsparametrar för indikatorvariabler, till exempel indikatorvariabeln för A, mäts förändringen i \\(Y\\) när \\(X = A\\) jämfört med när \\(X = C\\).\nGenerellt skapas \\(\\text{antal kategorier} - 1\\) indikatorvariabler för varje kvalitativa variabel som ska inkluderas i en regressionsmodell. Valet av referenskategori för respektive är godtyckligt, men vanligtvis används den första eller sista kategorin för detta ändamål.\nFör att slutföra modelleringen av Ekvation 3.1 ska vi inkludera Art och Kön i modellen. Då behöver vi skapa två respektive en indikatorvariabel enligt:\n\\[\\begin{align*}\n  Gentoo &= \\begin{cases}\n            1 \\qquad \\text{om art Gentoo}\\\\\n            0 \\qquad \\text{annars}\n        \\end{cases}\\\\\n  Chinstrap &= \\begin{cases}\n      1 \\qquad \\text{om art Chinstrap}\\\\\n      0 \\qquad \\text{annars}\n  \\end{cases}\n\\end{align*}\\]\noch\n\\[\\begin{align*}\n  hane &= \\begin{cases}\n            1 \\qquad \\text{om hane}\\\\\n            0 \\qquad \\text{annars}\n        \\end{cases}\n\\end{align*}\\]\nför att till slut skapa följande modell:\n\\[\n\\text{näbblängd} = \\beta_0 + \\beta_1 \\cdot \\text{kroppsvikt} + \\beta_2 \\cdot \\text{fenlängd} + \\beta_3 \\cdot \\text{näbbredd} + \\beta_4 \\cdot \\text{Gentoo} + \\beta_5 \\cdot \\text{Chinstrap} + \\beta_6 \\cdot \\text{hane} + E\n\\tag{3.2}\\]\ndär Adelie och honor agerar referenskategori för respektive kvalitativ variabel.",
    "crumbs": [
      "Regressionsanalys",
      "**DEL II - Regressionsanalys**",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Modellanpassning</span>"
    ]
  },
  {
    "objectID": "01-regression/02-model-structure.html#sec-indicator-variable",
    "href": "01-regression/02-model-structure.html#sec-indicator-variable",
    "title": "3  Modellanpassning",
    "section": "",
    "text": "Viktigt\n\n\n\nRent matematiskt kommer tre indikatorvariabler modellera ett perfekt samband och skapa problem med singularitet i beräkningarna.",
    "crumbs": [
      "Regressionsanalys",
      "**DEL II - Regressionsanalys**",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Modellanpassning</span>"
    ]
  },
  {
    "objectID": "01-regression/02-model-structure.html#modellanpassning",
    "href": "01-regression/02-model-structure.html#modellanpassning",
    "title": "3  Modellanpassning",
    "section": "3.2 Modellanpassning",
    "text": "3.2 Modellanpassning\nEkvation 3.2 visar den sanna modell som utgår ifrån populationens alla observerade värden, men nästintill alla undersökningar utgår från någon form av urval. Även en totalundersökning under en viss period kan anses vara ett urval i tiden om modellen avses att användas efter undersökningsperioden är slutförd.\nVi kan beteckna den anpassade modellen med dess skattade parametrar enligt:\n\\[\n\\hat{y}_i = b_0 + b_1 \\cdot x_{1i} + b_2 \\cdot x_{2i} + b_3 \\cdot x_{3i} + b_4 \\cdot x_{4i} + b_5 \\cdot x_{5i} + b_6 \\cdot x_{6i}\n\\tag{3.3}\\] där \\[\\begin{align*}\n  \\hat{y}_i &= \\text{responsvariabelns skattade värde för observation i}\\\\\n  b_0 &= \\text{skattning av interceptet}\\\\\n  b_1 - b_6 &= \\text{skattning av lutningsparametrar}\n\\end{align*}\\]\n\n\n\n\n\n\nNotera\n\n\n\nViss litteratur använder \\(\\hat{\\beta}\\) som beteckning för skattade parametrar.\n\n\nModellen anpassas med hjälp av minsta kvadratskattningen (eng. Ordinary Least Squares, OLS), där syftet är att minimera modellens totala fel. Vi kan notera att Ekvation 3.3 saknar feltermen \\(E\\) som inkluderas tidigare, vilket kommer från att den anpassade modellen endast består av regressionslinjen. Kom ihåg att en regressionsmodell ämnar att ge en förenkling av verkligheten. Men \\(E\\) beskrev ju felet i modellen och om vi ska minimera det totala felet behöver vi på något sätt ta hänsyn till denna term i modellanpassningen.\nAnta att vi anpassar en modell enbart på kroppsvikt och näbblängd. Om vi skulle projicera den anpassade enkla linjära modellen i ett spridningsdiagram över de två variablerna (Figur 3.1) skulle linjen inte lyckas träffa alla punkter exakt, varje enskilda observation kommer ligga ett visst avstånd från regressionslinjen. Detta avstånd är observationens residual som betecknas med \\(e_i\\).\n\n\n\n\n\n\n\n\nFigur 3.1: Visualisering av regressionsmodellens residualer\n\n\n\n\n\nMatematiskt beräknar vi \\(e_i = Y_i - \\hat{Y}_i\\), där \\(Y_i\\) är det observerade värdet (punkten) och \\(\\hat{Y}_i\\) är modellens anpassade värde (linjen). Minsta kvadratskattningen beräknar modellens alla parametrar så att det totala felet (Sum of Squares of Error, SSE) för alla residualer blir så litet som möjligt.\n\\[\nSSE = \\sum_{i = 1}^n e_i^2 = \\sum_{i = 1}^n (Y_i - \\hat{Y}_i)^2\n\\tag{3.4}\\]\nI en enkel linjär regression går det att härleda fram analytiska lösningar för de två parameterskattningarna, \\(b_0\\) och \\(b_1\\), som minimerar SSE men så fort vi inkluderar flera variabler blir detta betydligt svårare. Istället förlitar vi (och R) oss på matrisberäkningar som presenteras mer i Avsnitt 3.3.\n\n\n\n\n\n\nViktigt\n\n\n\nFormler för parameterskattningarna i en enkel linjär regression är: \\[\\begin{align*}\n  b_1 &= \\frac{\\sum_{i=1}^n(X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum_{i=1}^n(X_i - \\bar{X})^2}\\\\\n  b_0 &= \\bar{Y} - b_1 \\cdot \\bar{X}\n\\end{align*}\\]\n\\(b_1\\) kan också omformuleras till beräkningsformeln: \\[\n\\begin{aligned}\n\\frac{\\sum_{i=1}^n(X_i \\cdot Y_i) - \\frac{\\sum_{i=1}^nX_i \\cdot \\sum_{i=1}^nY_i}{n}}{\\sum_{i=1}^nX_i^2 - \\frac{(\\sum_{i=1}^nX_i)^2}{n}}\n\\end{aligned}\n\\]\n\n\nVi kan också använda den anpassade modellen för att prediktera nya värden på responsvariabeln för nya observationer. Från den anpassade regressionslinjen byter vi ut respektive variabel med observationens faktiska värde och får till slut en enkel summa som beskriver responsvariabelns värde på linjen. Mer om hur prediktioner används i relation till populationen tas upp i Avsnitt 5.4.\n\n3.2.1 Modellanpassning i R\nFör att anpassa en linjär regressionsmodell i R används funktionen lm() med följande argument:\n\nformula: modellens struktur som ett formelobjekt\ndata: datamaterialet som variablerna hittas\n\nEtt formelobjekt är ett speciellt format som R använder för att beskriva relationen mellan variabler. Generellt anges formatet som y ~ x där x består utav de olika förklarande variablerna, till exempel bill_length_mm ~ body_mass_g + bill_depth_mm. Det finns ett kortkommando (~ .) som används i exemplet nedan, där alla övriga variabler inkluderas i högerledet , men det kräver att vi först har ett datamaterial enbart bestående av de förklarande variablerna från Ekvation 3.2.\nVi måste också se till att alla variabler i datamaterialet har rätt variabeltyp som vi förväntar oss. Vi identifierade i Avsnitt 2.1 att vi hade tre kvantitativa variabler och två kvalitativa variabler som i R motsvarar typerna numeric och Factor. Att använda sig av Factor underlättar transformationen till indikatorvariabler eftersom R vet att den måste göra så för att modellen ska fungera. Om de kvalitativa variablerna var av typen character eller kodad numeric är det inte säkert att R skapar indikatorvariabler. Vi kan undersöka variabeltyperna för penguins med hjälp av str().\n\n# Tar endast med de variabler som vi ansåg ha ett samband med responsvariabeln\nmodelData &lt;- \n  penguins %&gt;% \n  select(\n    bill_length_mm,\n    body_mass_g,\n    flipper_length_mm,\n    bill_depth_mm,\n    species,\n    sex\n  )\n\n# Anpassar angiven modell\nsimpleModel &lt;- lm(formula = bill_length_mm ~ ., data = modelData)\n\nMed summary() får vi en detaljerad utskrift för modellen som inkluderar de anpassade regressionskoefficienterna. Vid presentation av en sådan utskrift kan vi använda kable() eller xtable() för att få en snyggare utskrift.\n\nVisa kod\nsummary(simpleModel)\n\n\n\n\n\n\n\nCall:\nlm(formula = bill_length_mm ~ ., data = modelData)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.3939 -1.3424 -0.0421  1.2695 11.4274 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       1.502e+01  4.374e+00   3.433 0.000674 ***\nbody_mass_g       1.084e-03  4.231e-04   2.562 0.010864 *  \nflipper_length_mm 6.856e-02  2.315e-02   2.961 0.003293 ** \nbill_depth_mm     3.130e-01  1.541e-01   2.032 0.043000 *  \nspeciesChinstrap  9.566e+00  3.497e-01  27.351  &lt; 2e-16 ***\nspeciesGentoo     6.404e+00  1.030e+00   6.215 1.56e-09 ***\nsexmale           2.030e+00  3.892e-01   5.215 3.27e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.217 on 326 degrees of freedom\nMultiple R-squared:  0.8386,    Adjusted R-squared:  0.8356 \nF-statistic: 282.3 on 6 and 326 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nFigur 3.2: Inte särskilt snygg utskrift\n\n\n\n\n\nVisa kod\nsummary(simpleModel) %&gt;% \n  coef() %&gt;% \n  as_tibble(rownames = NA) %&gt;% \n  rownames_to_column() %&gt;% \n  rename(\n    ` ` = rowname,\n    Skattning = Estimate,\n    Medelfel = `Std. Error`,\n    `t-värde` = `t value`,\n    `p-värde` = `Pr(&gt;|t|)`\n  ) %&gt;% \n  kable(\n    digits = 4\n  ) %&gt;% \n  kable_styling(\"striped\")\n\n\n\n\nTabell 3.1: En snygg utskrift av modellens anpassade parametrar\n\n\n\n\n \n  \n     \n    Skattning \n    Medelfel \n    t-värde \n    p-värde \n  \n \n\n  \n    (Intercept) \n    15.0166 \n    4.3742 \n    3.4330 \n    0.0007 \n  \n  \n    body_mass_g \n    0.0011 \n    0.0004 \n    2.5617 \n    0.0109 \n  \n  \n    flipper_length_mm \n    0.0686 \n    0.0232 \n    2.9608 \n    0.0033 \n  \n  \n    bill_depth_mm \n    0.3130 \n    0.1541 \n    2.0316 \n    0.0430 \n  \n  \n    speciesChinstrap \n    9.5655 \n    0.3497 \n    27.3508 \n    0.0000 \n  \n  \n    speciesGentoo \n    6.4044 \n    1.0304 \n    6.2154 \n    0.0000 \n  \n  \n    sexmale \n    2.0297 \n    0.3892 \n    5.2153 \n    0.0000 \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip 3.1\n\n\n\nFör att skapa denna snygga utskrift av koefficienterna behöver vi plocka ut en enskild del av summary() med hjälp av coef(). I dokumentationen för lm() finns mer information om vad som kan hämtas från det resulterande regressionsobjektet.\nR är ett objektorienterat programmeringsspråk, och funktionen lm() returnerar ett objekt av klassen ”lm”, vilket är en lista. Det är enkelt att plocka önskade delar från den listan vid behov. Det finns en mängd funktioner kopplade till objekt av klassen ”lm”:\n\ncoef(): Ger regressionskoefficienter\nresiduals(): Ger residualerna\nfitted(): Ger de anpassade värdena (\\(\\hat{Y}\\))\nsummary(): Ger en sammanfattande analys av regressionsmodellen. Funktionen returnerar ett objekt av klassen ”summary.lm”. Se ?summary.lm i dokumentationen. coef() funkar även på dessa objekt som vi såg ovan.\nanova(): Ger ANOVA-tabellen för modellen\npredict(): gör prediktioner för (nya) x-värden, alltså beräknar \\(\\hat{Y}\\) för givna x-värden. Kan även beräkna konfidensintervall och prediktionsintervall för \\(\\hat{Y}\\). Se ?predict.lm() för detaljer.\nplot(): Ger olika diagnostiska plottar, se ?plot.lm för detaljer.\nconfint(): Beräknar konfidensintervall för regressionskoefficienterna\nmodel.matrix(): skapar olika typer av designmatriser som kan användas i lm(), se Avsnitt 3.3.\n\nDet är också användbart att använda str() på lm-objekt. Kolla i ?lm() under Value rubriken för att se vilka olika delar som finns i objektet.\n\n\nTabell 3.1 visar de skattade lutningsparametrarna (koefficienterna). Exempelvis kan vi se att för varje gram mer en pingvin väger ökar näbbens längd med ca 0.0011 mm i genomsnitt givet att alla andra variabler hålls konstanta. Den sista delen av denna tolkning är viktig att inkludera då en förändring av flera variabler skulle medföra en annan förändring av responsvariabeln i relation till respektive koefficient.\nIndikatorvariablerna tolkas inom sin grupp jämfört med referenskategorin, till exempel har Gentoo-pingviner i genomsnitt en 6.4 mm större näbblängd än referenskategorin Adelie-pingviner givet att alla andra variabler hålls konstanta.\nInterceptet är endast relevant att tolka om värdemängden är alla 0, det vill säga att data täcker det område där alla förklarande variabler antar värdet 0. I just detta exempel finns det inte data över dessa områden vilket medför att värdet på interceptet inte har någon rimlig tolkning.\n\n\n\n\n\n\nViktigt\n\n\n\nÄven om tolkningen av interceptet inte blir rimlig måste interceptet inkluderas i modellanpassningen för att minsta kvadratskattningen ska minimera SSE. Om interceptet hade plockats bort motsvarar det en linje som tvingas att korsa y-axeln vid \\(y = 0\\) vilket resulterar i att modellen inte beskriver de fenomen som vi vill att den ska beskriva.\n\n\nDet är inte bara koefficienttabellen som är relevant att titta på i en modellanpassning och vi kommer tillbaka till de andra objekten som finns inuti lm senare.",
    "crumbs": [
      "Regressionsanalys",
      "**DEL II - Regressionsanalys**",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Modellanpassning</span>"
    ]
  },
  {
    "objectID": "01-regression/02-model-structure.html#sec-matrices",
    "href": "01-regression/02-model-structure.html#sec-matrices",
    "title": "3  Modellanpassning",
    "section": "3.3 Matrisberäkningar",
    "text": "3.3 Matrisberäkningar\nMatriser underlättar de tunga beräkningar som krävs för att anpassa en regressionsmodell med flera förklarande variabler. Vi kan formulera en regressionsmodell i matrisform enligt: \\[\n\\mathbf{Y} = \\mathbf{X} \\boldsymbol{\\beta} + \\mathbf{E}\n\\tag{3.5}\\] där, \\[\n    \\mathbf{Y} = \\underset{n \\times 1}{\\begin{bmatrix}Y_1\\\\Y_2\\\\\\vdots\\\\Y_n\\end{bmatrix}} \\quad \\mathbf{X} = \\underset{n \\times p}{\\begin{bmatrix}1 & X_{11} & X_{12} & \\cdots & X_{1k}\\\\1 & X_{21} & X_{22} & \\cdots & X_{2k}\\\\\\vdots & \\vdots & \\vdots & \\ddots & \\vdots\\\\1 & X_{n1} & X_{n2} & \\cdots & X_{nk}\\end{bmatrix}\n} \\quad \\boldsymbol{\\beta} = \\underset{p \\times 1}{\\begin{bmatrix}\\beta_0\\\\\\beta_1\\\\\\vdots\\\\\\beta_k\\end{bmatrix}} \\quad \\mathbf{E} = \\underset{n \\times 1} {\\begin{bmatrix}E_1\\\\E_2\\\\\\vdots\\\\E_n\\end{bmatrix}}\n\\] \\(\\mathbf{X}\\) kallas för designmatrisen och innehåller alla \\(k\\) förklarande variabler, en kolumn för varje, samt en första kolumn med 1:or som motsvarar interceptet. Indikatorvariabler adderar till antalet förklarande variabler trots att de utgår ifrån samma kvalitativa variabel, se Ekvation 3.2 där vi totalt har 6 förklarande variabler. \\(p\\) beskriver antalet parametrar, motsvarande \\(k + 1\\) antalet lutningsparametrar + interceptet, och \\(n\\) är antalet observationer.\nSkattningen av \\(\\hat{\\boldsymbol{\\beta}}\\) minimerar fortfarande SSE där: \\[\nSSE = (\\mathbf{Y} - \\mathbf{X}\\boldsymbol{\\hat{\\beta}})'(\\mathbf{Y}-\\mathbf{X}\\boldsymbol{\\hat{\\beta}})\n\\tag{3.6}\\]\noch \\[\n\\boldsymbol{\\hat{\\beta}} = (\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\mathbf{Y}\n\\tag{3.7}\\]\n\n3.3.1 Matriser i R\nR använder sig av matriser i bakgrunden när vi använder lm() men vi kan också skapa våra egna utifrån datamaterialet och genomföra matrisberäkningen för \\(\\boldsymbol{\\hat{\\beta}}\\) eller SSE.\nDesignmatrisen är den mest komplexa att skapa, speciellt om vi har kvalitativa variabler med i data, men som tur är kan vi använda samma formelobjekt i funktionen model.matrix().\n\nX &lt;- \n  model.matrix(\n    bill_length_mm ~ ., \n    data = modelData\n  )\n\n# Visar de första fem raderna i matrisen\nX[1:5,]\n\n  (Intercept) body_mass_g flipper_length_mm bill_depth_mm speciesChinstrap\n1           1        3750               181          18.7                0\n2           1        3800               186          17.4                0\n3           1        3250               195          18.0                0\n4           1        3450               193          19.3                0\n5           1        3650               190          20.6                0\n  speciesGentoo sexmale\n1             0       1\n2             0       0\n3             0       0\n4             0       0\n5             0       1\n\nY &lt;- \n  modelData$bill_length_mm %&gt;% \n  as.matrix()\n\n# Visar de första fem raderna i vektorn\nY[1:5,]\n\n[1] 39.1 39.5 40.3 36.7 39.3\n\n\nDe fem första raderna i respektive matris är transformerade värden från Tabell 2.1 och designmatrisen innehåller indikatorvariabler enligt Ekvation 3.2.\n\n3.3.1.1 Skattning av \\(\\boldsymbol{\\hat{\\beta}}\\)\nNu kan vi med hjälp av matrisberäkningsformler i R beräkna koefficienterna:\n\nbetaHat &lt;- solve(t(X) %*% X) %*% t(X) %*% Y\n\n\n\n\n\nTabell 3.2: Skattade koefficienter från matrisberäkning avrundat till fyra decimaler\n\n\n\n\n \n  \n     \n    Koefficient \n  \n \n\n  \n    (Intercept) \n    15.0166 \n  \n  \n    body_mass_g \n    0.0011 \n  \n  \n    flipper_length_mm \n    0.0686 \n  \n  \n    bill_depth_mm \n    0.3130 \n  \n  \n    speciesChinstrap \n    9.5655 \n  \n  \n    speciesGentoo \n    6.4044 \n  \n  \n    sexmale \n    2.0297 \n  \n\n\n\n\n\n\n\n\nTabell 3.2 visar samma parameterskattningar som Tabell 3.1, eftersom det är samma beräkningar som genomförts. Vi ser dock fler värden i den tidigare tabellen vilket uppkommer från att lm() omfattar fler beräkningar som sedan sammanställs i ett och samma objekt.\nTill exempel beräknas även prediktioner och residualer, vilket vi också kan göra med matrisberäkningar enligt:\n\nYhat &lt;- X %*% betaHat\n\ne &lt;- Y - Yhat\n\n\n\n3.3.1.2 Kovariansmatris för \\(\\boldsymbol{\\hat{\\beta}}\\)\nVariansen för respektive parameter kan också beräknas med matriser, där medelfelet är roten ur diagonalelementen från kovariansmatrisen. \\[\ns^2_{\\boldsymbol{\\hat{\\beta}}} = (\\mathbf{X}'\\mathbf{X})^{-1}MSE\n\\] där MSE är \\(\\frac{SSE}{n - (k + 1)}\\).\nDå beräkningen av SSE och MSE utgår från matriser kommer även deras objekt vara en \\(1 \\times 1\\) matris, men i beräkningen av kovariansmatrisen är MSE endast en skalär. Vi behöver därför explicit ange att MSE inte längre är en matris för att undvika problem med matrisdimensioner.\n\n# Beräknar SSE\nSSE &lt;- t(Y - Yhat) %*% (Y - Yhat)\n\n# Beräknas MSE\nMSE &lt;- SSE / (nrow(X) - ncol(X))\n\n# Beräknar kovariansmatrisen för Beta\ns2Beta &lt;- solve(t(X) %*% X) * as.numeric(MSE)\n\n\n\n\n\nTabell 3.3: Skattade medelfel från matrisberäkning avrundat till fyra decimaler\n\n\n\n\n \n  \n     \n    Medelfel \n  \n \n\n  \n    (Intercept) \n    4.3742 \n  \n  \n    body_mass_g \n    0.0004 \n  \n  \n    flipper_length_mm \n    0.0232 \n  \n  \n    bill_depth_mm \n    0.1541 \n  \n  \n    speciesChinstrap \n    0.3497 \n  \n  \n    speciesGentoo \n    1.0304 \n  \n  \n    sexmale \n    0.3892 \n  \n\n\n\n\n\n\n\n\nVi ser även i Tabell 3.3 samma värden som Tabell 3.1.",
    "crumbs": [
      "Regressionsanalys",
      "**DEL II - Regressionsanalys**",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Modellanpassning</span>"
    ]
  },
  {
    "objectID": "01-regression/02-model-structure.html#sec-exercise-model-fit",
    "href": "01-regression/02-model-structure.html#sec-exercise-model-fit",
    "title": "3  Modellanpassning",
    "section": "3.4 Övningsuppgifter",
    "text": "3.4 Övningsuppgifter\nVi kommer återigen använda datamaterialet marketing.\n\nAnpassa en linjär regressionsmodell med lm() som inkluderar de variabler som du valt ut i Avsnitt 2.4.\nSammanställ en tabell över de skattade koefficienterna och tolka respektive.\nSkapa designmatrisen och en matris för responsvariabeln och skatta lutningsparametrarna med hjälp av dessa. Kontrollera att du får samma värden som i tabellen från lm().\nAnvänd matrisberäkningar för att beräkna medelfelet för respektive parameter.",
    "crumbs": [
      "Regressionsanalys",
      "**DEL II - Regressionsanalys**",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Modellanpassning</span>"
    ]
  },
  {
    "objectID": "01-regression/03-model-assessment.html",
    "href": "01-regression/03-model-assessment.html",
    "title": "4  Modellutvärdering",
    "section": "",
    "text": "4.1 Residualanalys\nResidualanalys innebär att beräkna och visuellt utforska residualerna från en modell gentemot modellantaganden \\(E\\overset{iid}{\\sim}N(0, \\sigma^2)\\), det vill säga att residualerna är oberoende, normalfördelade med väntevärde 0 och lika varians. Residualerna kan också användas för att undersöka ifall den linjära modell som anpassats är lämplig. Vi kommer titta närmare på mer detaljerad residualanalys i ett senare kapitel.\nFör enkelhetens skull kan vi plocka ut residualerna samt de observerade och skattade värdena på responsvariabeln från den skattade modellen (se Tip 3.1).\n# Skapa ett datamaterial för visualiseringar\n\nresidualData &lt;- \n  tibble(\n    residuals = residuals(simpleModel),\n    y = modelData$bill_length_mm,\n    yHat = fitted(simpleModel)\n  )\nVi kommer visualisera dessa variabler i olika former med hjälp av ggplot2 vilket kräver att vi har en data.frame eller tibble med data.",
    "crumbs": [
      "Regressionsanalys",
      "**DEL II - Regressionsanalys**",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Modellutvärdering</span>"
    ]
  },
  {
    "objectID": "01-regression/03-model-assessment.html#sec-residual-simple",
    "href": "01-regression/03-model-assessment.html#sec-residual-simple",
    "title": "4  Modellutvärdering",
    "section": "",
    "text": "4.1.1 Normalfördelning\nVi kan undersöka antagandet om normalfördelade residualer genom ett histogram och ett QQ-diagram (quantile-quantile diagram).\n\n\nVisa kod\nggplot(residualData) + \n  aes(x = residuals, y = after_stat(density)) +\n  geom_histogram(binwidth = 1, fill = \"steelblue\", color = \"black\") + \n  theme_bw() + \n  labs(x = \"Residualer\", y = \"Densitet\")\n\n\n\n\n\nResidualernas fördelning\n\n\n\n\n\n\nVisa kod\nggplot(residualData) + \n  # Använder standardiserade residualer\n  aes(sample = scale(residuals)) + \n  geom_qq_line() +\n  geom_qq(color = \"steelblue\") +\n  theme_bw() + \n  labs(x = \"Teoretiska kvantiler\", y = \"Observerade kvantiler\")\n\n\n\n\n\nResidualernas observerade kvantiler jämfört med teoretiska normalfördelade kvantiler.\n\n\n\n\nI histogrammet vill vi se normalfördelningens symmetriska och klockliknande form centrerad kring 0 vilket ibland kan vara svårt att utläsa speciellt om datamaterialet är litet. QQ-diagrammet visar de observerade och de teoretiska kvantilerna där vi vill att punkterna ska följa den inritade linjen för en “perfekt” normalfördelning.\nFör denna modell ser vi inga tydliga avvikelser från det mönster vi vill se, men vi kan utläsa ett fåtal avvikande observationer som skulle kunna betraktas som extremvärden. Två stora positiva residualer kan identifieras i diagrammen men det finns även enstaka negativa som ligger långt från de övriga.\n\n\n\n\n\n\nViktigt\n\n\n\nVi kan betrakta antagandet om normalfördelning som inte uppfyllt om dessa diagram visar på starka avvikelser från det vi vill se. Även när vi vet att ett urval är draget från en normalfördelning är det inte alltid som histogrammet visar den form som vi söker.\n\n\nVisa kod\nset.seed(1234)\n\ntibble(\n  x = rnorm(30)\n) %&gt;% \n ggplot() + \n  aes(x = x, y = after_stat(density)) +\n  geom_histogram(bins = 10, fill = \"steelblue\", color = \"black\") + \n  theme_bw() + \n  labs(x = \"x\", y = \"Densitet\")\n\n\n\n\n\nFördelning av ett urval från den faktiska normalfördelningen\n\n\n\n\nStarka avvikelser från normalfördelningen innebär exempelvis att vi ser flera områden med hög densitet:\n\n\nVisa kod\nset.seed(1234)\n\ntibble(\n  x = runif(30)\n) %&gt;% \n ggplot() + \n  aes(x = x, y = after_stat(density)) +\n  geom_histogram(bins = 10, fill = \"steelblue\", color = \"black\") + \n  theme_bw() + \n  labs(x = \"x\", y = \"Densitet\")\n\n\n\n\n\nFördelning av ett urval från andra fördelningar\n\n\n\n\neller en väldigt skev fördelning:\n\n\nVisa kod\nset.seed(1234)\n\ntibble(\n  x = rchisq(30, df = 2)\n) %&gt;% \n ggplot() + \n  aes(x = x, y = after_stat(density)) +\n  geom_histogram(bins = 10, fill = \"steelblue\", color = \"black\") + \n  theme_bw() + \n  labs(x = \"x\", y = \"Densitet\")\n\n\n\n\n\nFördelning av ett urval från andra fördelningar\n\n\n\n\nDessa diagram indikerar att modellen saknar en förklarande variabel eller måste transformeras på något sätt för att uppfylla antagandet.\nOm QQ-diagrammet uppvisar tydliga mönster, till exempel om punkterna är krökta runt linjen, betyder det att modellen inte uppfyller antagandet om linjärt samband.\n\n\n\n\n\nExempel på mönster i QQ-diagram\n\n\n\n\n\n\n\n\n4.1.2 Lika varians\nVi kan kontrollera antagandet om residualernas lika varians genom ett spridningsdiagram med residualerna på y-axeln och någon av anpassade värden eller observerade värden på förklarande eller responsvariabeln. Vanligtvis används de anpassade värdena för att x-axeln ska beskriva hela modellen, men andra variabler kan vara användbara att visualisera för att identifiera potentiella orsaker till ett brustet antagande.\n\n\nVisa kod\nggplot(residualData) + \n  aes(x = yHat, y = residuals) + \n  geom_point(color = \"steelblue\") + \n  theme_bw() +\n  labs(x = \"Anpassade värden\", y = \"Residualer\") + \n  geom_hline(\n    aes(yintercept = 0)\n  ) + \n  # Imaginära gränser\n  geom_hline(\n    aes(yintercept = -5),\n    color = \"#d9230f\",\n    linetype = 2\n  ) + \n  geom_hline(\n    aes(yintercept = 5),\n    color = \"#d9230f\",\n    linetype = 2\n  )\n\n\n\n\n\n\n\n\nFigur 4.1: Residualernas spridning mot anpassade värden.\n\n\n\n\n\nFör att uppfylla antagandet om lika varians, ska punkterna i varje tvärsnitt av värden på x-axeln vara jämnt utspridda. Tänk som att vi vill placera två stycken parallella linjer längs med maximum och minimum-värden för residualerna (de två rödstreckade linjerna i Figur 4.1) och en stor majoritet av punkterna bör ligga utspridda emellan dessa. Vi ser i Figur 4.1 att några enstaka observationer faktiskt hamnar utanför och ökar variationen i vissa tvärsnitt, men då det inte är tydliga avvikelser kan vi anse att residualerna har uppfyllt antagandet om lika varians.\n\n\n\n\n\n\nViktigt\n\n\n\nOm linjerna som täcker maximum och minimum-värden för residualerna inte är parallella uppfyller inte modellen kravet om lika varians.\n\n\n\n\n\nExempel på icke-konstant varians i residualerna\n\n\n\n\n\n\n\nExempel på icke-konstant varians i residualerna\n\n\n\n\nDessa fenomen betyder oftast att hela eller delar av modellen behöver transformeras för att uppfylla antagandet om lika varians.\nVi kan också identifiera problem med linjäritet i detta spridningsdiagram. Figuren nedan uppvisar någorlunda konstant varians i avseende på variationen i varje tvärsnitt av x-axeln, men det finns ett tydligt mönster i residualerna. Detta betyder att modellen inte lyckats modellera sambandet på rätt sätt. I detta läge vore det lämpligt att visualisera residualerna mot respektive förklarande variabel för att identifiera vilken/vilka utav de som verkar bidra med det icke-linjära sambandet.\n\n\n\n\n\nMönster i residualerna som tyder på ett icke-linjärt samband\n\n\n\n\n\n\n\n\n4.1.3 Oberoende\nOfta är det svårt eller omöjligt att undersöka om observationerna är oberoende med avseeende på alla ordningar som data kan samlas in på. Undantaget är ifall vi vet hur datainsamlingen har gått till och om det finns någon tydlig tidsaspekt, till exempel i tidsseriedata, eller att samma enhet har uppmätts flera gånger som gör att vi vet att observationerna blir beroende. Vi vill att den modell som anpassas tar hänsyn till det beroende som finns i data så att de efterföljande residualerna endast uppvisar oberoende.\nEtt linjediagram över residualerna i observationsordning kan användas för att undersöka oberoende, men det är som sagt endast i specialfall som denna visualisering används. Linjediagrammet ska uppvisa “slump”, det vill säga inga tydliga mönster i residualerna.\n\n\nVisa kod\nggplot(residualData) + \n  aes(x = 1:nrow(residualData), y = residuals) + \n  geom_line(color = \"steelblue\") + \n  theme_bw() +\n  labs(x = \"Obs. index\", y = \"Residualer\") + \n  geom_hline(\n    aes(yintercept = 0),\n    color = \"black\"\n  )\n\n\n\n\n\nResidualer i observationsordning.\n\n\n\n\nAndra exempel på data som har ett beroende är:\n\nVi samlar in data från personer, men vissa personer kommer ifrån samma famlij, detta kan göra att det finns ett beroende mellan dessa personer.\nVi samlar in spatiala (rumsliga) data, till exempel temperatur eller regnmängd på olika platser i Östergötland. Då är det vanligt att det finns en positiv korrelation mellan geografiskt närliggande observationer.\n\n\n\n4.1.4 Funktion med alla diagram\nDessa diagram kommer vara återkommande i regressionsmodellering så vi kan skapa en funktion för att automatiskt generera alla fyra diagram samtidigt. Vi får genom paketet cowplot tillgång till en funktion (plot_grid) som kan kombinera flera diagram till en och samma.\n\n\nVisa kod\n# Funktionen kräver två argument, modellen som anpassats och bredden på staplarna i histogrammet.\nresidualPlots &lt;- function(model) {\n  \n  residualData &lt;- \n    data.frame(\n      residuals = residuals(model),\n      # Responsvariabeln finns som första kolumn i modellens model-objekt\n      y = model$model[,1],\n      yHat = fitted(model)\n    )\n  \n  \n  p1 &lt;- ggplot(residualData) + \n    aes(x = residuals, y = after_stat(density)) +\n    geom_histogram(bins = 20, fill = \"steelblue\", color = \"black\") + \n    theme_bw() + \n    labs(x = \"Residualer\", y = \"Densitet\")\n  \n  p2 &lt;- ggplot(residualData) + \n    aes(x = yHat, y = residuals) + \n    geom_hline(aes(yintercept = 0)) + \n    geom_point(color = \"steelblue\") + \n    theme_bw() +\n    labs(x = \"Anpassade värden\", y = \"Residualer\")\n    \n  \n  p3 &lt;- ggplot(residualData) + \n    # Använder standardiserade residualer\n    aes(sample = scale(residuals)) + \n    geom_qq_line() + \n    geom_qq(color = \"steelblue\") +\n    theme_bw() + \n    labs(x= \"Teoretiska kvantiler\", y = \"Observerade kvantiler\")\n  \n  cowplot::plot_grid(p1, p2, p3, nrow = 2)\n  \n}\n\nresidualPlots(simpleModel)\n\n\n\n\n\n\n\n\nFigur 4.2: Residualdiagrammen i en och samma bild\n\n\n\n\n\nSammanfattningsvis visar Figur 4.2 att residualerna uppfyller antagandet om normalfördelning med väntevärde 0 och lika varians. Det finns inga tydliga mönster i något diagram som indikerar på motsatsen eller att modellen missar att plocka upp något av sambandet. Några enstaka extremvärden har identifierats, specifikt två stycken stora positiva residualer som kommer undersökas mer i senare kapitel. Slutsatsen är att modellen är en lämplig förenkling av verkligheten.",
    "crumbs": [
      "Regressionsanalys",
      "**DEL II - Regressionsanalys**",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Modellutvärdering</span>"
    ]
  },
  {
    "objectID": "01-regression/03-model-assessment.html#sec-exercise-evaluate",
    "href": "01-regression/03-model-assessment.html#sec-exercise-evaluate",
    "title": "4  Modellutvärdering",
    "section": "4.2 Övningsuppgifter",
    "text": "4.2 Övningsuppgifter\nAnvänd återigen marketing från Avsnitt 2.4.\n\nSkatta residualerna genom att beräkna skillnaden mellan de observerade och anpassade värdena på responsvariabeln.\nSkapa ett histogram och ett kvantildiagram (QQ diagram) över residualerna och kontrollera antagandet om normalfördelning.\nSkapa ett spridningsdiagram med residualerna på y-axeln och de anpassade värdena på x-axeln och kontroller antagandet om lika varians.\nSammanfatta dina slutatser och bedöm om modellen som anpassats i Avsnitt 3.4 uppfyller modellantaganden.",
    "crumbs": [
      "Regressionsanalys",
      "**DEL II - Regressionsanalys**",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Modellutvärdering</span>"
    ]
  },
  {
    "objectID": "01-regression/04-statistical-inference.html",
    "href": "01-regression/04-statistical-inference.html",
    "title": "5  Statistisk inferens",
    "section": "",
    "text": "5.1 ANOVA\nAnalysis of Variance är en samling metoder som beräknar variationen av olika modellkomponenter. Målet med en modell är att förklara den totala variationen i responsvariabeln på bästa sätt. Allting som de förklarande variablerna hjälper till att beskriva kallas för den förklarade variationen och det som modellen inte lyckas förklara (felet) är den oförklarade variationen.\n\\[\n\\underbrace{\\mathbf{Y}}_\\text{total variation} = \\underbrace{\\mathbf{X} \\boldsymbol{\\beta}}_\\text{förklarad variation} + \\underbrace{\\mathbf{E}}_\\text{oförklarad variation}\n\\tag{5.1}\\]\nEkvation 5.1 visar att den totala variationen är en summa av den förklarade och oförklarade variationen vilket också ses i formlerna för dessa. Respektive komponent beräknas enligt:\n\\[\n  \\text{total variation} = SST = \\mathbf{Y}'\\mathbf{Y} - \\left(\\frac{1}{n}\\right)\\mathbf{Y}'\\mathbf{J}\\mathbf{Y}\n\\] där \\(\\mathbf{J}\\) är enhetsmatrisen, en \\(n \\times n\\) matris endast innehållande 1:or.\nDet kanske inte är så lätt att se vad dessa matrisberäkningar faktiskt beskriver men beräkningen motsvarar \\(\\sum_{i=1}^n(Y_i - \\bar{Y})^2\\), alltså täljaren i en variansberäkning för \\(Y\\). Den vänstra termen (\\(\\mathbf{Y}'\\mathbf{Y}\\)) motsvarar \\(Y_i\\) och den högra termen (\\(\\left(\\frac{1}{n}\\right)\\mathbf{Y}'\\mathbf{J}\\mathbf{Y}\\)) motsvarar \\(\\bar{Y}\\), responsvariabelns medelvärde. Den totala variationen beskriver hur mycket variation som uppkommer ifall vi skulle använda medelvärdet av \\(Y\\) som modell.\n\\[\n  \\text{oförklarad variation} = SSE = \\mathbf{Y}'\\mathbf{Y} - \\boldsymbol{\\hat{\\beta}}'\\mathbf{X}'\\mathbf{Y}\n\\] SSE har vi tidigare använt som ett mått på felet i modellen, se Ekvation 3.4, vilket betyder att \\(\\boldsymbol{\\hat{\\beta}}'\\mathbf{X}'\\mathbf{Y}\\) motsvarar \\(\\hat{Y}_i\\).\n\\[\n  \\text{förklarad variation} = SSR = \\boldsymbol{\\hat{\\beta}}'\\mathbf{X}'\\mathbf{Y} - \\left(\\frac{1}{n}\\right)\\mathbf{Y}'\\mathbf{J}\\mathbf{Y}\n\\]\nSSR beskriver variationen mellan modellens anpassade värde och medelvärdet av \\(Y\\). Det kan i sin tur kan tolkas som hur mycket mer variation som modellen bidrar med jämfört med medelvärdet, eller kort sagt hur mycket bättre modellen är på att förklara variationen i \\(Y\\).\nVi har tidigare använt en annan matrisformel för SSE men med hjälp av omformuleringen kan vi tydligt se hur SST = SSR + SSE: \\[\n\\mathbf{Y}'\\mathbf{Y} - \\left(\\frac{1}{n}\\right)\\mathbf{Y}'\\mathbf{J}\\mathbf{Y} = \\mathbf{Y}'\\mathbf{Y} \\underbrace{-  \\boldsymbol{\\hat{\\beta}}'\\mathbf{X}'\\mathbf{Y} + \\boldsymbol{\\hat{\\beta}}'\\mathbf{X}'\\mathbf{Y}}_\\text{summerar till 0} - \\left(\\frac{1}{n}\\right)\\mathbf{Y}'\\mathbf{J}\\mathbf{Y}\n\\] Vi kan också visualisera denna relation i ett stackat stapeldiagram. Den totala höjden av stapeln är SST medan de olika delarna beskriver hur stor del av den totala variationen som är förklarad eller oförklarad i en viss modell.\nVisualisering av de olika källor av variation",
    "crumbs": [
      "Regressionsanalys",
      "**DEL II - Regressionsanalys**",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Statistisk inferens</span>"
    ]
  },
  {
    "objectID": "01-regression/04-statistical-inference.html#anova",
    "href": "01-regression/04-statistical-inference.html#anova",
    "title": "5  Statistisk inferens",
    "section": "",
    "text": "5.1.1 ANOVA-tabellen\nEn ANOVA-tabell är ett sätt att effektivt få en översikt av dessa olika komponenter samt visa ytterligare information, såsom frihetsgraderna (\\(df\\)) för respektive komponent och medelkvadratsummor.\nFrihetsgrader beskriver hur många lutningsparametrar som skattas för respektive del1 och medelkvadratsummor visar den genomsnittliga variationen per frihetsgrad, \\(\\frac{SS}{df}\\).\n\n\n\n\nTabell 5.1: Enkel ANOVA-tabell\n\n\n\n\n\n\n\n\n\n\n\n\nSource\nDF\nSum of Squares\nMean Square\n\n\n\n\nModel (Regression)\n\\(df_R = k\\)\n\\(SSR = \\boldsymbol{\\hat{\\beta}}' \\mathbf{X}' \\mathbf{Y} - \\frac{1}{n} \\mathbf{Y}' \\mathbf{J} \\mathbf{Y}\\)\n\\(MSR = \\frac{SSR}{df_R}\\)\n\n\nError\n\\(df_E = n - (k + 1)\\)\n\\(SSE = \\mathbf{Y}' \\mathbf{Y} - \\boldsymbol{\\hat{\\beta}}' \\mathbf{X}' \\mathbf{Y}\\)\n\\(MSE = \\frac{SSE}{df_E}\\)\n\n\nTotal\n\\(df_T = n - 1\\)\n\\(SSY = \\mathbf{Y}' \\mathbf{Y} - \\frac{1}{n} \\mathbf{Y}' \\mathbf{J} \\mathbf{Y}\\)\n\n\n\n\n\n\n\n\n\nEn enkel ANOVA-tabell som Tabell 5.1 visar endast de tre huvudsakliga komponenterna, men olika programvaror kan ibland visa andra uppdelningar som standard. I en multipel linjär regressionsmodell är det vanligt att dela upp den förklarade variationen ytterligare, exempelvis i sekventiella kvadratsummor.\n\n\n5.1.2 Sekventiella kvadratsummor\nBeräkningarna för en ANOVA-tabell sker automatiskt i R när vi använder lm() och vi kan plocka ut tabellen från modellobjektet med hjälp av anova(), (se Tip 3.1).\n\n\nVisa kod\nanova(simpleModel) %&gt;% \n  round(4) %&gt;% \n  kable() %&gt;% \n  kable_styling(\"striped\")\n\n\n\n\nTabell 5.2: ANOVA-tabell från R\n\n\n\n\n \n  \n     \n    Df \n    Sum Sq \n    Mean Sq \n    F value \n    Pr(&gt;F) \n  \n \n\n  \n    species \n    2 \n    7015.3857 \n    3507.6929 \n    713.4929 \n    0 \n  \n  \n    bill_depth_mm \n    1 \n    818.5050 \n    818.5050 \n    166.4905 \n    0 \n  \n  \n    flipper_length_mm \n    1 \n    198.2269 \n    198.2269 \n    40.3210 \n    0 \n  \n  \n    body_mass_g \n    1 \n    160.3760 \n    160.3760 \n    32.6218 \n    0 \n  \n  \n    sex \n    1 \n    133.7191 \n    133.7191 \n    27.1995 \n    0 \n  \n  \n    Residuals \n    326 \n    1602.6899 \n    4.9162 \n     \n     \n  \n\n\n\n\n\n\n\n\nSom standard, delar R upp modellens kvadratsumma (SSR) i de enskilda förklarande variablerna med hjälp av sekventiella (även kallad betingade) kvadratsummor. En sekventiell kvadratsumma beskriver hur mycket variation en förklarande variabel bidrar med givet att modellen redan innehåller andra förklarande variabler.\nOrdningen som presenteras i Tabell 5.2 är ordningen som variablerna läggs till i modellen, till exempel visar andra raden \\(SS(\\text{bill\\_depth\\_mm} | \\text{species})\\), att näbbredden bidrar med 818.505 ytterligare unik förklarad variation av responsvariabeln som art inte redan har förklarat. Den tredje raden visar \\(SS(\\text{flipper\\_length\\_mm} | \\text{species}, \\text{bill\\_depth\\_mm})\\), det vill säga hur mycket ytterligare unik variation som fenlängden förklarar i en modell som inkluderar näbbredd och art.\nRent matematiskt beräknas den sekventiella kvadratsumman som en summa av antingen SSE eller SSR mellan två olika modeller, en utan den tillagda variabeln och en med variabeln inkluderad. Anta att vi vill lägga till variabel \\(X^*\\) till en modell som har \\(k\\) andra variabler, då ser beräkningen ut som följer:\n\\[\n\\begin{aligned}\nSS(X^*|X_1, \\ldots, X_k) &= SSE_{X_1, \\ldots, X_k} - SSE_{X_1, \\ldots, X_k, X^*} = \\\\\n&= SSR_{X_1, \\ldots, X_k, X^*} - SSR_{X_1, \\ldots, X_k}\n\\end{aligned}\n\\tag{5.2}\\]\nNotera att SSR ökar för varje ytterligare variabel som läggs till i modellen, medan SSE alltid minskar. En variation måste alltid vara positiv, därav beräknas \\(SSE_{reducerad} - SSE_{komplett}\\) eller \\(SSR_{komplett} - SSR_{reducerad}\\).\nSekventiella kvadratsummor påverkas av ordningen variablerna läggs till i modellen. Låt oss byta ordning på de förklarande variablerna när vi anpassar modellen:\n\n\nVisa kod\nmodel &lt;- lm(formula = bill_length_mm ~ sex + ., data = modelData)\n\nanova(model) %&gt;% \n  round(4) %&gt;% \n  kable() %&gt;% \n  kable_styling(\"striped\")\n\n\n\n\nTabell 5.3: Annan ordning på modellernas variabler\n\n\n\n\n \n  \n     \n    Df \n    Sum Sq \n    Mean Sq \n    F value \n    Pr(&gt;F) \n  \n \n\n  \n    sex \n    1 \n    1175.4780 \n    1175.4780 \n    239.1017 \n    0.0000 \n  \n  \n    species \n    2 \n    6975.5916 \n    3487.7958 \n    709.4457 \n    0.0000 \n  \n  \n    bill_depth_mm \n    1 \n    64.4987 \n    64.4987 \n    13.1196 \n    0.0003 \n  \n  \n    flipper_length_mm \n    1 \n    78.3815 \n    78.3815 \n    15.9434 \n    0.0001 \n  \n  \n    body_mass_g \n    1 \n    32.2629 \n    32.2629 \n    6.5625 \n    0.0109 \n  \n  \n    Residuals \n    326 \n    1602.6899 \n    4.9162 \n     \n     \n  \n\n\n\n\n\n\n\n\nI Tabell 5.3 ser vi att \\(SS(\\text{sex}) = 1175.478\\) vilket är betydligt högre än \\(SS(\\text{sex}|\\text{species}, \\text{bill\\_depth\\_mm}, \\text{flipper\\_length\\_mm}, \\text{body\\_mass\\_g}) = 133.7191\\) från Tabell 5.2. Variabeln kön bidrar med mycket variation när den är ensam i en modell, men när den läggs till i en modell som redan har andra variabler bidrar den inte med lika mycket unik information. Detta betyder att den förklarade variationen som variabeln bidrar med verkar finnas i övriga variabler också. Denna iakttagelse kommer vi komma tillbaka till i ett senare kapitel.\nNågonting som är lika i de två tabellerna är SSE. Vi har i båda modellerna inkluderad samma variabler vilket innebör att SST, SSR, och SSE överlag är densamma. Summan av alla sekventiella kvadratsummor ska fortfarande bli SSR oavsett ordningen på variablerna och på grund av den additiva egenskapen hos variationen har SST och SSE inte heller förändrats.",
    "crumbs": [
      "Regressionsanalys",
      "**DEL II - Regressionsanalys**",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Statistisk inferens</span>"
    ]
  },
  {
    "objectID": "01-regression/04-statistical-inference.html#sec-stat-inference",
    "href": "01-regression/04-statistical-inference.html#sec-stat-inference",
    "title": "5  Statistisk inferens",
    "section": "5.2 Statistisk inferens",
    "text": "5.2 Statistisk inferens\nMed hjälp av de olika källorna av variation kan vi beräkna tester för hela eller delar av modellen i olika F-test, medan de enskilda parameterskattningarna och dess tillhörande medelfel kan användas i tester för enskilda lutningsparametrar.\n\n5.2.1 F-test för modellen\nI en multipel linjär regression är ett F-test för hela modellen bra att börja med för att se ifall minst en lutningsparameter är signifikant. Vi undersöker hypoteserna:\n\\[\\begin{align*}\nH_0&: \\beta_1 = \\beta_2 = \\beta_3 = \\cdots = \\beta_k = 0\\\\\nH_a&: \\text{Minst en av } \\beta_j \\text{ i } H_0 \\text{ är skild från } 0\n\\end{align*}\\]\nOm minst en lutningsparameter är signifikant betyder det att det finns åtminstone en variabel som bidrar med förklarad variation, att modellen är bättre än att använda enbart \\(\\bar{Y}\\). Testvariabeln undersöker relationen mellan den förklarande och oförklarande variationen genom dess medelkvadratsummor.\n\\[\nF_{test} = \\frac{SSR / k}{SSE / (n - (k+1))} = \\frac{MSR}{MSE}\n\\]\nTestvariabeln följer en F-fördelning som styrs av två frihetsgrader; \\(df1\\) från täljaren och \\(df2\\) från nämnaren i beräkningen, det vill säga modellens och felets frihetsgrader. Om \\(H_0\\) är sann kommer testvariabeln bli 0, medan om \\(H_a\\) är sann kommer testvariabeln bli ett stort positivt tal. Eftersom båda medelkvadratsummorna är positiva tal innebär det att kvoten alltid kommer vara positiv och vi kan förkasta \\(H_0\\) om testvariabeln befinner sig nog långt från 0.\n\n\nVisa kod\n# Skapar en funktion för att generera olika F-fördelningar\ngenerateFdistribution &lt;- function(df1, df2, n = 1000) {\n  x &lt;- seq(0, 5, length.out = n)  \n  y &lt;- df(x, df1, df2)  \n  tibble(x = x, y = y, df1 = df1, df2 = df2)  \n}\n\n# Skapar en lista med olika frihetsgrader\ndfs &lt;- list(c(5, 30), c(10, 100), c(20, 50), c(30, 300))\n\n# Genererar data\nFdistributions &lt;- dfs %&gt;%\n  purrr::map_df(~generateFdistribution(.x[1], .x[2]), .id = \"Distribution\") %&gt;%\n  mutate(Distribution = paste0(\"df1 = \", df1, \", df2 = \", df2))\n\n# Plot the F-distributions using ggplot2\nggplot(Fdistributions) + \n  aes(x = x, y = y, color = Distribution) +\n  geom_line(linewidth = 1) +\n  labs(\n    x = \"F-värde\",\n    y = \"Densitet\",\n    color = \"Frihetsgrader\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 14, face = \"bold\"),\n    legend.position = \"right\",\n    legend.title = element_text(face = \"bold\")\n  ) +\n  scale_color_manual(values = c(\"steelblue\", \"#d9230f\", \"black\", \"grey50\"))\n\n\n\n\n\nOlika F-fördelningar och deras frihetsgrader\n\n\n\n\nFör att få fram SSR från en ANOVA-tabell i R behöver vi summera de sekventiella kvadratsummorna. Vi kan sedan bearbeta tabellen för att få fram testvariabeln och använda frihetsgraderna för respektive källa i pf(lower.tail = FALSE) för att få fram p-värdet för testet.\n\n\nVisa kod\nanovaTable &lt;- anova(simpleModel)\n\n# Beräknar raden för SSR utifrån alla rader förutom SSE\nSSR &lt;- anovaTable[-nrow(anovaTable),] %&gt;% \n  summarize(across(Df:`Sum Sq`, ~sum(.x))) %&gt;% \n  mutate(`Mean Sq` = `Sum Sq` / Df,\n         `F value` = NA,\n         `Pr(&gt;F)` = NA)\n\n# Kombinerar SSR med SSE från ursprungliga tabellen\nsimpleAnova &lt;- SSR %&gt;% \n  add_row(anovaTable[nrow(anovaTable),]) %&gt;% \n  mutate(\n    `F value` = \n      ifelse(row_number() == 1,\n            `Mean Sq`[1] / `Mean Sq`[2], \n            NA),\n    `Pr(&gt;F)` = \n        ifelse(row_number() == 1, \n              pf(q = `F value`[1], df1 = Df[1], df2 = Df[2], lower.tail = FALSE), \n              NA)\n    )\n\nrownames(simpleAnova) &lt;- c(\"Model\", \"Residuals\")\n\nkable(simpleAnova, digits = 4) %&gt;% \n  kable_styling(\"striped\")\n\n\n\n\nTabell 5.4: Bearbetad och förenklad ANOVA tabell\n\n\n\n\n \n  \n     \n    Df \n    Sum Sq \n    Mean Sq \n    F value \n    Pr(&gt;F) \n  \n \n\n  \n    Model \n    6 \n    8326.213 \n    1387.7021 \n    282.2698 \n    0 \n  \n  \n    Residuals \n    326 \n    1602.690 \n    4.9162 \n     \n     \n  \n\n\n\n\n\n\n\n\nEftersom p-värdet är mindre än 5 procent, kan \\(H_0\\) förkastas och minst en av variablerna har ett samband med responsvariabeln.2\n\n\n5.2.2 Partiella F-test för grupper av parametrar\nIbland är vi intresserade att undersöka delar av modellen, en grupp med lutningsparametrar. Ett sådant fall är om vi vill undersöka en kvalitativ variabels påverkan eftersom den kan ha transformerats till flera indikatorvariabler alla med en tillhörande lutningsparameter. Ett annat tillfälle är om vi vill undersöka om flera variabler tillsammans bidrar med förklarad variation till modellen.\nIstället för att undersöka alla lutningsparametrar undersöks nu ett urval: \\[\\begin{align*}\nH_0&: \\beta_1 = \\beta_2 = \\beta_3 = \\cdots = \\beta_s = 0\\\\\nH_a&: \\text{Minst en av } \\beta_j \\text{ i } H_0 \\text{ är skild från } 0\n\\end{align*}\\] där \\(s\\) är antalet parametrar som undersöks.\nTestvariabeln för ett partiellt F-test kräver en komplett (betecknad \\(_F\\)) och en reducerad modell (betecknad \\(_R\\)). Den kompletta modellen består av alla variabler medan den reducerade modellen utgår från att \\(H_0\\) är sann och variablerna som undersöks har plockats bort från anpassningen. Vi kan välja att antingen använda SSR eller SSE för att beräkna hur mycket förklarad variation som försvinner mellan de två modellerna enligt samma princip som Ekvation 5.2.\n\\[\nF_{test} = \\frac{(SSR_F - SSR_R) / s}{SSE_F / (n - (k+1))} = \\frac{(SSE_R - SSE_F) / s}{SSE_F / (n - (k+1))}\n\\tag{5.3}\\]\nTestvariabeln är fortfarande F-fördelat med \\(s\\) respektive \\(n - (k+1)\\) frihetsgrader.\n\n5.2.2.1 Räkneknep för partiella F-test\nMed hjälp av Ekvation 5.2 kan Ekvation 5.3 formuleras på ett tredje sätt som underlättar vår analysprocess. Vi kan skriva om skillnaden i förklarad variation mellan den kompletta och reducerade modellen som en sekventiell kvadratsumma. Exempelvis kan vi vilja undersöka om variabeln art har ett samband med responsvariabeln. Eftersom den variabeln transformeras till två indikatorvariabler omfattar hypoteserna två lutningsparametrar.\n\\[\\begin{align*}\nH_0&: \\beta_{Chinstrap} = \\beta_{Gentoo} = 0\\\\\nH_a&: \\text{Minst en av } \\beta_j \\text{ i } H_0 \\text{ är skild från } 0\n\\end{align*}\\]\nDen reducerade modellen skapas utifrån att \\(H_0\\) är sann, det vill säga \\(\\beta_{Chinstrap} = \\beta_{Gentoo} = 0\\) och de två modellernas förklarade variation skulle betecknas som: \\[\n\\begin{aligned}\n  SSR_{R} &= SSR_{bill\\_depth\\_mm, flipper\\_length\\_mm, body\\_mass\\_g, sex} \\\\\n  SSR_{F} &= SSR_{bill\\_depth\\_mm, flipper\\_length\\_mm, body\\_mass\\_g, sex, species}\n\\end{aligned}\n\\]\nVi kan omformulera täljaren i Ekvation 5.3 till: \\[\nSS(species|bill\\_depth\\_mm, flipper\\_length\\_mm, body\\_mass\\_g, sex)\n\\] I de ANOVA-tabeller som presenterats tidigare kan vi få fram denna kvadratsumma direkt om art läggs till som den sista variabeln i modellen.\n\n\nVisa kod\nmodel &lt;- lm(bill_length_mm ~ bill_depth_mm + flipper_length_mm + body_mass_g + sex + species, data = modelData)\n\nanova(model) %&gt;% \n  round(4) %&gt;% \n  kable() %&gt;% \n  kable_styling(\"striped\")\n\n\n\n\nTabell 5.5: ANOVA-tabell från en modell där art läggs till sist\n\n\n\n\n \n  \n     \n    Df \n    Sum Sq \n    Mean Sq \n    F value \n    Pr(&gt;F) \n  \n \n\n  \n    bill_depth_mm \n    1 \n    518.9806 \n    518.9806 \n    105.5648 \n    0.0000 \n  \n  \n    flipper_length_mm \n    1 \n    4045.7248 \n    4045.7248 \n    822.9329 \n    0.0000 \n  \n  \n    body_mass_g \n    1 \n    6.1329 \n    6.1329 \n    1.2475 \n    0.2649 \n  \n  \n    sex \n    1 \n    68.4245 \n    68.4245 \n    13.9181 \n    0.0002 \n  \n  \n    species \n    2 \n    3686.9500 \n    1843.4750 \n    374.9776 \n    0.0000 \n  \n  \n    Residuals \n    326 \n    1602.6899 \n    4.9162 \n     \n     \n  \n\n\n\n\n\n\n\n\nEn ANOVA-tabell med sekventiella kvadratsummor beräknar ett partiellt F-test för respektive variabel (och dess parameter/parametrar) som undersöker huruvida variabeln bidrar med en signifikant ökning av den förklarade variationen till en modell som redan inkluderar variablerna ovanför. Tabell 5.5 beräknar nu det partiella F-test för art (\\(F_{test} = 374.9776\\)) som vi var intresserade av och vi kan direkt tolka p-värdet för testet (\\(p-värde &lt; 0.001\\)) som att minst en av lutningsparametrarna är signifikant skild från 0.\nOm vi genomför ett partiellt F-test för flera variabler kan vi inte använda p-värden som anges i tabellen då hypoteserna omfattar fler lutningsparametrar/variabler än vad de sekventiella kvadratsummorna visar. Anta att vi vill undersöka om art och kön tillsammans bidrar något till modellen. Hypotesprövningen skulle då omfatta:\n\\[\n\\begin{aligned}\nH_0&: \\beta_{sexMale} = \\beta_{Chinstrap} = \\beta_{Gentoo} = 0\\\\\nH_a&: \\text{Minst en av } \\beta_j \\text{ i } H_0 \\text{ är skild från } 0\n\\end{aligned}\n\\]\nDen sekventiella kvadratsumman som vi vill använda anges som \\(SS(species, sex|bill\\_depth\\_mm, flipper\\_length\\_mm, body\\_mass\\_g)\\) och vi kan beräkna fram detta värde genom att summera de två variablernas SS från Tabell 5.5.\n\\[\n\\begin{aligned}\nSS(species, sex|bill\\_depth\\_mm, flipper\\_length\\_mm, body\\_mass\\_g) = \\\\\nSS(species|bill\\_depth\\_mm, flipper\\_length\\_mm, body\\_mass\\_g, sex) + \\\\\nSS(sex|bill\\_depth\\_mm, flipper\\_length\\_mm, body\\_mass\\_g)\n\\end{aligned}\n\\] Alternativet är att anpassa två modeller i R, den kompletta och reducerade och läsa av SSE eller summera SSR från respektive ANOVA-tabell.\n\n\n5.2.2.2 Partiellt F-test för specifika värden\nVi kan ställa upp en generell modell som: \\[\\begin{align*}\n  Y = \\beta_0 + \\beta_1 \\cdot X_1 + \\beta_2 \\cdot X_2 + \\beta_3 \\cdot X_3 +\\beta_4 \\cdot X_4 +\\beta_5 \\cdot X_5+ E\n\\end{align*}\\]\nOm vi ska undersöka specifika parametrars värden (som inte är 0) kan vi genomföra följande härledning. Anta \\(H_0:\\) \\(\\beta_2=4\\) och \\(\\beta_5 = -2\\) som ska undersökas med ett test.\n\\[\\begin{align*}\nY &= \\beta_0 + \\beta_1 \\cdot X_1 + 4 \\cdot X_2 + \\beta_3 \\cdot X_3 +\\beta_4 \\cdot X_4 - 2 \\cdot X_5+ E\\\\\nY - 4 \\cdot X_2 + 2 \\cdot X_5 &= \\beta_0 + \\beta_1 \\cdot X_1  + \\beta_3 \\cdot X_3 +\\beta_4 \\cdot X_4 + E \\\\\nY^* &= \\beta_0 + \\beta_1 \\cdot X_1  + \\beta_3 \\cdot X_3 +\\beta_4 \\cdot X_4 + E\n\\end{align*}\\]\n\\(Y^*\\) kan anses vara en reducerad modell för ett F-test. I R kan detta inte lösas genom anova() utan måste beräknas ‘’för hand’’ genom att anpassa två modeller, den kompletta och den reducerade.\n\n\n\n5.2.3 t-test för enskilda parametrar\nAtt använda ANOVA-tabellen för att undersöka enskilda parametrar är inte lämpligt då det kräver att variabeln anges sist i modelleringen för att det partiella F-testet undersöker just den enskilda variabeln i relation till övriga modellen. Istället bör vi använda t-test för respektive parameter.\nFormellt undersöks hypoteserna: \\[\n\\begin{aligned}\n  H_0&: \\beta_j = 0\\\\\n  H_a&: \\beta_j \\ne 0\n\\end{aligned}\n\\] där \\(j\\) är någon av lutningsparametrarna i en anpassad modell.\nTestvariabeln beräknas utifrån den skattade lutningsparametern och dess medelfel: \\[\n\\begin{aligned}\nt_{test} = \\frac{b_j - 0}{s_{b_j}}\n\\end{aligned}\n\\]\nTestvariabeln är t-fördelad givet \\(H_0\\) med \\(n-(k+1)\\) frihetsgrader.\nI R används t-test i koefficienttabellen som vi kan plocka ut ur summary()-objektet genom coef().\n\n\nVisa kod\nsummary(simpleModel) %&gt;% \n  coef() %&gt;% \n  round(4) %&gt;% \n  kable(format = \"markdown\",\n        col.names = c(\"Variabel\", \"Skattning\", \"Medelfel\", \"t-värde\", \"p-värde\"), \n        parse = TRUE) %&gt;% \n  kable_styling(\"striped\")\n\n\n\n\nTabell 5.6: Koefficienttabell för en modell med tillhörande t-test för enskilda parametrar\n\n\n\n\n \n  \n    Variabel \n    Skattning \n    Medelfel \n    t-värde \n    p-värde \n  \n \n\n  \n    (Intercept) \n    15.0166 \n    4.3742 \n    3.4330 \n    0.0007 \n  \n  \n    speciesChinstrap \n    9.5655 \n    0.3497 \n    27.3508 \n    0.0000 \n  \n  \n    speciesGentoo \n    6.4044 \n    1.0304 \n    6.2154 \n    0.0000 \n  \n  \n    bill_depth_mm \n    0.3130 \n    0.1541 \n    2.0316 \n    0.0430 \n  \n  \n    flipper_length_mm \n    0.0686 \n    0.0232 \n    2.9608 \n    0.0033 \n  \n  \n    body_mass_g \n    0.0011 \n    0.0004 \n    2.5617 \n    0.0109 \n  \n  \n    sexmale \n    2.0297 \n    0.3892 \n    5.2153 \n    0.0000 \n  \n\n\n\n\n\n\n\n\nI Tabell 5.6 ser vi att p-värdet för alla t-testen är väldigt låga (nära 0). För varje enskilda hypotesprövning kan vi på fem procents signifikans förkasta \\(H_0\\) vilket betyder att variabeln har en signifikant påverkan på responsvariabeln.\n\n\n\n\n\n\nViktigt\n\n\n\nOm en parameter inte anses signifikant är det en motivering till att variabeln kan plockas bort, vi anpassar en reducerad modell och en ny analys påbörjas. Om en variabel plockas bort kommer de övriga parameterskattningarna förändras och tolkningar samt inferens behöver uppdateras.\n\n\n\n\n5.2.4 Konfidensintervall för \\(\\beta\\)\nSlutsatsen vi kan dra från dessa hypotesprövningar är att modellen innehåller variabler som alla har ett signifikant samband med responsvariabeln. Om vi vill tolka magnituden av effekten gentemot populationen, inte bara om sambandet är signifikant, behöver vi beräkna intervallskattningar.\n\\[\n\\begin{aligned}\nb_j \\pm t_{n - (k+1); 1- \\alpha/2} \\cdot s_{b_j}\n\\end{aligned}\n\\]",
    "crumbs": [
      "Regressionsanalys",
      "**DEL II - Regressionsanalys**",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Statistisk inferens</span>"
    ]
  },
  {
    "objectID": "01-regression/04-statistical-inference.html#sec-r-square",
    "href": "01-regression/04-statistical-inference.html#sec-r-square",
    "title": "5  Statistisk inferens",
    "section": "5.3 Enkla utvärderingsmått",
    "text": "5.3 Enkla utvärderingsmått\nBara för att en modell är lämplig, uppfyller modellantaganden och innehåller signifikanta parametrar, betyder det inte att modellen är den bästa som kan skapas eller överhuvudtaget bra. Med hjälp av olika utvärderingsmått kan vi få en överblick på hur bra modellen är.\nFörklaringsgraden (\\(R^2\\)) beskriver hur stor andel av den totala variationen som förklaras av modellens förklarande variabler. Med denna beskrivning kan vi beräkna \\(R^2\\) som: \\[\n\\begin{aligned}\n  R^2 = \\frac{SSR}{SST} = 1 - \\frac{SSE}{SST}\n\\end{aligned}\n\\] På grund av att SSR alltid blir större ju fler variabler som en modell innehåller, behöver vi justera måttet för att kunna jämföra modeller av olika storlekar. Istället bör vi titta på den justerade förklaringsgraden (\\(R^2_{a}\\)) för att se vilken modell som är bäst. En förbättrad \\(R^2_{a}\\) betyder att modellen har tagit bort onödig komplexitet.\n\\[\n\\begin{aligned}\n  R^2_a = 1 - \\frac{SSE / (n - (k+1))}{SST / (n - 1)}\n\\end{aligned}\n\\]",
    "crumbs": [
      "Regressionsanalys",
      "**DEL II - Regressionsanalys**",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Statistisk inferens</span>"
    ]
  },
  {
    "objectID": "01-regression/04-statistical-inference.html#sec-predictions",
    "href": "01-regression/04-statistical-inference.html#sec-predictions",
    "title": "5  Statistisk inferens",
    "section": "5.4 Prediktioner",
    "text": "5.4 Prediktioner\nPrediktioner innebär att vi skattar värdet på Y givet observerade värden på X med hjälp av den anpassade regressionslinjen. Dessa prediktioner kommer falla längsmed linjen vilket ytterligare motiverar att modellen behöver vara lämplig och bra. Vi vill inte att en prediktion i ett område är mer träffsäker än en prediktion i en annan eller att regressionslinjen generellt är en dålig representation av responsvariabeln.\nModellen utgår från en specifik definitionsmängd, de observerade värdena på \\(\\mathbf{X}\\), och det är även inom denna mängd som prediktioner bör göras. Det finns vissa tillfällen, till exempel inom tidsserieanalys, där prediktioner görs utanför definitionsmängden men där finns ett beroende i tiden som möjliggör dessa extrapoleringar. Inom “vanlig” regression bör vi undvika att extrapolera regressionslinjen utanför definitionsmängden.\n\n5.4.1 Medelvärdet av Y för givna \\(\\mathbf{X}\\)\nOm vi är intresserad av det genomsnittliga värdet på responsvariabeln för alla nya observationer med givna värden på \\(\\mathbf{X}\\) kan vi skatta \\(\\mu_{Y|{\\mathbf{X}_0}}\\) där \\(\\mathbf{X}_0\\) innehåller värden för den nya observationen.\n\\[\n\\mathbf{X}_0 = \\begin{bmatrix}\n    1 \\\\\n    X_{1,0}\\\\\n    \\vdots\\\\\n    X_{k,0}\n    \\end{bmatrix}\n\\] Vi utgår från den anpassade regressionsmodellen och beräknar en punktprediktion av responsvariabeln enligt: \\[\n\\hat{Y}_{\\mathbf{X}_0} = \\mathbf{X}_0'\\boldsymbol{\\hat{\\beta}}\n\\]\nMedelfelet för skattningen tar hänsyn till:\n\\[\ns^2_{\\hat{Y}_{\\mathbf{X}_0}} = \\mathbf{X}_0'\\mathbf{s}^2_{\\boldsymbol{\\hat{\\beta}}}\\mathbf{X}_0\n\\]\nIntervallskattningen för ett genomsnitt blir ett konfidensintervall.\n\n\n5.4.2 Enskild prediktion av Y för givna \\(\\mathbf{X}\\)\nOm vi istället är intresserad av ett enskilt värde på Y med givna värden på \\(\\mathbf{X}\\), kan vi skatta \\(Y_{\\mathbf{X}_0}\\).\n\\[\ns^2_{pred} = MSE + s^2_{\\hat{Y}_{\\mathbf{X}_0}}\n\\] Intervallskattningen för ett värde av Y blir ett prediktionsintervall.",
    "crumbs": [
      "Regressionsanalys",
      "**DEL II - Regressionsanalys**",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Statistisk inferens</span>"
    ]
  },
  {
    "objectID": "01-regression/04-statistical-inference.html#footnotes",
    "href": "01-regression/04-statistical-inference.html#footnotes",
    "title": "5  Statistisk inferens",
    "section": "",
    "text": "Frihetsgrader beskriver egentligen hur många bitar oberoende information som finns för en beräkning. Tänk tillbaka på beräkningen av en stickprovsstandardavvikelse vars frihetsgrader är \\(n - 1\\), antalet observationer - 1, för att vi skattar medelvärdet när vi beräknar standardavvikelsen.↩︎\nOm vi hade tagit ett annat beslut (att inte förkasta nollhypotesen) hade det inte varit relevant att fortsätta med analysen, eller åtminstone att fokusera resterande analys på att undersöka varför en multipel linjär regressionsmodell som vi förväntar har ett samband utifrån parvisa spridningsdiagram inte visar på det tillsammans.↩︎",
    "crumbs": [
      "Regressionsanalys",
      "**DEL II - Regressionsanalys**",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Statistisk inferens</span>"
    ]
  },
  {
    "objectID": "01-regression/05-simultaneous-inference.html",
    "href": "01-regression/05-simultaneous-inference.html",
    "title": "6  Simultan inferens",
    "section": "",
    "text": "6.1 Simultan inferens\nNär en modell innehåller flera förklarande variabler, alla med minst en tillhörande lutningsparameter, kommer vi genomföra flera inferensberäkningar om vi vill dra slutsatser om flera parametrar eller intervallskatta prediktioner för flera nya observationer. Varje hypotesprövning eller intervallskattning som beräknas utgår från att vi alltid har en risk att fatta fel beslut, vilket innebär att denna risk inflateras ju fler parametrar eller värden som undersöks med enskilda beräkningar. Typ I felet (signifikansnivån) beskriver risken att förkasta en sann \\(H_0\\).\nDen satiriska vetenskapliga serietecknaren xkcd har publicerat en serie som berör signifikansen av hypotesprövningar.\nEn grupp forskare, som hellre vill spela Minecraft, har fått i uppdrag att undersöka effekten av Jelly Beans på förekomsten av acne. Forskarna undersöker 20 olika färger av Jelly Beans med en signifikansnivå på fem procent. En utav färgerna visade sig ha en signifikant påverkan på förekomsten av acne och som vi ser i nyhetsartikeln allra sist i serien blir denna upptäckt innehållet på första sidan.\nVad är det som faktiskt har hänt här och är det korrekt att annonsera ut detta för världen? Vi kan skapa ett liknande simulerat exempel genom att genomföra 20 stycken urval från en population där vi vet det sanna medelvärdet.",
    "crumbs": [
      "Regressionsanalys",
      "**DEL II - Regressionsanalys**",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Simultan inferens</span>"
    ]
  },
  {
    "objectID": "01-regression/05-simultaneous-inference.html#simultan-inferens",
    "href": "01-regression/05-simultaneous-inference.html#simultan-inferens",
    "title": "6  Simultan inferens",
    "section": "",
    "text": "Figur 6.1: xkcd, CC BY-NC 2.5\n\n\n\n\n\n\n6.1.1 Simulering\n\n\nVisa kod\n## Anger ett seed\nset.seed(12345)\n\n## Simulerar ett datamaterial från en given population med sanna värden\ndata &lt;- \n  replicate(\n    n = 20,\n    expr = rnorm(n = 30, mean = 0, sd = 1)\n  )\n\n\nObjektet data innehåller nu 20 stycken kolumner med 30 observationer i varje och varje kolumn är ett nytt urval (motsvarande en undersökning av en färg Jelly Bean). Alla dessa urval kommer från en population där \\(\\mu = 0\\) och det skulle vi kunna undersöka genom ett enkelt t-test för ett medelvärde. Formellt undersöks hypoteserna:\n\\[\n\\begin{aligned}\n  H_0 &: \\mu = 0\\\\\n  H_A &: \\mu \\ne 0\n\\end{aligned}\n\\]\ndär testvariabeln är \\[\nt_{test} = \\frac{\\bar{x} - 0}{ \\frac{s}{\\sqrt{n}}}\n\\]\n\n\nVisa kod\n## Testvariabeln för första urvalet\ntTest &lt;- (mean(data[,1]) - 0) / (sd(data[,1]) / sqrt(nrow(data))) \n\n\np-värdet för testet kan beräknas utifrån t-fördelningen med \\(n - 1\\) frihetsgrader. Vi måste dock ta hänsyn till att vi undersöker en dubbelsidig mothypotes vilket innebär att p-värdet är både större än den positiva testvariabeln och mindre än den negativa testvariabeln. Med hjälp av symmetrin i t-fördelningen kan vi räkna ut detta genom att beräkna absolutbeloppet av testvariabeln och multiplicera ytan större än detta värde med \\(2\\).\n\n\nVisa kod\n## Beräkning av p-värdet\npValue &lt;- 2 * pt(q = abs(tTest), df = nrow(data) - 1, lower.tail = FALSE) \n\n\nVi kan också använda inbyggda funktionen t.test() för att få fram samma resultat, där standardargumentet är att vi undersöker om populationsmedelvärdet är skilt från 0.\n\n\nVisa kod\nt.test(data[,1])\n\n\n\n    One Sample t-test\n\ndata:  data[, 1]\nt = 0.46006, df = 29, p-value = 0.6489\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n -0.2715323  0.4291463\nsample estimates:\n mean of x \n0.07880701 \n\n\nVi kan replikera detta test och spara p-värdet för alla 20 olika utval med hjälp av apply(). Funktionen genomför en angiven funktion för varje element i ett objekt, i detta fall anger vi att den ska genomföra beräkningen för varje kolumn (MARGIN = 2) i objektet data.\n\n\nVisa kod\n## Kör samma funktion (t.test) för varje kolumn (MARGIN = 2) av datamaterialet\npValues &lt;- \n  apply(\n    X = data,\n    MARGIN = 2,\n    FUN = function(x){\n      t.test(x)$p.value\n    }\n  )\n\n\n\n\n\n\nTabell 6.1: p-värden för t-testet i respektive urval\n\n\n\n\n \n  \n    p-värde \n  \n \n\n  \n    0.649 \n  \n  \n    0.195 \n  \n  \n    0.036 \n  \n  \n    0.895 \n  \n  \n    0.806 \n  \n  \n    0.978 \n  \n  \n    0.341 \n  \n  \n    0.765 \n  \n  \n    0.981 \n  \n  \n    0.269 \n  \n  \n    0.092 \n  \n  \n    0.497 \n  \n  \n    0.228 \n  \n  \n    0.467 \n  \n  \n    0.114 \n  \n  \n    0.333 \n  \n  \n    0.268 \n  \n  \n    0.790 \n  \n  \n    0.290 \n  \n  \n    0.896 \n  \n\n\n\n\n\n\n\n\nI Tabell 6.1 är det endast ett test vars tillhörande p-värde är mindre än \\(\\alpha = 0.05\\) (urval 3), motsvarande den gröna Jelly Bean. Eftersom vi skapat dessa urval från en sann fördelning, vet vi att populationen har medelvärde 0 vilket innebär att vi gör ett fel av typ I, förkastar en sann \\(H_0\\). Med en signifikansnivå på 5 procent räknar vi att i 1 av 20 fall (5%) så fattar vi fel beslut utav ren slump. Desto fler tester som genomförs desto större risk att minst en av dessa tester kommer generera ett typ I-fel och att tolka ett enskilt test med 5 procents signifikans är missvisande.\nUnder antagandet att de olika urvalen är oberoende av varandra kan vi räkna ut den slutgiltiga risken för typ-I fel i minst en av testerna med hjälp av multiplikationssatsen för oberoende händelser:\n\\[\n\\begin{aligned}\n1 - (0.95 \\cdot 0.95 \\cdot  0.95 \\cdot  0.95 \\cdot  0.95 \\cdot  0.95 \\cdot  0.95 \\cdot  0.95 \\cdot  0.95 &\\cdot  0.95 \\cdot  0.95 \\cdot  0.95 \\cdot  0.95 \\cdot  0.95 \\cdot  0.95 \\cdot  0.95 \\cdot  0.95 \\cdot  0.95 \\cdot  0.95 \\cdot  0.95)\\\\\n&1 - (0.95^{20})\\\\\n&0.6415141\n\\end{aligned}\n\\] Risken att minst en utav 20 tester har förkastat en sann \\(H_0\\) är istället ca 64% istället för 5%.\n\n\n6.1.2 Familjekonfidens\nNär vi genomför flera tester kan vi istället justera konfidensgraden för varje individuella test så att vi får en sammanfattande konfidensnivå som vi också använder i tolkningen. Den enklaste varianten är att beräkna Bonferronis familjekonfidens där konfidensgraden beräknas enligt: \\[\n\\begin{aligned}\n  1 - \\alpha/2 \\Rightarrow 1 - \\alpha/(2 \\cdot g)\n\\end{aligned}\n\\] där \\(g\\) är antalet tester som genomförs.\n\n\nVisa kod\n# Plockar ur testvariabeln\nstatistic &lt;- \n  apply(\n    X = data,\n    MARGIN = 2,\n    FUN = function(x){\n      t.test(x)$statistic\n    }\n  )\n\ntibble(`Testvariabel` = statistic) %&gt;% \n  kable(digits = 3) %&gt;% \n  kable_styling(\"striped\", full_width = FALSE) \n\n\n\n\nTabell 6.2: Beräknad testvariabel för respektive urval\n\n\n\n\n \n  \n    Testvariabel \n  \n \n\n  \n    0.460 \n  \n  \n    1.328 \n  \n  \n    2.196 \n  \n  \n    0.134 \n  \n  \n    -0.248 \n  \n  \n    0.028 \n  \n  \n    0.969 \n  \n  \n    0.302 \n  \n  \n    0.024 \n  \n  \n    -1.127 \n  \n  \n    1.744 \n  \n  \n    0.689 \n  \n  \n    1.231 \n  \n  \n    -0.737 \n  \n  \n    1.629 \n  \n  \n    -0.984 \n  \n  \n    -1.129 \n  \n  \n    0.268 \n  \n  \n    1.077 \n  \n  \n    0.132 \n  \n\n\n\n\n\n\n\n\nDet justerade kritiska tabellvärdet är \\(\\pm t_{30 - 1, 1 - 0.05 / (2 \\cdot 20)} = \\pm\\) 3.31 istället för \\(\\pm t_{30 - 1, 1 - 0.05 / (2)} = \\pm\\) 2.045 vilket innebär att ingen testvariabel i Tabell 6.2 ligger extremare än det justerade. Med en simultan signifikansnivå på fem procent förkastas ingen av testernas nollhypoteser.\n\n\n6.1.3 Familjekonfidens för lutningsparametrar\nMed hjälp av Bonferroni:s familjekonfidens kan vi justera formlerna för intervallskattning av lutningsparametrarna. En generell formel för en intervallskattning kan skrivas såsom: \\[\n\\text{punktskattning} \\pm \\text{tabellvärde} \\cdot \\text{medelfel}\n\\tag{6.1}\\]\nI fallet för \\(\\beta_1\\) blir formeln: \\[\n    b_1 \\pm t_{n - (k+1); 1- \\alpha/2} \\cdot s_{b_1}\n\\] Bonferroni:s familjekonfidens leder till att formeln justeras enligt: \\[\n    b_1 \\pm t_{n - (k+1); 1- \\alpha/(2\\cdot g)} \\cdot s_{b_1}\n\\] , där \\(g\\) är antalet lutningsparametrar som ska undersökas samtidigt.\n\\(t_{n-(k+1); 1 - \\alpha/(2\\cdot g)}\\) brukar också anges som \\(B\\). När \\(g\\) är stort, kommer värden från \\(t\\)-fördelningen också vara stora vilket i slutändan kan leda till intervall som inte ger någon vettig information. Därför följer att Bonferronis metod är lämpligast att användas när antalet intervall eller tester är få.",
    "crumbs": [
      "Regressionsanalys",
      "**DEL II - Regressionsanalys**",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Simultan inferens</span>"
    ]
  },
  {
    "objectID": "01-regression/05-simultaneous-inference.html#intervallskattning-av-flera-prediktioner",
    "href": "01-regression/05-simultaneous-inference.html#intervallskattning-av-flera-prediktioner",
    "title": "6  Simultan inferens",
    "section": "6.2 Intervallskattning av flera prediktioner",
    "text": "6.2 Intervallskattning av flera prediktioner\nOm vi är intresserade av att prediktera flera nya observationer kommer vi behöva använda simultan inferens för att samtidigt dra slutsatser om de alla.\n\n6.2.1 Skattning av flera konfidensintervall\nOm vi vill beräkna konfidensband för hela regressionslinjen, i.e. \\(\\mu_Y\\) för alla punkter av \\(\\mathbf{X}\\), kan Working-Hotelling:s metod skapa denna region. Strukturen från formel Ekvation 6.1 ger då följande formel:\n\\[\n\\begin{aligned}\n  \\hat{Y}_{\\mathbf{X}_0} \\pm W \\cdot s_{\\hat{Y}_{\\mathbf{X}_0}}\n\\end{aligned}\n\\tag{6.2}\\]\ndär \\(W^2 = 2\\cdot F_{k+1; n-(k+1); 1-\\alpha}\\).\nDenna beräkning tar redan hänsyn till alla möjliga punkter av \\(\\mathbf{X}\\) som regressionslinjen täcker, vilket innebär att samma formler kan användas för enstaka nya observationer, \\(\\mathbf{X}_{\\mathbf{X}_0}\\).\nÄven för prediktioner av responsvariabeln kan Bonferroni användas enligt: \\[\n  \\hat{Y}_{\\mathbf{X}_0} \\pm B \\cdot s_{\\hat{Y}_{\\mathbf{X}_0}}\n\\tag{6.3}\\] där \\(B = t_{n-(k+1); 1 - \\alpha/(2\\cdot g)}\\)\nEkvation 6.2 ger generellt smalare intervall än Ekvation 6.3 då \\(W\\) är samma värde oavsett hur många intervall som skattas medan \\(B\\) ökar med antalet \\(g\\).\n\n\n6.2.2 Skattning av flera prediktionsintervall\nFör enstaka nya observationer beräknas istället prediktionsintervall vilket innebär att skattningens medelfel förändras. Metoder för att beräkna \\(g\\) prediktionsintervall är Scheffé och Bonferronis metoder.\nScheffés metod: \\[\\begin{align}\n   \\hat{Y}_{\\mathbf{X}_0} \\pm S \\cdot s_{pred}\n\\end{align}\\] där \\[\\begin{align*}\n    S &= g \\cdot F_{g; n-k+1; 1 - \\alpha}\\\\\n    s_{pred} &= \\sqrt{MSE + s^2_{\\hat{Y}_{\\mathbf{X}_0}}}\n\\end{align*}\\]\nBonferronis metod: \\[\\begin{align}\n    \\hat{Y}_{\\mathbf{X}_0} \\pm B \\cdot s_{pred}\n\\end{align}\\] där \\[\\begin{align*}\n    B = t_{n-k+1; 1 - \\alpha/(2\\cdot g)}\n\\end{align*}\\]\nBåda dessa metoder kommer skapa bredare intervall i relation till antalet intervall som skattas eftersom \\(g\\) inkluderas i båda beräkningarna. Detta skiljer sig från Ekvation 6.2 där \\(W\\) inte blir större desto fler intervall.",
    "crumbs": [
      "Regressionsanalys",
      "**DEL II - Regressionsanalys**",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Simultan inferens</span>"
    ]
  },
  {
    "objectID": "01-regression/06-complex-predictors.html",
    "href": "01-regression/06-complex-predictors.html",
    "title": "7  Komplexa samband",
    "section": "",
    "text": "7.1 Interaktioner\nIbland kan en kombination av flera variabler förändra effekten som variablerna har enskilt med en responsvariabel. Till exempel kan temperaturens effekt på elkonsumtionen (länk till datamaterialet ) i en bostad påverkas av förekomsten av bergvärme.\nLäs in materialet och filtera bort saknade värden med:\npower &lt;- read.csv2(file = \"electricityconsumption.csv\", dec = \".\") %&gt;% \n  dplyr::mutate(Bergsvärme = Bergsvärme %&gt;% as_factor()) %&gt;% \n  dplyr::filter(!(Bergsvärme %&gt;% is.na() | Dygnsmedel %&gt;% is.na()))\nVisa kod\nggplot(power) + \n  aes(x = Dygnsmedel, y = Energi_KWh, color = Bergsvärme) + \n  geom_point() + \n  theme_bw() + \n  scale_color_manual(values = c(\"steelblue\", \"#d9230f\"), \n                     labels = c(\"Ej installerad\", \"Installerad\")) +\n  labs(x = \"Temperatur\", y = \"Elkonsumtion (kWh)\", caption = \"Källa: Insamlad data från en bostad i Norrbotten.\")\n\n\n\n\n\n\n\n\nFigur 7.1: Sambandet mellan temperatur och elkonsumtion uppdelat på förekomsten av bergsvärme\nFigur 7.1 visar två olika samband mellan temperatur och elkonsumtion beroende på om observationen (tidpunkten) mätte sambandet när bergsvärme var installerad i bostaden eller ej. När bergsvärme finns verkar påverkan av temperatur vara mindre (\\(\\beta = -1.432\\)) än när bergsvärme saknas (\\(\\beta = -3.626\\)), förändringen i elkonsumtion för varje ökad grad av temperatur har förändrats.\nFör att modellen ska plocka upp variablernas synergi med varandra skapas en interaktion som en produkt av, i det enklaste fallet, de två variablerna och läggs till i modellen med en ytterligare tillhörande lutningsparameter.",
    "crumbs": [
      "Regressionsanalys",
      "**DEL II - Regressionsanalys**",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Komplexa samband</span>"
    ]
  },
  {
    "objectID": "01-regression/06-complex-predictors.html#sec-interaction",
    "href": "01-regression/06-complex-predictors.html#sec-interaction",
    "title": "7  Komplexa samband",
    "section": "",
    "text": "7.1.1 Interaktion mellan en kvalitativ och en kvantitativ variabel\nEn utav de enklare formerna av en interaktion är mellan en kvalitativ och en kvantitativ variabel, som vi såg ett exempel på i Figur 7.1. Om vi ser ett sådant fenomen i ett spridningsdiagram tyder det på att de två variablerna interagerar med varandra för att skapa olika lutningsparametrar.\n\\[\n\\begin{aligned}\n  Y_i = \\beta_0 + \\beta_1 \\cdot \\text{temperatur}_i + \\beta_2 \\cdot \\text{bergsvärme}_i + \\beta_3 \\cdot \\text{temperatur}_i \\cdot \\text{bergsvärme}_i + E_i\n\\end{aligned}\n\\] där \\(\\text{bergsvärme}_i\\) är en indikatorvariabel som indikerar 1 om bergsvärme förekommer och 0 annars.\nKodningen av indikatorvariabeln betyder att vi egentligen får två olika möjliga utfall av modellen, en när \\(\\text{bergsvärme}_i = 1\\) och en när \\(\\text{bergsvärme}_i = 0\\).\n\\[\n\\begin{aligned}\n  Y_{i|\\text{bergsvärme} = 1} &= \\beta_0 + \\beta_1 \\cdot \\text{temperatur}_i + \\beta_2 \\cdot 1 + \\beta_3 \\cdot \\text{temperatur}_i \\cdot 1 + E_i \\\\\n  &= \\beta_0 + \\beta_1 \\cdot \\text{temperatur}_i + \\beta_2 + \\beta_3 \\cdot \\text{temperatur}_i + E_i\\\\\n  &= \\underbrace{( \\beta_0 + \\beta_2 )}_{\\beta_0^*} + \\underbrace{(\\beta_1 + \\beta_3)}_{\\beta_1^*} \\cdot \\text{temperatur}_i + E_i\\\\\n  \\\\\n  Y_{i|\\text{bergsvärme} = 0} &= \\beta_0 + \\beta_1 \\cdot \\text{temperatur}_i + \\beta_2 \\cdot 0 + \\beta_3 \\cdot \\text{temperatur}_i \\cdot 0 + E_i\\\\\n  &= \\beta_0 + \\beta_1 \\cdot \\text{temperatur}_i + E_i\n\\end{aligned}\n\\] När \\(\\text{bergsvärme}_i = 1\\) skapas en enkel linjär modell med ett ‘nytt’ intercept \\(\\beta_0^*\\) och en ny lutningsparameter \\(\\beta_1^*\\). \\(\\beta_2\\) och \\(\\beta_3\\) är alltså den kategoriska variabelns påverkan på regressionslinjens intercept respektive lutning. Vi kan generellt säga att med flera kategorier (indikatorvariabler) skapas flera varianter av modellen med en referenslinje när alla indikatorer är 0, och en linje för varje indikator som blir 1 där interceptet och lutningen förändras jämfört med referenslinjen.\nFör att skapa en interaktion i modellen med hjälp av R kan symbolen * användas:\n\nmodel &lt;- lm(Energi_KWh ~ Dygnsmedel * Bergsvärme, data = power)\n\nsummary(model) %&gt;% \n  coef() %&gt;% \n  round(3) %&gt;% \n  kable(col.names = c(\"Variabel\", \"Skattning\", \"Medelfel\", \"t-värde\", \"p-värde\")) %&gt;% \n  kable_styling(\"striped\")\n\nKoefficienttabell från en modell med en interaktion\n \n  \n    Variabel \n    Skattning \n    Medelfel \n    t-värde \n    p-värde \n  \n \n\n  \n    (Intercept) \n    80.115 \n    0.328 \n    244.008 \n    0 \n  \n  \n    Dygnsmedel \n    -3.626 \n    0.030 \n    -119.777 \n    0 \n  \n  \n    Bergsvärme1 \n    -45.664 \n    0.375 \n    -121.610 \n    0 \n  \n  \n    Dygnsmedel:Bergsvärme1 \n    2.194 \n    0.035 \n    62.308 \n    0 \n  \n\n\n\n\n\nEftersom bergsvärme anges som en faktorvariabel hanterar R detta korrekt (se Avsnitt 3.1) och hade även gjort det om det fanns flera kategorier. I detta fall skapas endast en ny variabel mellan indikatorvariabeln och den numeriska då det endast fanns två kategorier (en indikatorvariabel). Om fler indikatorvariabler hade skapats från den kvalitativa variabeln, hade fler interaktioner skapats, en för varje kombination av indikatorvariabel och kontinuerlig variabel.\nVi kan tolka dessa skattningar som att om bostaden har bergsvärme installerad, sänks förbrukningen vid 0 grader med ca 45 kWh och temperaturens påverkan på konsumtionen förändras med ca 2.2 kWh mindre per ökad grad1 jämfört med en bostad utan bergsvärme. Vi får alltså två modeller:\n\\[\n\\begin{aligned}\n  \\hat{Y}_{i|\\text{bergsvärme} = 1} &= ( 80.115 + -45.664 ) + (-3.626 + 2.194) \\cdot \\text{temperatur}_i \\\\\n  \\\\\n  \\hat{Y}_{i|\\text{bergsvärme} = 0} &= 80.115 + -3.626 \\cdot \\text{temperatur}_i\n\\end{aligned}\n\\] som vi kan visualisera i det tidigare diagrammet med:\n\n\nVisa kod\nggplot(power) + \n  aes(x = Dygnsmedel, y = Energi_KWh, color = Bergsvärme) + \n  geom_point() + \n  theme_bw() + \n  scale_color_manual(values = c(\"steelblue\", \"#d9230f\"), \n                     labels = c(\"Ej installerad\", \"Installerad\")) +\n  labs(x = \"Temperatur\", y = \"Elkonsumtion (kWh)\", caption = \"Källa: Insamlad data från en bostad i Norrbotten.\") +\n  ## Lägga till regressionslinjer\n  #  Ej installerad (Referenslinjen)\n  geom_abline(\n    intercept = coef(model)[1],\n    slope = coef(model)[2],\n    color = \"black\",\n    linewidth = 1,\n    linetype = 2\n  ) + \n  #  Installerad\n  geom_abline(\n    intercept = coef(model)[1] + coef(model)[3],\n    slope = coef(model)[2] + coef(model)[4],\n    color = \"black\",\n    linewidth = 1\n  )\n\n\n\n\n\nSpridningsdiagram med gruppvisa regressionslinjer\n\n\n\n\n\n7.1.1.1 Simpson’s paradox\n\n\n\n7.1.2 Interaktion mellan två kvantitativa variabler\nEn interaktion mellan två kvantitativa variabler är svårare att visualisera och identifiera. Anta att vi har ett datamaterial som består av två förklarande variabler och en responsvariabel där det sanna sambandet inkluderar en interaktion mellan de förklarande variablerna.\n\n\nVisa kod\n# Antal observationer\nn &lt;- 200\n\nset.seed(64)\n\n## Skapa ett datamaterial\ndata &lt;- \n  tibble(\n    x1 = runif(n = n, min = 0, max = 5),\n    x2 = rnorm(n = n, mean = 0, sd = 3),\n    y = 10 + 1.5*x1 - 1.5*x2 - 3*x1*x2 + rnorm(n = n)\n  )\n\n## Utforskning av materialet\np1 &lt;- ggplot(data) + aes(x = x1, y = y) + geom_point(color = \"steelblue\") + theme_bw()\n\np2 &lt;- ggplot(data) + aes(x = x2, y = y) + geom_point(color = \"steelblue\") + theme_bw()\n\np3 &lt;- ggplot(data) + aes(x = x1, y = x2) + geom_point(color = \"steelblue\") + theme_bw()\n\ncowplot::plot_grid(p1, p2, p3)\n\n\n\n\n\nSpridningsdiagram över de olika variablerna i materialet.\n\n\n\n\nMed detta enkla exempel med endast två variabler och deras interaktion kan vi visualisera materialet med ett fåtal diagram och utläsa att modellen uppvisar någon form av komplext samband som inte de enskilda förklarande variablerna kan modellera. Desto fler förklarande variabler som finns minskar effektiviteten att identifiera interaktioner från enskilda parvisa diagram, vilket innebär att vi oftast identifierar behov av interaktioner utifrån modellanpassningar och residualanalys.\n\n\nVisa kod\ndåligModell &lt;- lm(y ~ x1 + x2, data = data)\n\nresidualPlots(dåligModell)\n\n\n\n\n\nResidualer från den felaktigt formulerade modellen.\n\n\n\n\nLikt när vi tittar på en potentiell interaktion mellan en kvalitativ och kvantitativ variabel visas det tydligt i residualerna mot de anpassade värdena att det saknas något samband i modellen då residualerna inte uppvisar konstant varians. Även normalfördelningsdiagrammen identifierar långa svansar med många värden i extremerna.\n\n\nVisa kod\nbraModell &lt;- lm(y ~ x1 * x2, data = data)\n\nresidualPlots(braModell)\n\n\n\n\n\nResidualerna från en lämplig modell\n\n\n\n\nNu ser residualerna bättre ut och det verkar som att modellen är en korrekt representation av sambandet.\nTill skillnad från Avsnitt 7.1.1 kommer tolkningar av interaktioner mellan två kvantitativa variabler bli betydligt svårare. Modellen kan inte på ett enkelt sätt ‘förenklas’ likt i det kvalitativa fallet men på ett ungefär kan vi säga att lutningsparametern för interaktionen påverkar lutningsparametern för respektive grundvariabel.\n\\[\n\\begin{aligned}\n  Y_i &= \\beta_0 + \\beta_1 \\cdot X_{i1} + \\beta_2 \\cdot X_{i2} + \\beta_3 \\cdot X_{i1} \\cdot X_{i2} + E_i \\\\\n  &\\qquad \\qquad \\qquad \\text{alternativt}  \\\\\n  Y_i &= \\beta_0 + \\beta_1 \\cdot X_{i1} + (\\beta_2 + \\beta_3 \\cdot X_{i1}) \\cdot X_{i2} + E_i\\\\\n  \\\\\n  Y_i &= \\beta_0 + (\\beta_1 + \\beta_3 \\cdot X_{i2}) \\cdot X_{i1} + \\beta_2 \\cdot X_{i2} + E_i\n\\end{aligned}\n\\] Eftersom vi i en multipel linjär regression tolkar parameterskattningar som att den förklarande variabeln förändrar responsvariabeln givet att alla andra variabler är fixa kommer en tolkning av en förändring i en förklarande variabel innebära att Y förändras på två platser. I den första alternativa formuleringen ser vi hur en förändring av \\(X_{i1}\\) leder till att Y förändras både via \\(\\beta_1\\) och hur \\(X_{i2}\\)s samband med Y (\\(\\beta_2\\)) förändras med anledning av \\(\\beta_3\\).\nDet vi kan göra för att tolka en variabels effekt på responsvariabeln är att kombinera alla termer som omfattar variabeln och visualisera dens sammanfattande effekt givet att alla andra variabler är konstanta. För en interaktion mellan två kontinuerliga variabler kan vi fixera olika värden på den ena variabeln och modellera den andra variabelns effekt mot responsvariabeln. Ett vanligt sätt att göra detta är att fixera med hjälp av medelvärdet och en standardavvikelse åt båda hållen.\n\n\nVisa kod\nsummary(braModell) %&gt;% \n  coef() %&gt;% \n  round(3) %&gt;% \n  kable(col.names = c(\"Variabel\", \"Skattning\", \"Medelfel\", \"t-värde\", \"p-värde\")) %&gt;% \n  kable_styling(\"striped\")\n\n\n\n\nTabell 7.1: Koefficienttabell från en modell med en interaktion\n\n\n\n\n \n  \n    Variabel \n    Skattning \n    Medelfel \n    t-värde \n    p-värde \n  \n \n\n  \n    (Intercept) \n    9.960 \n    0.155 \n    64.063 \n    0 \n  \n  \n    x1 \n    1.466 \n    0.052 \n    28.279 \n    0 \n  \n  \n    x2 \n    -1.538 \n    0.053 \n    -29.287 \n    0 \n  \n  \n    x1:x2 \n    -2.981 \n    0.017 \n    -176.049 \n    0 \n  \n\n\n\n\n\n\n\n\n\n\nVisa kod\n# Tar ut skattade koefficienter från modellen med interaktion\nb &lt;- coef(braModell)\nmeanX2 &lt;- mean(data$x2)\nsdX2 &lt;- sd(data$x2)\n\nintData &lt;- \n  tibble(\n    # Skapa olika värden av x1 givet värdemängden\n    x1 = seq(min(data$x1), max(data$x1), by = 0.01),\n    y1 = b[1] + (b[2] + b[4]*(meanX2 - sdX2))*x1 + b[3] * (meanX2 - sdX2),\n    y2 = b[1] + (b[2] + b[4]*(meanX2))*x1 + b[3] * (meanX2),\n    y3 = b[1] + (b[2] + b[4]*(meanX2 + sdX2))*x1 + b[3] * (meanX2 + sdX2)\n  ) %&gt;% \n  pivot_longer(\n    !x1\n  )\n\nggplot(intData) + aes(x = x1, y = value, color = name) + geom_line(linewidth = 1) + \n  scale_color_manual(expression(X[2]), values = c(\"steelblue\", \"#d9230f\", \"black\"), labels = expression(mu - sigma, mu, mu + sigma)) + \n  theme_bw() + labs(x = expression(X[1]), y = \"Y\")\n\n\n\n\n\n\n\n\nFigur 7.2: \\(X_1\\):s effekt på responsvariabeln för ett givet värde på \\(X_2\\)\n\n\n\n\n\n\n\n\n\n\n\nVarning\n\n\n\nOm fördelningen av \\(X_2\\) inte är symmetrisk kan valet av ovanstående fixa värden vara missvisande. Om vi vill visualisera olika värden bör vi fundera över vilka som faktiskt är lämpliga att använda.\n\n\nDet vi kan utläsa från Figur 7.2 är att effekten av \\(X_1\\) är positiv för låga värden av \\(X_2\\) och vänder till negativ för höga värden av \\(X_2\\). I ett enkelt fall som denna går det att visa och till viss del tolka interaktionens effekt på ett någorlunda tydligt sätt, men i en mer komplex modell blir det direkt rörigt. Vi bör istället titta på residualanalyser och annan utvärdering av modellen för att bedöma om interkationen gör att modellen blir bättre. Avvägningen mellan modellens komplexitet och dess träffsäkerhet är än mer viktig att diskutera för att modellen ska uppnå sitt syfte.\n\n\n7.1.3 Identifiera interaktion\nVanligtvis kan vi få ledtrådar om interaktioner i de parvisa sambanden, speciellt om det är en interaktion beskriven i Avsnitt 7.1.1, men ibland är det svårt att direkt se ifall en interaktion behövs. Det är också svårt att utläsa mellan exakt vilka variabler som interaktionen finns. Med hjälp av grupperade spridningsdiagram likt Figur 7.1 (kvalitativa och kvantitativa) eller 3D-diagram (kvantitativa och kvantitativa) kan sambandet mellan par av förklarande variabler och responsvariabeln undersökas. Vi kan också skapa potentiella interaktioner och modellera dessa mot responsvariabeln i ett spridningsdiagram.\n\n\nVisa kod\nggplot(data) + aes(x = x1*x2, y = y) + geom_point(color = \"steelblue\") + \ntheme_bw() + labs(x = \"Interaktion\", y = \"Y\")\n\n\n\n\n\nInteraktionen mellan \\(X_1\\) och \\(X_2\\) och dess samband med Y\n\n\n\n\nDiagrammet visar ett starkt negativt linjärt samband mellan interaktionen och responsvariabeln, vilket antyder att interaktionen har en betydande roll i modelleringen. Med flera variabler kommer antalet interaktioner som skulle kunna skapas öka exponentiellt\nDet går också att utläsa från residualanalysen ifall modellen inte plockar upp något samband med responsvariabeln men även med hjälp av dessa diagram kan det vara svårt att utläsa exakt vad för samband som saknas och vilka variabler som behöver justeras. Det är i detta läge som spridningsdiagram över residualerna uppdelat på de olika förklarande variablerna kan ge en indikation på vad som behöver justeras.\nLåt oss anpassa en felaktig modell utan interaktion och titta på residualerna:\n\n\nVisa kod\nresidualPlots(model = lm(Energi_KWh ~ Dygnsmedel + Bergsvärme, data = power))\n\n\n\n\n\n\n\n\n\nDet är framförallt i diagrammet överst till höger som det syns att modellen saknar att modellera någon form av samband mellan variablerna.\n\n\nVisa kod\nmodelNoInt &lt;- lm(Energi_KWh ~ Dygnsmedel + Bergsvärme, data = power)\n\nvisData &lt;- \n  tibble(\n    Residualer = resid(modelNoInt),\n    X1 = power$Dygnsmedel,\n    X2 = power$Bergsvärme\n  ) \n\nggplot(visData) + aes(x = X1, y = Residualer) + \n  geom_point(color = \"steelblue\") +\n  theme_bw() + labs(x = \"Temperatur\")\n\n\n\n\n\nResidualer mot förklarande variabler\n\n\n\n\nVisa kod\nggplot(visData) + aes(x = X2, y = Residualer) + \n  geom_violin(fill = \"steelblue\") +\n  theme_bw() + labs(x = \"Bergsvärme\")\n\n\n\n\n\nResidualer mot förklarande variabler\n\n\n\n\nOm vi sedan visualiserar residualerna uppdelat på de två förklarande variablerna kan vi utläsa ett korsliknande mönster i den kvantitativa variabeln vilket antyder att vi har gruppvisa samband.\n\n\nVisa kod\nresidualPlots(model = model) \n\n\n\n\n\n\n\n\n\nNär interaktionen lagts till ser residualerna mycket bättre ut, dock inte helt perfekta för just denna modell.\n\n\nVisa kod\nmodelInt &lt;- lm(Energi_KWh ~ Dygnsmedel * Bergsvärme, data = power)\n\nvisData &lt;- \n  tibble(\n    Residualer = resid(modelInt),\n    X1 = power$Dygnsmedel,\n    X2 = power$Bergsvärme\n  ) \n\nggplot(visData) + aes(x = X1, y = Residualer) + \n  geom_point(color = \"steelblue\") +\n  theme_bw() + labs(x = \"Temperatur\")\n\n\n\n\n\nResidualer mot förklarande variabler\n\n\n\n\nResidualerna mot temperatur har nu inte samma korsliknande mönster vilket har plockats upp av interaktionen dock syns ett svagt icke-linjärt mönster och framförallt problem med lika varians.",
    "crumbs": [
      "Regressionsanalys",
      "**DEL II - Regressionsanalys**",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Komplexa samband</span>"
    ]
  },
  {
    "objectID": "01-regression/06-complex-predictors.html#polynom",
    "href": "01-regression/06-complex-predictors.html#polynom",
    "title": "7  Komplexa samband",
    "section": "7.2 Polynom",
    "text": "7.2 Polynom\nVi kan modellera vissa typer av icke-linjära samband mellan x och y genom att genomföra lämpliga transformationer av x, exempelvis polynomtermer. Vi simulerar ett datamaterial för att illustrera detta.\n\n## Simulera data\nset.seed(2323)\n\n# Skapa 100 observationer\nn &lt;- 100\n\n# Slumpa värden mellan -5 och 15 från den likformiga fördelningen\nx &lt;- runif(n = n, min = -5, max = 15)\n\n# Skapa responsvariabeln genom en kvadratisk funktion och lägg till slumpvariation med rnorm()\ny &lt;- 4-1*x+0.2*x^2 + rnorm(n = n)\n\nVi kan visualisera detta datamaterial för att se ett icke-linjärt samband mellan de två variablerna. Eftersom vi utför simulering vet vi också vad de sanna parametrarna för modellen ska vara, vilket vi kan stämma av i senare utskrifter.\n\n\nVisa kod\n## Lägg in de två variablerna i ett datamaterial för ggplot2\ndata &lt;- \n  tibble(\n    X = x,\n    Y = y\n  )\n\nggplot(data) + aes(x = X, y = Y) + \n  geom_point(color = \"steelblue\") + theme_bw()  \n\n\n\n\n\nExempel på icke-linjärt samband mellan X och Y\n\n\n\n\nOm vi testar att först anpassa en “vanlig” linjär regression och utvärderar residualerna kommer vi se att residualantagandet om linjäritet inte uppfylls.\n\n\nVisa kod\nmodel &lt;- lm(Y ~ X, data = data)\n\nresidualPlots(model)\n\n\n\n\n\nResidualdiagram från en linjär modell med ett icke-linjärt samband\n\n\n\n\nDessa residualer ser inte bra ut, egentligen inte för någon utav antaganden, men det är främst residualerna mot de anpassade värdena som visar på det största problemet. Vi ser ett tydligt krökt mönster i punkterna som indikerar på att modellen inte är korrekt strukturerad.\nVi kan anpassa polynom på olika sätt. Ett enkelt sätt att göra är att inkludera en ny variabel i datamaterialet som är en transformation av den förklarande variabeln, till exempel skapa \\(X^2\\) som en ny kolumn.\n\n\nVisa kod\n# Sparar över det gamla materialet\ndata &lt;- \n  data %&gt;% \n  # Skapar en ny variabel som kvadraten av x\n  mutate(\n    X2 = x^2\n  )\n\n# Anpassa en ny modell\nmodel &lt;- lm(Y ~ ., data = data)\n\nsummary(model) %&gt;% \n  coef() %&gt;% \n  round(3) %&gt;% \n  kable(col.names = c(\"Variabel\", \"Skattning\", \"Medelfel\", \"t-värde\", \"p-värde\")) %&gt;% \n  kable_styling(\"striped\")\n\n\n\n\nTabell 7.2: Skattade koefficienter från en modell med polynom\n\n\n\n\n \n  \n    Variabel \n    Skattning \n    Medelfel \n    t-värde \n    p-värde \n  \n \n\n  \n    (Intercept) \n    4.240 \n    0.151 \n    28.103 \n    0 \n  \n  \n    X \n    -1.057 \n    0.042 \n    -24.930 \n    0 \n  \n  \n    X2 \n    0.204 \n    0.004 \n    55.630 \n    0 \n  \n\n\n\n\n\n\n\n\n\n\nVisa kod\nresidualPlots(model)\n\n\n\n\n\nResidualdiagram från en modell med polynom\n\n\n\n\nDenna modell ser ut att uppfylla modellantaganden eftersom vi har lagt till en variabel som tar hänsyn till det icke-linjära samband som X har med Y. Parameterskattningarna som vi får från modellen stämmer också till stor del överens med den sanna modell som vi simulerat materialet ifrån.\n\n7.2.1 Centrering\nNär polynom används är det önskvärt att centrera eller standardisera grundvariablerna som används för polynomen. Detta görs för att minska de värden som modellen använder till sin anpassning (beräkningskomplexitet) och för att variablerna skapas utifrån varandra och har ett starkt beroende mellan sig (multikollinearitet, Avsnitt 8.1). Det finns många problem med starka beroenden mellan förklarande variabler och ett utav dem är att höga kovarianser leder till icke-informativ inferens.\n\n\nVisa kod\n# Skapa centrerad data\ndataCent &lt;- \n  data %&gt;% \n  mutate(\n    # Centrera variabeln x med hjälp av scale()\n    # Standardisering kan göras genom argument scale = TRUE \n    XCent = X %&gt;% scale(center = TRUE, scale = FALSE),\n    X2Cent = XCent^2\n  )\n\n\nVi kan med hjälp av colMeans() se medelvärden för de icke-centrerade och centrerade variablerna. Det finns en stor skillnad, främst för polynomens medelvärde vilket visar på syftet med centrering, att reducera värden som används inom modellanpassningen.\n\n\nVisa kod\ncolMeans(dataCent) %&gt;% \n  round(3) %&gt;% \n  kable(col.names = \"Medelvärde\") %&gt;% \n  kable_styling(full_width = FALSE)\n\n\nVariablernas medelvärden\n \n  \n     \n    Medelvärde \n  \n \n\n  \n    X \n    5.399 \n  \n  \n    Y \n    11.555 \n  \n  \n    X2 \n    63.871 \n  \n  \n    XCent \n    0.000 \n  \n  \n    X2Cent \n    34.727 \n  \n\n\n\n\n\nNär vi anpassar en modell med polynom finns det också ett alternativt sätt att göra det på. Vi har i de tidigare exemplen skapat polynomet som en ny variabel i datamaterialet och kan inkludera den variabeln (X2 eller X2Cent) i modellstrukturen för lm(). Vi behöver egentligen inte skapa en ny variabel, och det brukar vi inte heller göra i praktiken om det ska skapas flera polynom av olika grad. Istället kan vi i formeln ange hur vi vill transformera grundvariabler med hjälp av I(X^2) där exponenten anger graden av polynomet. Om vi inte använder I() runt vår beräkning kommer R inte skapa ett polynom.\n\n\nVisa kod\n# Anpassa ny modell med centrerad x\nmodelCent &lt;- lm(Y ~ XCent + X2Cent, data = dataCent)\n\n# Alternativt \nmodelCent &lt;- lm(Y ~ XCent + I(XCent^2), data = dataCent)\n\nsummary(modelCent) %&gt;% \n  coef() %&gt;% \n  round(3) %&gt;% \n  kable(col.names = c(\"Variabel\", \"Skattning\", \"Medelfel\", \"t-värde\", \"p-värde\")) %&gt;% \n  kable_styling(\"striped\")\n\n\n\n\nTabell 7.3: Skattade koefficienter från en modell med centrerade polynom\n\n\n\n\n \n  \n    Variabel \n    Skattning \n    Medelfel \n    t-värde \n    p-värde \n  \n \n\n  \n    (Intercept) \n    4.475 \n    0.168 \n    26.681 \n    0 \n  \n  \n    XCent \n    1.144 \n    0.019 \n    61.526 \n    0 \n  \n  \n    I(XCent^2) \n    0.204 \n    0.004 \n    55.630 \n    0 \n  \n\n\n\n\n\n\n\n\n\n\nVisa kod\nresidualPlots(modelCent)\n\n\n\n\n\nResidualdiagram från en modell med centrerade polynom\n\n\n\n\nModellen får olika parameterskattningar för grundvariabeln jämfört med den icke-centrerade modellen (Tabell 7.2) eftersom variablerna som används till anpassningen har olika tolkningar. När vi centrerar en variabel tolkas lutningsparametern som när den förklarand variabelns avstånd från sitt medelvärde ökar med ett, förändras y med parameterns värde. Däremot ser vi att parametern för polynomet är densamma samt att residualerna från modellen också är det.\nEn centrering av variabler för polynom ändrar alltså inte hur bra modellen är på att anpassa responsvariabeln men förenklar bakomliggande beräkningar och förändrar tolkningar av parameterskattningar.",
    "crumbs": [
      "Regressionsanalys",
      "**DEL II - Regressionsanalys**",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Komplexa samband</span>"
    ]
  },
  {
    "objectID": "01-regression/06-complex-predictors.html#sec-exercises-complex",
    "href": "01-regression/06-complex-predictors.html#sec-exercises-complex",
    "title": "7  Komplexa samband",
    "section": "7.3 Övningsuppgifter",
    "text": "7.3 Övningsuppgifter\nI detta kapitel introduceras två datamaterial som kommer återkomma i efterföljande kapitel.\n\n7.3.1 Infektionsrisker vid sjukhus\nSENIC står för the Study on the Efficacy of Nosocomial Infection Control (Haley m.fl. 1980) och behandlar olika sätt att identifiera och kontrollera infektioner som uppstår på sjukhus. Datamaterialet består utav ett slumpmässigt urval om 113 sjukhus från de 338 undersökta. Materialet inkluderar följande variabler:\n\nID: Ett ID nummer för respektive sjukhus,\nLength_of_stay: Genomsnittligt antal dagar en patient stannar på sjukhuset,\nAge: Genomsnittlig ålder (år) på en patient,\nInfection_risk: Genomsnittlig uppskattad sannolikhet (i procent) att smittas av en infection på sjukhuset,\nRoutine_culturing_ratio: Kvoten mellan antalet odlingar som genomförts med antalet patienter utan symtom på infektion (multiplicerat med 100),\nRoutine_chest_X_ray_ratio: Kvoten mellan antalet röntgenbilder som genomförts med antalet patienter utan symtom på lunginflammation (multiplicerat med 100),\nNumber_of_beds: Genomsnittlig antal sjukhussängar (platser) vid sjukhuset under undersökningsperioden,\nMedical_school_affiliation: Ja/Nej om sjukhuset är kopplat till en läkarutbildning (1 = Ja, 2 = Nej),\nRegion: Sjukhusets geografiska område (1 = Nordost, 2 = Mellanvästern, 3 = Syd, 4 = Väst),2\nAverage_daily_census: Genomsnittligt antal patienter vid sjukhuset under undersökningsperioden,\nNumber_of_nurses: Genomsnittligt antal heltidsanställda licensierade sjuksköterskor under undersökningsperioden (antal heltids + 0.5 antal deltidsanställda),\nAvailable_facilties_and_services: Andel av 35 möjliga anläggningar och tjänster ett sjukhus kan erjbuda.\n\nDatamaterialet kan laddas ner här.\nEfter att ha laddat ner datamaterialet, skapa en designmatris med variablerna:\n\n\\(X_1 =\\) Length_of_stay\n\\(X_2 =\\) Age\n\\(X_3 =\\) Routine_chest_X_ray_ratio\n\\(X_4 =\\) Medical_shool_affiliation\n\nKoda \\(X_4\\) så att 1 betyder att sjukhuset är kopplat till en läkarutbildning och 0 annars.\n\nVisualisera de parvisa samband mellan de fyra förklarande variablerna och responsvariabeln. Är det något i dessa diagram som sticker ut som motiverar att en mer komplex modell behöver anpassas?\nAnpassa en regressionsmodell med infektionsrisken (Infection_risk) som responsvariabel och alla variablerna i designmatrisen ni skapat som förklarande variabler. Utvärdera modellen med hjälp av residualanalys och fokusera på att kontrollera antagandet om linjäritet.\nVissa forskare tror att det kan finnas en interaktion mellan variablerna \\(X_2\\) och \\(X_4\\) samt mellan \\(X_3\\) och \\(X_4\\) i relation till responsvariabeln. Utgå från modellen i b) och lägg till lämpliga interaktionstermer till designmatrisen för dessa två samband och anpassa en ny modell (med sex förklarande variabler).\n\nUtvärdera modellen med hjälp av residualanalys och jämför med diagrammen från b). Hur har modellen blivit bättre?\nTesta med ett test om interaktionstermerna kan uteslutas.\n\n\n\n\n7.3.2 Bostadsuthyrning\nDatamaterialet för denna övning innehåller en responsvariabel (\\(Y\\) = uthyrningskostnad i tusentals dollar) och fyra förklarande variabler:\n\n\\(X_1\\) = ålder (år),\n\\(X_2\\) = driftkostnad och skatt (tusentals dollar),\n\\(X_3\\) = vakansgrad (andel),\n\\(X_4\\) = yta (kvadratfot)\n\nDatamaterialet kan laddas ner här.\n\nVisualisera de parvisa samband mellan de fyra förklarande variablerna och responsvariabeln. Är det något i dessa diagram som sticker ut som motiverar att en mer komplex modell behöver anpassas?\nAnpassa en regressionsmodell där Y förklaras av \\(X_1\\), \\(X_2\\) och \\(X_4\\). Utvärdera modellen med hjälp av residualanalys och fokusera på att kontrollera antagandet om linjäritet.\nAnpassa två regressionsmodeller:\n\nModell 1: \\(Y\\) som responsvariabel och \\(X_1\\), \\(X_1^2\\), \\(X_2\\) och \\(X_4\\) som förklarande variabler.\nModell 2: Samma variabler som i modell 1, men \\(X_1\\) centreras: \\(X_{1,c} = X_1 - \\bar{X}_1\\) där \\(\\bar{X}_1\\) är medelvärdet för \\(X_1\\).\n\nTa fram parameterskattningarna för de båda modellerna och jämför koefficienterna för de icke-centrerade och centrerade variablerna.\nBeräkna korrelationsmatrisen för designmatrisen från de båda modellerna, avrundade till 2 decimaler. Undersök hur de två matriserna skiljer sig åt och fundera kring vilken effekt som centrering haft.\n\nFöljande uppgifter använder sig av modell 2.\n\nUtvärdera modellen med hjälp av residualanalys och jämför med diagrammen från b). Hur har modellen blivit bättre?\nSkatta medelvärdet av \\(Y\\) med ett 95-procentigt konfidensintervall för följande observation \\(\\{X_1 = 8, X_2 = 16, X4 = 250 000\\}\\). Tolka intervallet.3",
    "crumbs": [
      "Regressionsanalys",
      "**DEL II - Regressionsanalys**",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Komplexa samband</span>"
    ]
  },
  {
    "objectID": "01-regression/06-complex-predictors.html#referenser",
    "href": "01-regression/06-complex-predictors.html#referenser",
    "title": "7  Komplexa samband",
    "section": "Referenser",
    "text": "Referenser\n\n\n\n\nHaley, Robert, DANA QUADE, HOWARD FREEMAN, och JOHN BENNETT. 1980. ”The SENIC Project. Study on the efficacy of nosocomial infection control (SENIC Project). Summary of study design”. American journal of epidemiology 111 (juni): 472–85. https://doi.org/10.1093/oxfordjournals.aje.a112928.",
    "crumbs": [
      "Regressionsanalys",
      "**DEL II - Regressionsanalys**",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Komplexa samband</span>"
    ]
  },
  {
    "objectID": "01-regression/06-complex-predictors.html#footnotes",
    "href": "01-regression/06-complex-predictors.html#footnotes",
    "title": "7  Komplexa samband",
    "section": "",
    "text": "Tänk på att temperatur har ett negativt samband som blir ett värde närmare 0 med interaktionen.↩︎\nSe Wikipedia för en beskrivning av dessa regioner↩︎\nNotera att modellen utgår från ett centrerad \\(X_1\\), vilket innebär att vi måste centrera den nya observationens värde innan prediktionen beräknas.↩︎",
    "crumbs": [
      "Regressionsanalys",
      "**DEL II - Regressionsanalys**",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Komplexa samband</span>"
    ]
  },
  {
    "objectID": "01-regression/07-multicollinearity.html",
    "href": "01-regression/07-multicollinearity.html",
    "title": "8  Multikollinearitet",
    "section": "",
    "text": "8.1 Multikollinearitet\nVi börjar med att undersöka de parvisa sambanden mellan de två förklarande variablerna, trädets höjd och diameter, och responsvariabeln, trädets volym.\nVisa kod\ntrees %&gt;% \n  pivot_longer(!Volume) %&gt;% \n  ggplot() + aes(x = value, y = Volume) + geom_point(color = \"steelblue\") + \n  facet_wrap(vars(name), scale = \"free\") + theme_bw() +\n  labs(x = \"Förklarande variabel\")\n\n\n\n\n\nSamband mellan förklarande variabler och responsvariabeln\nBåda variablerna verkar ha ett måttligt starkt, positivt, och linjärt samband, där diameterns samband verkar vara starkare än höjden. Vi kan då ställa upp modellen med båda förklarande variablerna enligt:\n\\[\n\\begin{aligned}\n  Y_i &= \\beta_0 + \\beta_1 \\cdot X_{i1} + \\beta_2 \\cdot X_{i2} + E_i\\\\\n\\\\\n&\\text{eller i matrisform}\\\\\n\\\\\n\\mathbf{Y} &= \\mathbf{X} \\boldsymbol{\\beta} + \\mathbf{E}\n\\end{aligned}\n\\]\ndär \\[\n\\begin{aligned}\n  \\mathbf{Y} = \\begin{bmatrix}Y_1\\\\Y_2\\\\\\vdots\\\\Y_{31}\\end{bmatrix} \\quad \\mathbf{X} = \\begin{bmatrix}1 & X_{1,1} & X_{1,2} \\\\1 & X_{2,1} & X_{2,2} \\\\\\vdots & \\vdots & \\vdots \\\\1 & X_{31,1} & X_{31,2} \\end{bmatrix} \\quad \\boldsymbol{\\beta} = \\begin{bmatrix}\\beta_0\\\\\\beta_1\\\\\\beta_2\\end{bmatrix} \\quad \\mathbf{E} = \\begin{bmatrix}E_1\\\\E_2\\\\\\vdots\\\\E_{31}\\end{bmatrix}\n\\end{aligned}\n\\] Vi anpassar modellen in R och kan se de skattade koefficienterna i följande tabell:\nVisa kod\nmodel &lt;- lm(formula = Volume ~ ., data = trees)\n\nsummary(model) %&gt;% \n  coef() %&gt;% \n  round(3) %&gt;% \n  kable(col.names = c(\"Variabel\", \"Skattning\", \"Medelfel\", \"t-värde\", \"p-värde\")) %&gt;% \n  kable_styling(\"striped\")\n\n\nAnpassad modell där volymen av ett träd förklaras av dess diameter\noch dess höjd\n \n  \n    Variabel \n    Skattning \n    Medelfel \n    t-värde \n    p-värde \n  \n \n\n  \n    (Intercept) \n    -57.988 \n    8.638 \n    -6.713 \n    0.000 \n  \n  \n    Girth \n    4.708 \n    0.264 \n    17.816 \n    0.000 \n  \n  \n    Height \n    0.339 \n    0.130 \n    2.607 \n    0.014\nANOVA-tabellen visar hur responsvariabelns variation fördelar sig på modellens olika förklarande variabler och den oförklarade variationen (felet). Vi kan plocka ut denna information från modellobjektet genom anova().\nVisa kod\nanova(model) %&gt;% \n  kable(col.names = c(\"Källa\", \"df\", \"SS\", \"MS\", \"F-värde\", \"p-värde\")) %&gt;% \n  kable_styling(\"striped\")\n\n\nAnpassad modell där volymen av ett träd förklaras av dess diameter\noch dess höjd\n \n  \n    Källa \n    df \n    SS \n    MS \n    F-värde \n    p-värde \n  \n \n\n  \n    Girth \n    1 \n    7581.7813 \n    7581.78133 \n    503.15034 \n    0.000000 \n  \n  \n    Height \n    1 \n    102.3812 \n    102.38118 \n    6.79433 \n    0.014491 \n  \n  \n    Residuals \n    28 \n    421.9214 \n    15.06862\nVi kan läsa av de sekventiella kvadratsummorna \\(SS(Girth)\\) och \\(SS(Height|Girth)\\), alltså hur mycket förklarande variation diametern bidrar med och hur mycket förklarande variation höjden bidrar med givet att diametern redan finns med i modellen. Vi kan också uttrycka det som att höjden bidrar med ca 102.4 ytterligare unik förklarad variation av responsvariabeln som diametern inte redan har förklarat, vilket relativt den totala variationen på ca 8000 inte är mycket trots det starka parvisa sambandet.\nEftersom tabellen visar sekventiella kvadratsummor kommer värdena påverkas av vilken ordning variablerna inkluderas i modellen. Låt oss byta ordning på de förklarande variablerna när vi anpassar modellen:\nVisa kod\nmodel &lt;- lm(formula = Volume ~ Height + Girth, data = trees)\n\nanova(model) %&gt;% \n  kable(col.names = c(\"Källa\", \"df\", \"SS\", \"MS\", \"F-värde\", \"p-värde\")) %&gt;% \n  kable_styling(\"striped\")\n\n\nAnpassad modell där volymen av ett träd förklaras av dess diameter\noch dess höjd, men annan ordning på variablerna\n \n  \n    Källa \n    df \n    SS \n    MS \n    F-värde \n    p-värde \n  \n \n\n  \n    Height \n    1 \n    2901.1889 \n    2901.18887 \n    192.5318 \n    0 \n  \n  \n    Girth \n    1 \n    4782.9736 \n    4782.97364 \n    317.4129 \n    0 \n  \n  \n    Residuals \n    28 \n    421.9214 \n    15.06862\nI denna tabell ser vi att \\(SS(Height) = 2901.2\\) vilket är betydligt högre än \\(SS(Height|Girth) = 102.4\\). När dessa två kvadratsummor är olika indikerar det att de förklarande variablerna förklarar samma sak/del av responsvariabeln, där diametern verkar vara den variabel som enskilt har mest unik information, \\(SS(Girth) = 7581.7813\\).\nVi ser också att p-värden för de olika F-testen förändras beroende på ordningen och det är självklart eftersom de undersöker olika modeller. Alla testen som genomförs är partiella F-test, eftersom vi testar enskilda parametrar och inte hela modellen, men den kompletta och reducerade modellen förändras i de två tabellerna.\nExempelvis, \\(SS(Height)\\) betyder att den kompletta modellen endast inkluderar höjden medan den reducerade modellen är en tom modell. \\(SS(Height|Girth)\\) betyder att den kompletta modellen inkluderar två variabler medan den reducerade modellen endast inkluderar diametern. Vi kan därför dra slutsatsen att, med en procents signifikansnivå vardera, höjden bidrar till att förklara volymen om det är den enda variabeln i modellen, men att höjden inte bidrar någon ytterligare information om diametern redan inkluderats.\nNågonting som är lika i de två tabellerna är \\(SSE\\). Detta gäller för att den additiva egenskapen för kvadratsummor delar upp den totala variationen (\\(SSY\\)) i den förklarade variationen (\\(SSR\\)) och den oförklarade variationen (\\(SSE\\)). Vi har i båda tabellerna tagit med samma variabler så den totala och förklarade variationen har inte förändrats. Vi kan matematiskt uttrycka det som: \\[\n\\begin{aligned}\n    SSR &= SS(Height) + SS(Girth|Height) \\\\\n    &= SS(Girth) + SS(Height|Girth)\n\\end{aligned}\n\\]",
    "crumbs": [
      "Regressionsanalys",
      "**DEL II - Regressionsanalys**",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Multikollinearitet</span>"
    ]
  },
  {
    "objectID": "01-regression/07-multicollinearity.html#sec-multicollinearity",
    "href": "01-regression/07-multicollinearity.html#sec-multicollinearity",
    "title": "8  Multikollinearitet",
    "section": "",
    "text": "8.1.1 Variance Inflating Factors (VIF)\nI en multipel linjär regression kan en förklarande variabel enskilt, eller flera förklarande variabler tillsammans, beskriva en annan förklarande variabel och enkla parvisa korrelationer eller spridningsdiagram kan inte på ett lätt sätt identifiera detta. Istället kan vi undersöka de förklarande variablerna effekt med varandra med hjälp av variance inflating factors, härmed kallad för VIF.\n\n\n\n\n\n\nNotera\n\n\n\nNamnet VIF uppkommer på grund av att variansen för parameterskattningarna inflateras om flera förklarande variabler har starka samband med varandra.\n\\[\n\\begin{aligned}\n  \\sigma^2_{\\boldsymbol{\\hat{\\beta}}} = (\\mathbf{X}'\\mathbf{X})^{-1} \\sigma^2\n\\end{aligned}\n\\] I värsta fall är \\((\\mathbf{X}'\\mathbf{X})\\) singulär och kan inte inverteras, som leder till att vi kan inte beräkna variansen överhuvudtaget.\n\n\nVIF beräknas genom att mäta hur mycket en förklarande variabel förklaras av de övriga förklarande variablerna. Detta har vi ju ett mått på sedan tidigare i relation till regressionsmodellen, nämligen förklaringsgraden (Avsnitt 5.3), men istället för att modellera responsvariabeln skapas en regressionsmodell per förklarande variabel som beskrivs av de övriga variablerna förutom responsvariabeln.\nVIF beräknas enligt: \\[\n\\begin{aligned}\n  VIF_j = \\frac{1}{1-R^2_{j}}\n\\end{aligned}\n\\] där \\(j\\) är den j:te förklarande variabeln och \\(R^2_j\\) är förklaringsgraden från en regressionsmodell med \\(X_j\\) som responsvariabel. Eftersom en förklaringsgrad är begränsad mellan 0 och 1 kan vi visualisera vilka värden på VIF som skapas för olika nivåer av samband mellan de förklarande variablerna.\n\n\nVisa kod\ntibble(\n  R2 = seq(0, 1, by = 0.001),\n  VIF = 1 / (1 - R2)\n  ) %&gt;% \n  ggplot() + aes(x = R2, y = VIF) + geom_line(color = \"steelblue\", linewidth = 1) + \n  theme_bw() + labs(x = expression(R[j]^2)) + \n  scale_x_continuous(breaks = seq(0, 1, by = 0.1)) + \n  scale_y_continuous(breaks = seq(0, 100, by = 10), limits = c(0, 100)) +\n  geom_hline(yintercept = 5, color = \"#d9230f\", linetype = 2)\n\n\n\n\n\n\n\n\nFigur 8.1: VIF för olika förklaringsgrader, där VIF = 5 är markerad\n\n\n\n\n\nTolkningen av VIF är väldigt subjektivt men Figur 8.1 visar att VIF-värden kring 5 eller större motsvarar en förklaringsgrad på 75% eller större. När vi tolkar förklaringsgraden för en “vanlig” regressionsmodell är 75-80% där vi kan säga att modellen beskriver responsvariabeln bra, och vi kan använda samma värde även här, fast vi nu menar något negativt. Vi kan använda följande tumregler:\n\nnär VIF för en enskild variabel överskrider 10\nnär genomsnittliga VIF för alla variabler överskrider 5\n\nOm modellen uppvisar någon av dessa bör vi undersöka modellens förklarande variabler vidare då risken för problem med multikollinearitet är hög.\nFrån paketet car får vi en funktion (vif()) som beräknar VIF för varje förklarande variabel i en regressionsmodell.\n\n\nVisa kod\nrequire(car)\n\nvif(model) %&gt;% \n  round(3) %&gt;% \n  kable(col.names = c(\"Variabel\", \"VIF\")) %&gt;% \n  kable_styling(\"striped\", full_width = FALSE)\n\n\n\n \n  \n    Variabel \n    VIF \n  \n \n\n  \n    Height \n    1.369 \n  \n  \n    Girth \n    1.369 \n  \n\n\n\n\n\nFör modellen över trädens volym verkar inte VIF indikera på några risker med multikollinearitet då alla värden är &lt;5.\n\n\n8.1.2 Generaliserade VIF (GVIF)\nNär vi använder kvalitativa förklarande variabler kommer förklaringsgraden i beräkningen för VIF inte kunna använda en linjär regressionsmodell då indikatorvariabler som skapas från den variabeln inte längre är kontinuerliga. Detta specialfall löses genom att skapa generaliserade VIF som tar hänsyn till den binära variabelns struktur. Istället för att använda förklaringsgraden \\(R^2_j\\) beräknas effekten som de övriga förklarande variablerna har med \\(X_j\\) med hjälp av determinanter av korrelationsmatriser mellan olika förklarande variabler. (Fox och Monette 1992)\nVärden på GVIF kan tolkas på likt VIF, vi vill inte ha höga värden och tumregeln &lt;5 för genomsnittet eller &lt;10 för enskilda parametrar indikerar på en modell med låg risk för multikollinearitetsproblem.",
    "crumbs": [
      "Regressionsanalys",
      "**DEL II - Regressionsanalys**",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Multikollinearitet</span>"
    ]
  },
  {
    "objectID": "01-regression/07-multicollinearity.html#specialfall-med-polynom-och-interaktioner",
    "href": "01-regression/07-multicollinearity.html#specialfall-med-polynom-och-interaktioner",
    "title": "8  Multikollinearitet",
    "section": "8.2 Specialfall med polynom och interaktioner",
    "text": "8.2 Specialfall med polynom och interaktioner\nOm en modell inkluderar polynom eller interaktioner har vi artificiellt skapat ett samband mellan variabler och då kommer VIF, eller GVIF, vara högre än vad som vi förväntar. Multikollinearitet medför problem med tolkningar och inferens av enskilda parametrar men en modell innehållande polynom eller interaktioner har redan detta problem. Om en variabel förekommer i flera termer av modellen kan en enskild lutningsparameter inte tolkas och inferens för variabelns effekt mot responsvariabeln omfattar fler parametrar än bara grundvariabelns enskilda effekt. Därför bör tolkningar av VIF fokusera på de enskilda variablerna.\nVi kan titta närmare på Tabell 7.1 och Tabell 7.2 som exempel. I dessa två modeller har vi skapat en interaktion och polynom vilket innebär att vi har skapat variabler som har ett tydligt samband med de övriga.\n\n\nVisa kod\nvif(braModell) %&gt;% \n  round(3) %&gt;% \n  kable(col.names = c(\"Variabel\", \"VIF\")) %&gt;% \n  kable_styling(\"striped\", full_width = FALSE)\n\n\nVIF för modellens variabler inklusive interaktionen\n \n  \n    Variabel \n    VIF \n  \n \n\n  \n    x1 \n    1.004 \n  \n  \n    x2 \n    5.422 \n  \n  \n    x1:x2 \n    5.424 \n  \n\n\n\n\n\n\n\n\n\n\n\nViktigt\n\n\n\nvif() varnar faktiskt när vi beräknar VIF på en modell innehållande interaktioner just för att den identifierar att vi skapat variabler av en högre “ordning” och grundvariablerna förekommer på flera ställen i modellen. Vi kan med hjälp av argumentet vif(type = \"predictor\") beräkna GVIF som grupperar sambandet endast mellan grundvariablerna.\n\n\nVisa kod\ngvif &lt;- vif(braModell, type = \"predictor\") \n\ngvif %&gt;% \n  tibble() %&gt;% \n  select(1:3) %&gt;% \n  round(3) %&gt;% \n  mutate(Variabel = rownames(gvif)) %&gt;% \n  relocate(Variabel) %&gt;% \n  kable() %&gt;% \n  kable_styling(\"striped\", full_width = FALSE)\n\n\nGeneraliserade VIF beräknad per variabel istället för per term i\nmodellen\n \n  \n    Variabel \n    GVIF \n    Df \n    GVIF^(1/(2*Df)) \n  \n \n\n  \n    x1 \n    1 \n    3 \n    1 \n  \n  \n    x2 \n    1 \n    3 \n    1 \n  \n\n\n\n\n\n\n\nNär polynom introduceras till en modell är det endast en variabel som bidrar till sambandet till skillnad från två, eller fler, i en interaktion. På grund utav det starka beroende som då uppkommer är centrering ett måste om vi vill kunna bedöma enskilda parametrars bidrag till modellen. Vi kan i följande två tabeller se effekten av centrering på VIF\n\n\nVisa kod\nvif(model) %&gt;% \n  round(3) %&gt;% \n  kable(col.names = c(\"Variabel\", \"VIF\")) %&gt;% \n  kable_styling(\"striped\", full_width = FALSE)\n\n\nVIF för en modell med icke-centrerade polynom\n \n  \n    Variabel \n    VIF \n  \n \n\n  \n    X \n    5.228 \n  \n  \n    X2 \n    5.228 \n  \n\n\n\n\n\n\n\nVisa kod\nvif(centModel) %&gt;% \n  round(3) %&gt;% \n  kable(col.names = c(\"Variabel\", \"VIF\")) %&gt;% \n  kable_styling(\"striped\", full_width = FALSE)\n\n\nVIF för en modell med centrerade polynom\n \n  \n    Variabel \n    VIF \n  \n \n\n  \n    XCent \n    1.006 \n  \n  \n    X2Cent \n    1.006",
    "crumbs": [
      "Regressionsanalys",
      "**DEL II - Regressionsanalys**",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Multikollinearitet</span>"
    ]
  },
  {
    "objectID": "01-regression/07-multicollinearity.html#övningsuppgifter",
    "href": "01-regression/07-multicollinearity.html#övningsuppgifter",
    "title": "8  Multikollinearitet",
    "section": "8.3 Övningsuppgifter",
    "text": "8.3 Övningsuppgifter\nVi kommer återigen använda SENIC data från Avsnitt 7.3.\n\nAnpassa en modell som förklarar infektionsrisken vid sjukhuset med hjälp av följande variabler:\n\n\\(X_1\\) = Length_of_stay\n\\(X_2\\) = Average_daily_census\n\\(X_3\\) = Number_of_beds\n\\(X_4\\) = Routine_chest_X_ray_ratio\n\n\n\nBeräkna en korrelationsmatris för de förklarande variablerna och identifiera de tre största parvisa korrelationerna.\nVisualisera de parvisa sambanden mellan de förklarande variablerna och bedöm om korrelationerna i 2. är missvisande eller ej.\nJämför de riktningen av de skattade koefficienterna i 1. och signifikansen av de enskilda testen med parvisa samband mellan respektive förklarande variabel och responsvariabeln. Jämför resultaten för respektive variabel. Visar de på samma sorts samband?\nBeräkna VIF för modellen och bedöm, tillsammans med informationen i tidigare uppgifter, vilka variabler som verkar bidra allra mest till en hög risk för multikollinearitetsproblem.",
    "crumbs": [
      "Regressionsanalys",
      "**DEL II - Regressionsanalys**",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Multikollinearitet</span>"
    ]
  },
  {
    "objectID": "01-regression/07-multicollinearity.html#referenser",
    "href": "01-regression/07-multicollinearity.html#referenser",
    "title": "8  Multikollinearitet",
    "section": "Referenser",
    "text": "Referenser\n\n\n\n\nFox, John, och Georges Monette. 1992. ”Generalized Collinearity Diagnostics”. Journal of the American Statistical Association 87 (417): 178–83. http://www.jstor.org/stable/2290467.",
    "crumbs": [
      "Regressionsanalys",
      "**DEL II - Regressionsanalys**",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Multikollinearitet</span>"
    ]
  },
  {
    "objectID": "01-regression/08-detailed-residuals.html",
    "href": "01-regression/08-detailed-residuals.html",
    "title": "9  Detaljerad residualanalys",
    "section": "",
    "text": "9.1 Avvikande observationer\nLåt oss anpassa fyra olika modeller, en för vardera simulerade responsvariabel y1-y4.\n## Anpassar modellen som ska undersökas\nmodel1 &lt;- lm(y1 ~ x, data = data)\nmodel2 &lt;- lm(y2 ~ x, data = data)\nmodel3 &lt;- lm(y3 ~ x, data = data)\nmodel4 &lt;- lm(y4 ~ x, data = data)\nVisa kod\nresidualPlots(model1)\n\n\n\n\n\nResidualdiagram för modell 1\nVi har ett värde vid \\(\\hat{y} \\approx 20\\) och \\(\\hat{E} \\approx 6\\) som har ett något större residualvärde än de övriga, men det är inte ett jättetydligt extremvärde.\nVisa kod\nresidualPlots(model2)\n\n\n\n\n\nResidualdiagram för modell 2\nI denna modell verkar vi återigen ha en observation vars residual avviker något från de övriga. Vi har också en ökad osäkerhet både i residualerna (större absolutbelopp) vilket uppkommer då datamaterialet generellt visar på ett större brus.\nVisa kod\nresidualPlots(model3)\n\n\n\n\n\nResidualdiagram för modell 3\nModell 3 har en extremt tydlig avvikande observation som påverkar modellanpassningen negativt om vi jämför med modell 1.\nVisa kod\nresidualPlots(model4)\n\n\n\n\n\nResidualdiagram för modell 4\nNär den avvikande observationer finns i utkanten av x:s värdemängd ser vi ännu tydligare en residual i diagrammen.\nOm vi sammanställer en tabell för alla de fyra anpassade modellerna kan vi tydligare se effekten av hur olika sorters brus och extremvärden påverkar modellanpassningen.\nVisa kod\nmodelJämför &lt;- \n  tibble(\n    `Sann modell` = c(\"5\", \"5\", \"-\"),\n    `Lite brus` = c(model1$coefficients, summary(model1)$r.squared),\n    `Stort brus` = c(model2$coefficients, summary(model2)$r.squared),\n    `Avvikande nära mitt` = c(model3$coefficients, summary(model3)$r.squared),\n    `Avvikande i ytterkant` = c(model4$coefficients, summary(model4)$r.squared)\n  ) %&gt;% \n  as.data.frame()\n\nrownames(modelJämför) &lt;- c(\"$\\\\hat{\\\\beta}_0$\", \"$\\\\hat{\\\\beta}_1$\", \"$R^2$\")\n\nmodelJämför %&gt;% \n  kable(caption = \"Skattade koefficienter för alla modeller.\", digits = 3, escape = FALSE) \n\n\n\n\nTabell 9.1: Skattade koefficienter för alla modeller.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSann modell\nLite brus\nStort brus\nAvvikande nära mitt\nAvvikande i ytterkant\n\n\n\n\n\\(\\hat{\\beta}_0\\)\n5\n6.460\n12.257\n6.655\n11.425\n\n\n\\(\\hat{\\beta}_1\\)\n5\n4.852\n4.216\n4.916\n4.192\n\n\n\\(R^2\\)\n-\n0.986\n0.694\n0.951\n0.750\nVisa kod\np1 &lt;- \n  data %&gt;% \n  ggplot() + aes(x = x, y = y1) + \n  geom_point(size = 1,  color = \"steelblue\") + \n  geom_smooth(formula = y~x, method = \"lm\", se = FALSE, color = \"steelblue\", linewidth = 1) + \n  geom_abline(intercept = 5, slope = 5, color = \"#d9230f\", linewidth = 1, linetype = 2) + \n  theme_bw()\n\np2 &lt;- \n  data %&gt;% \n  ggplot() + aes(x = x, y = y2) + \n  geom_point(size = 1,  color = \"steelblue\") + \n  geom_smooth(formula = y~x, method = \"lm\", se = FALSE, color = \"steelblue\", linewidth = 1) + \n  geom_abline(intercept = 5, slope = 5, color = \"#d9230f\", linewidth = 1, linetype = 2) + \n  theme_bw()\n\np3 &lt;- \n  data %&gt;% \n  ggplot() + aes(x = x, y = y3) + \n  geom_point(size = 1,  color = \"steelblue\") + \n  geom_smooth(formula = y~x, method = \"lm\", se = FALSE, color = \"steelblue\", linewidth = 1) + \n  geom_abline(intercept = 5, slope = 5, color = \"#d9230f\", linewidth = 1, linetype = 2) + \n  theme_bw()\n\np4 &lt;- \n  data %&gt;% \n  ggplot() + aes(x = x, y = y4) + \n  geom_point(size = 1,  color = \"steelblue\") + \n  geom_smooth(formula = y~x, method = \"lm\", se = FALSE, color = \"steelblue\", linewidth = 1) + \n  geom_abline(intercept = 5, slope = 5, color = \"#d9230f\", linewidth = 1, linetype = 2) + \n  theme_bw()\n\ncowplot::plot_grid(p1, p2, p3, p4, nrow = 2)\n\n\n\n\n\nDe fyra anpassade modellerna och den sanna regressionslinjen\nVi ser att modell 1 med litet brus (y1) faller ganska nära det sanna modellen, \\(y = 5 + 5 \\cdot x\\), och detsamma gäller modellen med den avvikande observationen nära mittpunkten av den förklarande variabeln (y3). Däremot verkar modellerna där det finns ett stort brus överlag (y2) och en avvikande observation i ytterkanten av den förklarande variabeln (y4) hamna längre ifrån den sanna modellen och ha en större grad av osäkerhet eftersom förklaringsgraden är närmare 70 procent jämfört med 95-99 procent.\nVad detta resultat visar oss är att en avvikande observation behöver inte alltid generera en påverkan på modellanpassningen i stort men modellen kommer inte lyckas anpassa just den observationen bra. Vi behöver titta närmare på residualerna för att korrekt identifiera ifall en avvikande observation faktiskt påverkar modellen.",
    "crumbs": [
      "Regressionsanalys",
      "**DEL II - Regressionsanalys**",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Detaljerad residualanalys</span>"
    ]
  },
  {
    "objectID": "01-regression/08-detailed-residuals.html#avvikande-observationer",
    "href": "01-regression/08-detailed-residuals.html#avvikande-observationer",
    "title": "9  Detaljerad residualanalys",
    "section": "",
    "text": "9.1.1 Standardiserade residualer\nStandardiserade residualer sätter residualernas absoluta värden i relation till den generella residualspridningen. En residual med stort absolutvärde i en modell med stor generell residualspridning är mindre avvikande än en residual med stort absolutvärde i en modell med liten generell residualspridning.\n\\[\n\\begin{aligned}\n  z_i = \\frac{e_i}{s_{e_i}}\n\\end{aligned}\n\\]\n\n\nVisa kod\n## Funktion för att beräkna standardiserade residualer\nstandardResid &lt;- function(model) {\n  \n  ## Residualerna dividerat med residualspridningen\n  z &lt;- model$residuals / summary(model)$sigma\n  \n  return(z)\n}\n\n\n# Beräknar standardiserade residualer\nresidualData &lt;- \n  tibble(\n    `Lite brus` = standardResid(model1),\n    `Stort brus` = standardResid(model2),\n    `Avvikande nära mitt` = standardResid(model3),\n    `Avvikande i ytterkant` = standardResid(model4)\n  ) %&gt;% \n  mutate(\n    index = 1:n()\n  ) %&gt;% \n  as.data.frame()\n  \nresidualData %&gt;% \n  ## \"Roterar\" datamaterialet för att förenkla visualiseringar\n  pivot_longer(\n    cols = -index,\n    names_to = \"Data\",\n    values_to = \"resid\"\n  ) %&gt;% \n  mutate(\n    ## Konverterar grupperingsvariabeln till en factor med en angiven ordning på nivåerna\n    Data = factor(Data, levels = c(\"Lite brus\", \"Stort brus\", \"Avvikande nära mitt\", \"Avvikande i ytterkant\"))\n  ) %&gt;% \n  ggplot() + aes(x = index, y = resid) + \n  geom_point(size = 2, color = \"steelblue\") + theme_bw() + \n  facet_wrap(~Data, nrow = 2, ncol = 2) + \n  labs(y = \"Standard. residualer\")\n\n\n\n\n\nStandardiserade residualer för de olika modellerna.\n\n\n\n\nI figuren visas de standardiserade residualerna för respektive modell och här syns det tydligt vilka residualer som faktiskt är en avvikande observation och vilka som ändå ligger inom residualernas generella spridning. Det är främst i modellerna med extremvärden som de standardiserade residualerna uppvisar tydliga indikationer att dessa observationer är avvikande.\n\n\n9.1.2 Avvikande observationer i X\nI Tabell 9.1 såg vi att avvikande observationer i utkanten av X:s värdemängd verkar ha en större påverkan på modellanpassningen än avvikande observationer närmare mittpunkten av värdemängden. I en multipel linjär regressionsmodell kan denna jämförelse bli svår att genomföra med visualiseringar eller jämförelser mellan flera skattade lutningsparametrar. Istället kan leverage beräknas som visar hur långt bort den enskilda observationen (residualen) är från genomsnittet av alla förklarande variabler.\nLeverage beräknas utifrån hattmatrisen, \\(\\mathbf{H} = \\mathbf{X}(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\), specifikt diagonalelementen av matrisen. \\(h_i\\) är det i:te diagonalelementet från matrisen vars värden kan falla mellan 0 och 1, där 1 indikerar på stort avstånd från genomsnittet. Vi kan beräkna hattmatrisen i R med hjälp av matrisalgebra eller med hjälp av funktionen lm.influence() där objektet hat innehåller diagonalen av hattmatrisen.\n\n\nVisa kod\n## Skapar designmatrisen\nX &lt;- \n  model.matrix(y ~ x, data = data)\n\n## Beräknar hattmatrisen\nH &lt;- X %*% solve(t(X) %*% X) %*% t(X)\n\n## Plockar ut leverage\nleverage &lt;- diag(H)\n\n## Funktion för att plocka ut leverage\nleverage &lt;- lm.influence(model1)$hat\n\n\nI alla de fyra modellerna använder vi samma värden på den förklarande variabeln, så vi behöver endast skapa en visualisering som gäller för alla modeller.\n\n\nVisa kod\n## Skapar data för visualisering\nvisData &lt;- \n  tibble(\n    leverage = leverage\n  ) %&gt;% \n  mutate(\n    index = 1:n()\n  )\n\n## Visualiserar leverage\nggplot(visData) + aes(x = index, y = leverage) + \n  geom_point(size = 2, color = \"steelblue\") + \n  theme_bw() + \n  ## Anger en skala som går mellan 0 och 1 (möjliga värden av h)\n  scale_y_continuous(\n    limits = c(0, 1)\n  ) + \n  ## Skapar gränsvärdet för observationer som har för \"höga\" leverage och bör undersökas vidare\n  geom_hline(\n    yintercept = 2 * ncol(X) / nrow(X),\n    color = \"#d9230f\",\n    linewidth = 1,\n    linetype = 2\n  ) +\n  labs(x = \"Obs. index\", y = \"Leverage\")\n\n\n\n\n\nLeverage-värden för respektive observation\n\n\n\n\nDen rödstreckade linjen beskriver en tumregel som ger oss lite hjälp på traven att bedöma ifall en residual har ett “högt” leverage värde eller inte. Tumregeln beräknas som: \\[\n\\begin{aligned}\n  \\frac{2\\cdot (k+1)}{n} = \\frac{2 \\cdot p}{n}\n\\end{aligned}\n\\] Det finns två observationer, \\(i = 11\\) och \\(i = 24\\), som har leverage-värden som anses vara högre än tumregeln. Dessa två observationer är de som har minst värden på x, alltså de som ligger längst till vänster i Figur 9.1. Eftersom dessa punkter verkar följa det linjära samband som resten av punktsvärmen visar, finns det ingenting som tyder på att de är några observationer vi behöver oroa oss för.\n\n\n9.1.3 Jackknife-residualer\nEtt ytterligare sätt att använda leverage för att identifiera avvikande observationer är att beräkna Jackknife-residualer. Denna typ av residual beräknas genom att anpassa en modell utan observation \\(i\\) som sedan används för att skatta prediktera värdet på responsvariabeln och beräkna residualen. Eftersom en avvikande observation kan påverka modellanpassningen riskerar en vanlig eller standardiserad residual att underskatta avvikelsen då modellen dras mot den avvikande observationen. När en modell anpassas som inte inkluderar den specifika observationen kommer residualen ge en mer verklig bild av observationens faktiska avvikelse från det generella sambandet.\nIstället för att behöva anpassa n olika modeller där varje respektive observation plockas bort kan vi använda leverage och modellens SSE enligt:\n\\[\n\\begin{aligned}\n    r_{(-i)} = \\frac{e_i \\cdot \\sqrt{n - (k + 2)}}{SSE \\cdot (1 - h_i) - e_i^2}\n\\end{aligned}\n\\] Vi kan skapa denna funktion i R genom följande kod:\n\n\nVisa kod\n## Skapar en funktion som beräknar jackknife-residualer från en angiven modell\njackknife &lt;- function(model) {\n  \n  # Sparar modellens residualer och leverage\n  residuals &lt;- model$residuals\n  \n  leverage &lt;- lm.influence(model)$hat\n  \n  # Antal observationer\n  n &lt;- nrow(model$model)\n  \n  # Antal förklarande variabler (-1 för att ta hänsyn till y)\n  k &lt;- ncol(model$model) - 1\n  \n  # Sparar modellens kvadratsumma för felet\n  SSE &lt;- sum(residuals^2)\n  \n  # Beräknar jackknife residualen\n  jackknife &lt;- residuals * sqrt((n - k - 2) / (SSE * (1 - leverage) - residuals^2))\n  \n  return(jackknife)\n}\n\n\n\n\nVisa kod\nresidualData &lt;- \n  tibble(\n    `Lite brus` = jackknife(model1),\n    `Stort brus` = jackknife(model2),\n    `Avvikande nära mitt` = jackknife(model3),\n    `Avvikande i ytterkant` = jackknife(model4)\n  ) %&gt;% \n  mutate(\n    index = 1:n()\n  ) %&gt;% \n  as.data.frame()\n  \nresidualData %&gt;% \n  ## \"Roterar\" datamaterialet för att förenkla visualiseringar\n  pivot_longer(\n    cols = -index,\n    names_to = \"Data\",\n    values_to = \"resid\"\n  ) %&gt;% \n  mutate(\n    Data = factor(Data, levels = c(\"Lite brus\", \"Stort brus\", \"Avvikande nära mitt\", \"Avvikande i ytterkant\"))\n  ) %&gt;% \n  ggplot() + aes(x = index, y = resid) + \n  geom_point(size = 2, color = \"steelblue\") + theme_bw() + \n  facet_wrap(~Data, nrow = 2, ncol = 2) + \n  labs(y = \"Jackknife-residualer\")\n\n\n\n\n\n\n\n\nFigur 9.2: Jackknife residualer för de olika modellerna.\n\n\n\n\n\nMed hjälp av Jackknife-residualerna ser vi att de observationer med höga leverage-värden inte påverkar modellanpassningen då värdena är i linje med alla andra observationers värden. Däremot syns ett värde var i de två nedersta diagrammen som tydligt indikerar på de extremvärden som vi placerat in avviker från det generella sambandet, vilket vi också förväntar oss.",
    "crumbs": [
      "Regressionsanalys",
      "**DEL II - Regressionsanalys**",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Detaljerad residualanalys</span>"
    ]
  },
  {
    "objectID": "01-regression/08-detailed-residuals.html#inflytelserika-observationer",
    "href": "01-regression/08-detailed-residuals.html#inflytelserika-observationer",
    "title": "9  Detaljerad residualanalys",
    "section": "9.2 Inflytelserika observationer",
    "text": "9.2 Inflytelserika observationer\nTill viss del beskriver Jackknife-residualer huruvida en observation har ett inflytande på modellanpassningen genom att beräkna en mer rättvis avvikelse för varje observation från det generella sambandet. Däremot såg vi i Figur 9.2 att extremvärdet nära mittpunkten av X också fått ett relativt högt värde trots att vi i Tabell 9.1 inte såg att den observationen hade en påverkan på modellanpassningen. Dessa residualer är alltså inte alltid så bra att identifiera observationens effekt på modellanpassningen, mer än att de identifierar avvikande observationer. Istället kan vi använda andra mått såsom DFFITS och Cook’s Distance för att avgöra om en observation påverkar modellanpassningen och anses vara en inflytelserik observation.\n\n9.2.1 DFFITS\nVi kan mäta en observations inflytande på två olika sätt, antingen på sitt egna anpassade värde eller på alla anpassade värden. DFFITS undersöker om en observation har ett inflytande på sitt egna anpassat värde och använder sig utav Jackknife-residualerna som vi beräknade tidigare.\n\\[\n\\begin{aligned}\n  {DFFITS}_i = r_{(-i)} \\cdot \\sqrt{ \\frac{h_i}{1 - h_i}}\n\\end{aligned}\n\\]\n\n\nVisa kod\n## Funktion för att beräkna dffits\ndffits &lt;- function(model) {\n  \n  # Hämtar Jackknife-residualerna\n  jackknife &lt;- jackknife(model)\n  \n  # Sparar leverage\n  leverage &lt;- lm.influence(model)$hat\n  \n  # Beräknar DFFITS\n  dffits &lt;- jackknife * sqrt(leverage / (1 - leverage))\n  \n  return(dffits)\n  \n}\n\n\nGränsvärden för när vi anser ha identifierat en inflytelserik observation med DFFITS beräknas utifrån storleken på datamaterialet, där små till medelstora material har gränsvärdet 1 (heldragen linje i figuren nedan) eller \\(2\\cdot \\sqrt{\\frac{p}{n}}\\) för stora datamaterial (streckad linje).\n\n\nVisa kod\nresidualData &lt;- \n  tibble(\n    `Lite brus` = dffits(model1),\n    `Stort brus` = dffits(model2),\n    `Avvikande nära mitt` = dffits(model3),\n    `Avvikande i ytterkant` = dffits(model4)\n  ) %&gt;% \n  mutate(\n    index = 1:n()\n  ) %&gt;% \n  as.data.frame()\n  \nresidualData %&gt;% \n  ## \"Roterar\" datamaterialet för att förenkla visualiseringar\n  pivot_longer(\n    cols = -index,\n    names_to = \"Data\",\n    values_to = \"resid\"\n  ) %&gt;% \n  mutate(\n    Data = factor(Data, levels = c(\"Lite brus\", \"Stort brus\", \"Avvikande nära mitt\", \"Avvikande i ytterkant\"))\n  ) %&gt;% \n  ggplot() + aes(x = index, y = abs(resid)) + \n  geom_point(size = 2, color = \"steelblue\") + theme_bw() + \n  facet_wrap(~Data, nrow = 2, ncol = 2, scale = \"free_y\") + \n  labs(y = \"DFFITS\") + \n  geom_hline(\n    yintercept = 1,\n    linewidth = 1,\n    linetype = 1,\n    color = \"#d9230f\"\n  ) + \n  geom_hline(\n    yintercept = 2*sqrt(ncol(X) / nrow(X)),\n    linewidth = 1,\n    linetype = 2,\n    color = \"#d9230f\"\n  )\n\n\n\n\n\nAbsolutbelopp för DFFITS från de olika modellerna.\n\n\n\n\nVårt simulerade data kan anses vara litet (\\(n = 31\\)) och vi kan därför använda värdet 1 som gräns. Vi ser att observation 11 anses inflytelserik i modellerna utan extremvärden men att den inte anses det när vi lagt till en tydligt avvikande observation som istället tagit över allt inflytande.\n\n\n9.2.2 Cook’s Distance\nDet är nästan självklart att en avvikande observation kommer påverka sitt egna anpassade värde. Det som vi oftast är intresserad av att undersöka är ifall en avvikande observation ger oss en felaktig modellanpassning för det generella sambandet, alltså om andra observationers anpassning påverkas. Måttet Cook’s Distance visar hur mycket inflytande en observation har på alla anpassade värden.\nBeroende på vilka sorters residualer vi har tillhanda kan vi beräkna Cook’s Distance på två olika sätt. Studentiserade residualer kan delas upp i interna studentiserade och externa studentiserade. Den internt studentiserade residualen (angivet som \\(r_i\\) i Kleinbaum m.fl. (2013)) är en form av standardisering där vi också tar hänsyn till observationens leverage-värde inte bara residualspridningen.\n\\[\n\\begin{aligned}\n    r_i = \\frac{e_i}{\\sqrt{MSE \\cdot (1 - h_i)}} = \\frac{e_i}{s_{e_i} \\cdot \\sqrt{(1 - h_i)}}\n\\end{aligned}\n\\] Med hjälp av de internt studentiserade residualerna kan vi beräkna Cook’s Distance enligt: \\[\n\\begin{aligned}\n  d_i = \\left( \\frac{1}{k + 1} \\right) \\cdot \\left( \\frac{h_i}{1 - h_i}\\right)\\cdot r_i^2\n\\end{aligned}\n\\]\nExternt studentiserade residualer (det som funktionerna stats::rstudent() och MASS::studres() kallar enbart studentiserade residualer) standardiserar residualen baserat på en modell som saknar observation \\(i\\), vilket överensstämmer med definitionen av Jackkniferesidualer från Kleinbaum m.fl. (2013). I själva verket är det Jackknife residualerna som R använder sig utav i sina beräkningar av Cook’s Distance i cooks.distance() men vi kan få samma resultat genom att använda de (internt) studentiserade residualerna istället:\n\n\nVisa kod\n## Funktion för att beräkna Cook's Distance\ncooks &lt;- function(model) {\n  \n  # Sparar residualerna\n  residuals &lt;- model$residuals\n  \n  # Sparar residualspridningen\n  s &lt;- summary(model)$sigma\n  \n  # Sparar leverage\n  leverage &lt;- lm.influence(model)$hat\n  \n  # Sparar antalet variabler (-1 för att inte räkna y)\n  k &lt;- ncol(model$model) - 1\n  \n  # Beräknar studentiserade residualer\n  studentized &lt;- residuals / (s * sqrt(1 - leverage))\n  \n  # Beräknar Cook's Distance\n  cooks &lt;- (1 / (k + 1)) * (leverage / (1 - leverage)) * studentized^2 \n  \n  return(cooks)\n  \n}\n\n\nGränsvärdet för vad som anses vara en inflytelserik observation kan dras vid 1 (den heldragna linjen i figurerna nedan), men det finns också mer robusta gränsvärden (den streckade linjen) som har plockats fram av Muller och Mok (1997) som används likt kritiska värden i hypotesprövningar.1\n\n\nVisa kod\nresidualData &lt;- \n  tibble(\n    `Lite brus` = cooks(model1),\n    `Stort brus` = cooks(model2),\n    `Avvikande nära mitt` = cooks(model3),\n    `Avvikande i ytterkant` = cooks(model4)\n  ) %&gt;% \n  mutate(\n    index = 1:n()\n  ) %&gt;% \n  as.data.frame()\n  \nresidualData %&gt;% \n  ## \"Roterar\" datamaterialet för att förenkla visualiseringar\n  pivot_longer(\n    cols = -index,\n    names_to = \"Data\",\n    values_to = \"resid\"\n  ) %&gt;% \n  mutate(\n    Data = factor(Data, levels = c(\"Lite brus\", \"Stort brus\", \"Avvikande nära mitt\", \"Avvikande i ytterkant\"))\n  ) %&gt;% \n  ggplot() + aes(x = index, y = resid) + \n  geom_point(size = 2, color = \"steelblue\") + theme_bw() + \n  facet_wrap(~Data, nrow = 2, ncol = 2) + \n  labs(y = \"Cook's Distance\") + \n  geom_hline(\n    yintercept = 1,\n    linewidth = 1,\n    linetype = 1,\n    color = \"#d9230f\"\n  ) + \n  geom_hline(\n    # Gränsvärde enligt tabell A.10 i Kleinbaum där 17.18 hittas vid k = 1, n = 25 (närmast n = 31) och alpha = 0.05\n    # 29 är n - (k + 1)\n    yintercept = 17.18/29,\n    linewidth = 1,\n    linetype = 2,\n    color = \"#d9230f\"\n  )\n\n\n\n\n\nCook’s Distance för de olika modellerna.\n\n\n\n\nOm vi använder gränsvärdet 1, är det bara den tydligt avvikande observationen i ytterkanten på x:s värdemängd som anses vara inflytelserik. Om vi istället använder den mer robusta gränsen pekar det på att de små värden i det brusiga data utan tillagda extremvärden anses vara inflytelserika. Sedan tidigare har vi identifierat att observation 11 är en utav de observationer med minst värde på x vilket innebär att den också ligger i ytterkanten av värdemängden. Vad detta resultat då antyder är att en observation med högt leverage, avvikande långt bort från mittpunkten av X, eller en observation med en väldigt stor avvikelse i Y, kommer påverka modellanpassningen utöver sitt egna värde.",
    "crumbs": [
      "Regressionsanalys",
      "**DEL II - Regressionsanalys**",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Detaljerad residualanalys</span>"
    ]
  },
  {
    "objectID": "01-regression/08-detailed-residuals.html#övningsuppgifter",
    "href": "01-regression/08-detailed-residuals.html#övningsuppgifter",
    "title": "9  Detaljerad residualanalys",
    "section": "9.3 Övningsuppgifter",
    "text": "9.3 Övningsuppgifter\nAnvänd SENIC data som presenterades första gången i Avsnitt 7.3.\n\nAnpassa en modell som beskriver infektionsrisken med alla variabler förutom ID. Utvärdera den och kontrollera modellens antaganden på ett lämpligt sätt.\nBeräkna leverage och visualisera måtten i observationsordning. Beräkna och visa gränsvärdet i visualiseringen. Är det någon observation som identifieras som avvikande?\nBeräkna DFFITS och Cook’s Distance och visualisera måtten i observationsordning. Beräkna och visa respektive gränsvärde i visualiseringen. Är det någon observation som identifieras som inflytelserik för sitt egna anpassade värde? För modellen i sin helhet?\nSammanställ en tabell med alla de observationer som identifierades i b) och c) och markera vilka som är avvikande och/eller inflytelserika (kolumner med TRUE/FALSE). Motivera potentiella orsaker till att de anses påverka modellanpassningen.",
    "crumbs": [
      "Regressionsanalys",
      "**DEL II - Regressionsanalys**",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Detaljerad residualanalys</span>"
    ]
  },
  {
    "objectID": "01-regression/08-detailed-residuals.html#referenser",
    "href": "01-regression/08-detailed-residuals.html#referenser",
    "title": "9  Detaljerad residualanalys",
    "section": "Referenser",
    "text": "Referenser\n\n\n\n\nKleinbaum, D. G., L. L. Kupper, A. Nizam, och E. S. Rosenberg. 2013. Applied Regression Analysis and Other Multivariable Methods. Cengage Learning. https://books.google.se/books?id=v590AgAAQBAJ.\n\n\nMuller, Keith, och Mario Mok. 1997. ”The distribution of Cook’s D statistic”. Communications in statistics: theory and methods 26 (januari). https://doi.org/10.1080/03610927708831932.",
    "crumbs": [
      "Regressionsanalys",
      "**DEL II - Regressionsanalys**",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Detaljerad residualanalys</span>"
    ]
  },
  {
    "objectID": "01-regression/08-detailed-residuals.html#footnotes",
    "href": "01-regression/08-detailed-residuals.html#footnotes",
    "title": "9  Detaljerad residualanalys",
    "section": "",
    "text": "Tabell A.10 i Kleinbaum m.fl. (2013) innehåller de kritiska värdena som måste divideras med \\(n - (k + 1)\\) för att jämföras med våra beräknade värden på \\(d_i\\).↩︎",
    "crumbs": [
      "Regressionsanalys",
      "**DEL II - Regressionsanalys**",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Detaljerad residualanalys</span>"
    ]
  }
]